WEBVTT

00:00:00.000 --> 00:00:12.000
Come on, guys. You should have told me. All right, there you go. All right. Thanks, Aaron. Back off to you. And we're going to be making the thread as we always do.

00:00:12.000 --> 00:00:19.000
Thank you. So I've got the question thread and I'll put my eyes in that.

00:00:19.000 --> 00:00:23.000
So that's the best place for questions for me to see.

00:00:23.000 --> 00:00:32.000
And yeah, excited to talk about the topic today, which is lang graph, which is a very powerful general tool.

00:00:32.000 --> 00:00:45.000
That enables building a variety of patterns using large language models and using really other code as well. So, uh.

00:00:45.000 --> 00:00:56.000
Let's get to it.

00:00:56.000 --> 00:01:04.000
All right. Assuming you're seeing my screen here. Go ahead.

00:01:04.000 --> 00:01:10.000
So LaneGraff, like most of the libraries we've been looking at, is part of Langchain.

00:01:10.000 --> 00:01:21.000
And there are, of course, alternatives in this space, but we very much recommend focusing on this, at least initially.

00:01:21.000 --> 00:01:43.000
What Langraf is providing is it's a framework that will let you basically use a graph to coordinate or execute LLM agents or chains or Really just LLM things, as well as whatever other general logic you wish to code that can become part of that system.

00:01:43.000 --> 00:01:59.000
And it lets you put it in a graph structure which I imagine everybody has at least some familiarity with, but if you're not super into data structures and such, the mental model you want to have here are nodes and edges.

00:01:59.000 --> 00:02:08.000
Sometimes nodes are also called vertices or vertexes, but I'll probably try to stick to nodes and edges for my language.

00:02:08.000 --> 00:02:14.000
Where a node is a dot in the visualization and an edge is a line.

00:02:14.000 --> 00:02:33.000
And what we're making the dots, as it were, the nodes will be things like LLM agents or just or code in general, that's where sort of like execution happens. And then the edges represent messages being passed containing state from

00:02:33.000 --> 00:02:55.000
Some node to some other node. And these edges and these nodes can be connected pretty much arbitrarily uh now arbitrarily now That said, that leads to a huge potential space. So you don't want to just go nuts uh There are some patterns we're going to show you of typical ways that these graphs can be built.

00:02:55.000 --> 00:03:04.000
For this domain. But this is an area where you could potentially tweak things or come up with your own approach as well.

00:03:04.000 --> 00:03:11.000
So. What are we getting out of this? Well, we're going to learn the components of an AI graph, the value of state.

00:03:11.000 --> 00:03:17.000
Some cases for it and comparing this a little bit to some of the other techniques we've learned about.

00:03:17.000 --> 00:03:40.000
Multi-agent graphs and then a few other more advanced, I would refer really to all the four through six here as different design patterns sort of borrowing from the language of software design patterns for anybody familiar with that from object oriented programming. So different design patterns with LLM graphs that

00:03:40.000 --> 00:03:49.000
Could be suitable for an application you're working on. Or at least could be a source of inspiration for you.

00:03:49.000 --> 00:04:07.000
So this is what a graph looks like here. And to be clear, this is just a particular example graph and we'll talk through what it's doing specifically. But you can see There are nodes and the nodes here are the dots

00:04:07.000 --> 00:04:36.000
And the edges are the lines. And then there's a little bit more going on here, but actually it sort of visually is helping break it down these red indicators basically are You can think of them as either nodes that are really simple, that are basically just if statements, or you could think of them as just a visualization of a conditional edge. So this edge will then

00:04:36.000 --> 00:04:42.000
Go here and then go left or right. Based on the state being passed into it.

00:04:42.000 --> 00:04:51.000
And this is just a visualization of the state being passed here. Technically, you'd have an object like this being passed along all the lines.

00:04:51.000 --> 00:04:58.000
Thanks for just showing it here. So let's talk through what this system is actually.

00:04:58.000 --> 00:05:13.000
Doing here. You see the entry point. And this is a fairly typical pattern for it because a lot of the power of LLM is that, hey, you can take natural language questions, natural language instructions and input. So often the input looks like something like this.

00:05:13.000 --> 00:05:24.000
But you'll see there's also this input here. So this is implying, hey, maybe there's also a rag query that is feeding some documents to the first known as well.

00:05:24.000 --> 00:05:29.000
So we've got a few inputs. And we'll get to this last input later.

00:05:29.000 --> 00:05:39.000
And the idea here is we're going to be asking it to say write some code. So we're asking it to write some Langchain code.

00:05:39.000 --> 00:05:50.000
And it generates something and we have it structure the output so it fits in the schema of the state that we expect to be passing along the edges.

00:05:50.000 --> 00:05:55.000
And so in this case, that means we have the actual coding logic.

00:05:55.000 --> 00:06:10.000
We have the imports separated out and then we have preamble. You can think of that as like readme human human facing documentation summary or LLM facing actually, right? But natural language.

00:06:10.000 --> 00:06:13.000
Summary. That gets passed here.

00:06:13.000 --> 00:06:26.000
And what this node is doing is it's checking all the imports. It's going to actually, and this To be clear, this is just an example, so it's a little open-ended what you'd really do here, but I think one good way to do it would be to

00:06:26.000 --> 00:06:34.000
Actually try the imports, right? Have some sort of system set up that can verify that the imports are real and valid and work.

00:06:34.000 --> 00:06:39.000
And if they fail. We pass a stack trace.

00:06:39.000 --> 00:06:50.000
And that's where we get back here, looped back to the generation. And we basically say, you know, hey, we tried to run this we couldn't import this and fix it, right?

00:06:50.000 --> 00:06:59.000
And so then the generation node will try again. And eventually at some point when it gets past the import check, all the imports are good, there's an actual code execution check. And again.

00:06:59.000 --> 00:07:17.000
What this is would be a little bit context dependent. But a loose sketch of it could be something that fires up a test suite on all the code and make sure there haven't been any regressions on the tests.

00:07:17.000 --> 00:07:27.000
Or at the very least, something that just runs this code and make sure there's no syntax errors, right? Like make sure this code parses and doesn't die automatically.

00:07:27.000 --> 00:07:38.000
Whatever this check does, if it fails, it's also going to result in some stack trace, some message or something that's going to go back Try again. And if it passes.

00:07:38.000 --> 00:07:48.000
We're happy. We're done. And then this here is the exit, really. This is the Sorry about that.

00:07:48.000 --> 00:07:58.000
The actual answer that will go out to the user, which will probably contain all of these things. It'll describe what it is and then say, here's your code, here's your imports.

00:07:58.000 --> 00:08:04.000
And yes, I see the question. I'll get to it in a sec.

00:08:04.000 --> 00:08:10.000
And the thing that I also emphasize about this end-to-end interface here is user facing wise.

00:08:10.000 --> 00:08:15.000
This is basically the same interface as just zero shotting with an LLM.

00:08:15.000 --> 00:08:27.000
So what I mean by that is if you just go to chatgpt.com, you ask your question, you get your answer, right? And if you say, hey, chat GPT, write me some Python code with imports and a preamble.

00:08:27.000 --> 00:08:33.000
That whatever does stuff You're going to get your response.

00:08:33.000 --> 00:08:54.000
But what we're adding here is this sort of self-reference. And yeah, to the Czechs, exactly. The idea is the check is probably going to be if not If not an LLM, then at least some programmatic check, some automatic check, right? There is such a thing as human in the loop, and we'll talk about that at some point, I'm sure.

00:08:54.000 --> 00:08:59.000
But a lot of what we're seeing here today, the idea is these nodes are going to be code.

00:08:59.000 --> 00:09:06.000
Which certainly could involve sending this to another LLM with another prompt saying, hey.

00:09:06.000 --> 00:09:19.000
Check this. In this particular example, I feel like you wouldn't want to pass it to an LLM to check because the idea is to see if the code actually runs. So if this is Python code, you're probably passing it to Python, really.

00:09:19.000 --> 00:09:24.000
But there could absolutely be cases. We'll see some cases, yeah.

00:09:24.000 --> 00:09:26.000
And Ash has got a good answer in the thread too.

00:09:26.000 --> 00:09:32.000
But yeah, that's the idea here is that even though there's a lot of complexity.

00:09:32.000 --> 00:09:45.000
The user interface is as simple as chat, is as simple as the regular chat GPT interface. And all of this just becomes a bit of a more complicated black box to them.

00:09:45.000 --> 00:09:51.000
But obviously, we're the ones building some boxes, so we need to know what's going on inside.

00:09:51.000 --> 00:10:05.000
I'll leave this up for a moment longer just because I think it's a good summary of what we're doing and I'll see another question coming in.

00:10:05.000 --> 00:10:14.000
I vaguely understand that you have functions and data that gets piped I'm having trouble understanding what is this, why is this as opposed to say a set of function calls?

00:10:14.000 --> 00:10:36.000
It comes from the graph structure. I mean, in a sense, you could model this as functions. Really, all of this is our way of modeling the system. The question that came up before, we need more system architecture. Well, system architecture is mental models applied to complicated technology. Could you say, oh, at the end, it's all just assembler. Why are we modeling anything, right? Like it's all just

00:10:36.000 --> 00:11:02.000
Electrons on silicon or something. But having these abstractions, having these models with You know, in this case, we're using a graph, we're using nodes and edges, but having that model to let us build our understanding and let us design the system and the code will correspond to this too, of course. This is the language that we'll see in the high level code we're using.

00:11:02.000 --> 00:11:08.000
And yeah, we'll see a lot more examples in the remaining slides. So I will leave that to the slides.

00:11:08.000 --> 00:11:17.000
Although this, I would say, is already an example. If you wanted to write a code generating system that had self checks.

00:11:17.000 --> 00:11:29.000
That's perhaps kind of valuable, right? Like you don't want hallucinations or syntax errors in your code. You want those to be checked and fixed automatically but we will see more examples too.

00:11:29.000 --> 00:11:42.000
So a little bit Well, Cursor has a human in the loop though, right? You all were doing this with Cursor. The idea though is that this system would just do it.

00:11:42.000 --> 00:11:48.000
And I guess maybe the agent does actually have some implicit graphs. The agent is a little bit more self-managing.

00:11:48.000 --> 00:12:01.000
But still, there's a lot of human in the loop. In any case, we'll see more examples too. But exactly, cursor could be thought of, at least cursor agent could be thought of as somewhat graphy.

00:12:01.000 --> 00:12:05.000
I still think of cursors mostly with a human involved too, though.

00:12:05.000 --> 00:12:15.000
Anyway, state. So state is represented typically as a dictionary, which again, for the people who are not Pythonistas.

00:12:15.000 --> 00:12:33.000
Json blob, key value pairs, right? I have dictionaries, though, are a nice way to represent it. And you'll see pedantic, which is a Python type system, you'll see that in a lot of the sort of code in this ecosystem.

00:12:33.000 --> 00:12:53.000
Also, when you expose APIs with these things. And… What the state has to have is it has to have all the information that is going to be passed from node to node, because the whole point of this structure, if nodes didn't pass anything, it wouldn't be a graph. If nodes just did their work.

00:12:53.000 --> 00:13:05.000
Then that's just something doing some computation. It's a graph because you have nodes passing the results of their computation back and forth. There are potentially loops. So there were a few loops here.

00:13:05.000 --> 00:13:21.000
And you see an explicit loop here. A loop is whenever you potentially can just go around and round. And as you can probably intuit, this can mean that the computation time can be arbitrarily long. Now.

00:13:21.000 --> 00:13:33.000
With laying graph, I believe there's a default recursion limit. Five or 10 or something. And that's the sort of parameter that you can also set if you're concerned about your system truly just going forever.

00:13:33.000 --> 00:13:38.000
But even if you have a limit.

00:13:38.000 --> 00:13:48.000
Graphs are a good example of something that like this is going to take longer to compute than just sending something to an LLM and getting it back or certainly than running non-LLM code most of the time.

00:13:48.000 --> 00:14:00.000
And so this also, you know. Back what we said Monday about asynchronous execution. You might, if there's a situation where you have some graph stuff going on, you might want to look at async.

00:14:00.000 --> 00:14:15.000
So that it's not blocking on everything else. Anyway, so… this state often the typical pattern you'll see This is like the schema for what's being passed on every edge.

00:14:15.000 --> 00:14:22.000
And at the beginning, it will mostly the message, like this will mostly be unpopulated. It'll kind of be empty, right?

00:14:22.000 --> 00:14:28.000
And then it'll get populated field by field by the nodes as you go.

00:14:28.000 --> 00:14:38.000
And those fields, the populated fields. Will also be used by the nodes to make their decision. So the query will go in here.

00:14:38.000 --> 00:14:45.000
To the browser and the editor and the browser and the editor are probably going to, you know.

00:14:45.000 --> 00:15:02.000
Talk about what sections should be written. This is a research state, a research paper writer, right? And then there's this loop where I want to talk about the loop. It goes to the researcher. The researcher will have, say, actual internet access, some tools to search the internet.

00:15:02.000 --> 00:15:09.000
And we'll find research data, which we're representing here as a list of more blobs.

00:15:09.000 --> 00:15:19.000
All sorts of stuff that goes to a reviewer and a revisor. And if they're not happy with it, it goes back to the researcher who would then update, find more research data, right?

00:15:19.000 --> 00:15:24.000
And this could potentially keep going for a while, depending how you've prompted and designed it.

00:15:24.000 --> 00:15:43.000
Until eventually you have all this sort of structure and research data populated and that goes to the writer that actually writes the report itself, right? And also presumably the conclusion introduction and stuff so So the idea of this system is instead of just asking an LLM to write a paper.

00:15:43.000 --> 00:16:00.000
We are having a number of steps that are going to gather information And update that information until it's satisfactory to make a good plan and a good structure and have good research information and then pass that finally to an LLM prompt that will actually

00:16:00.000 --> 00:16:14.000
Write the paper. But chances are most of these nodes are going to involve some LLM not necessarily to like only this one's the only one doing a lot of writing, but to do other logic to search the internet, to do things like that.

00:16:14.000 --> 00:16:26.000
To the question, I thought of this as a state machine I mean, state machines could be a model for almost anything. Yeah, you can think of this as a state machine.

00:16:26.000 --> 00:16:34.000
Where the state change is basically following one edge or another edge.

00:16:34.000 --> 00:16:42.000
I think that the graph representation is useful visually. And again, it's also going to be language we'll see in the software framework itself.

00:16:42.000 --> 00:16:53.000
But yeah, I think you're coming at this from a reasonable direction and you'll continue getting understanding as we see the actual code too.

00:16:53.000 --> 00:17:09.000
So revisers are LLMs with certain rules. Well, again, this is just a sketch. We haven't actually implemented fully every node and how you implement it would impact how reliable it is. But yes, the idea that I would see here is that the

00:17:09.000 --> 00:17:18.000
Revisor and the reviewer would be LLMs that are given the output from the earlier steps and are told like, hey.

00:17:18.000 --> 00:17:25.000
Review this for this sort of issue and then, hey, fix these sorts of issues.

00:17:25.000 --> 00:17:34.000
And then decide whether or not we need to get more resources or it's fixed enough for the writer to do what they should.

00:17:34.000 --> 00:17:52.000
But yeah, each of these nodes can be LLMs a super prompt. Well, I mean, I think of it as separate individual prompts and that's what makes this so powerful. As you've all probably discovered, if you try to cram too much in a prompt, you sometimes overwhelm the LLM.

00:17:52.000 --> 00:18:07.000
And so even if these are all the same actual model, these are all just calls to GPT 40 mini or something, they're going to be prompted differently. They're going to have different contexts. They could potentially have different hyperparameters if that's suitable for you. And

00:18:07.000 --> 00:18:18.000
And I'm not going to like, I'm not… I think browser, I'm not sure that browser is an LLM here. I'd have to think about that one more. But certainly a lot of these capital ones most likely are in this case.

00:18:18.000 --> 00:18:27.000
And yeah, the multiple prompts with different input will really change how they behave.

00:18:27.000 --> 00:18:35.000
All right. Keep moving here. Because I think these questions will continue to be relevant with the ongoing slides.

00:18:35.000 --> 00:18:47.000
And yeah, as Ash said, you know, you could have, these could each have their own rag. They could have separate retrievers They could be different models. Absolutely. Yeah.

00:18:47.000 --> 00:19:04.000
And part of the good news is the open AI's API format has become a little bit of a de facto standard. So even if you develop against OpenAI's API, it's not too hard to port that or to use that with

00:19:04.000 --> 00:19:27.000
Something that is, you know, self-hosted or via some other service All right. So comparing a little bit. And while we compare, I also want to emphasize all this stuff is composable. I mean, you all write code. You know what I'm talking about here. You can have classes within classes and functions within functions, right? Well, you can do the same thing with all this. You could have

00:19:27.000 --> 00:19:50.000
A graph where one of the nodes is a chain. Say, right. Or a chain that occasionally kicks off a graph for some reason or whatever uh Again, as I've said a few times, don't just do that for giggles. Do it if and only if it's truly reflective of the complexity of the problem you're trying to solve and it's the right structure for what you're doing.

00:19:50.000 --> 00:19:56.000
Use the simplest approach that's going to work, especially while you're getting started, but really just in general.

00:19:56.000 --> 00:20:06.000
That said, that's what's so powerful here. The incredible flexibility And that's emblematic in particular of graphs.

00:20:06.000 --> 00:20:23.000
So chains, as we discussed on Monday, are these sequential execution structures. You can think of a chain as a subset of a graph. A chain is a graph where it's just node to node to node to node with one edge along each link.

00:20:23.000 --> 00:20:35.000
So a chain is a graph, but I mean, a chain is a special case that is particularly common in something like a data pipeline. It's a graph that only goes one way.

00:20:35.000 --> 00:20:45.000
Graphs, by the way. Have subtypes. Does anybody know the name for a graph that only goes one way that's more general than chains?

00:20:45.000 --> 00:20:46.000
Close. Or maybe. I think, yeah, DAG directed it.

00:20:46.000 --> 00:20:50.000
Diagraph.

00:20:50.000 --> 00:20:51.000
Directional graph.

00:20:51.000 --> 00:20:57.000
Directed acyclic graph, exactly. And you've been using that every time you've been using Git.

00:20:57.000 --> 00:21:13.000
So that would be a graph that doesn't have these loops. And it's still more flexible than a chain. It can still have forks. It's just like you can have branches and Git. But a DAG can be a very useful structure. And sometimes it's a very useful restriction to say, hey, no loops.

00:21:13.000 --> 00:21:27.000
Of course, sometimes you might actually want the loops. You want the system to be reflective and you're able to structure around it to make sure that the time it spends doing that is in some way limited so it doesn't just spin forever.

00:21:27.000 --> 00:21:56.000
But the point is, is that there's all these different structures you can do with graphs that are more complex than chains. And then agents, again, you could think of an agent as Essentially, the LLM making the decisions. And the LLM being given more flexibility perhaps than just like left or right on a graph. But the LLM actually being given a suite of options

00:21:56.000 --> 00:22:21.000
And deciding what comes next. So really, I would actually say to that question about cursor, cursor is less of a graph and more of a human, well, an agent, right? The agent code is right there in the in the name. In agent mode, it's acting like an agent. And of course, it's still an agent that has at least some human in the loop, which refers to the fact that it will ask your permission, for example, before running

00:22:21.000 --> 00:22:26.000
Certain things. And this is good because what if it accidentally ran RM-RF, right?

00:22:26.000 --> 00:22:32.000
So you, you… often want human in the loop with agents.

00:22:32.000 --> 00:22:45.000
Because of how powerful they are. In a sense, they are the most general because they're not even limited by any structure. They just sort of quote, do what they want, or at least within the realm of the tools and the options that they are given.

00:22:45.000 --> 00:22:55.000
And there's no prescription or limit when they choose what They choose based on the step of the problem they're facing as they see it.

00:22:55.000 --> 00:22:56.000
So.

00:22:56.000 --> 00:23:08.000
And I want to also, whenever I talk about agents, I feel necessary to give a slight caveat. It's convenient to talk about them with language like they, right? And they decide and they see.

00:23:08.000 --> 00:23:11.000
It's 59 degrees in here. So cool.

00:23:11.000 --> 00:23:18.000
Somebody's muted, by the way. Unmuted, I mean.

00:23:18.000 --> 00:23:27.000
Great. Thanks. Whenever we talk about agents, it's very natural to anthropomorphize them.

00:23:27.000 --> 00:23:38.000
That's the fancy way to say to treat like a human, to use language like, oh, they, they see, they think. And that's because that's the most convenient way to describe what we observe and what we're interacting with.

00:23:38.000 --> 00:24:01.000
That is, again, I'm not going to philosophize fully because we have a lot of brown to cover here, but from a technical perspective, I just want to remind all of us that that is not at least what is happening in any way we understand it. That is not the internal model of them in any way comparable to us, at least consciously.

00:24:01.000 --> 00:24:18.000
They're still very cool. I'm not ragging on them at all here. But when you anthropomorphize your tools too much and get superstition about them, you're not going to build as reliably with them. You're not going to understand them and be on your toes and know their limitations. So they're not magic.

00:24:18.000 --> 00:24:26.000
And whether or not they are conscious, I will leave to people to philosophize some other way. But just be careful when you anthropomorphize them.

00:24:26.000 --> 00:24:35.000
That'll hopefully be the main time I give this caveat as we talk about agents, but they are very cool.

00:24:35.000 --> 00:24:38.000
A few other questions, but they look like they're all handled.

00:24:38.000 --> 00:24:55.000
Agents can choose which nodes. I mean, with agents, there maybe aren't even nodes. Agents can just choose their actions based on whatever actions are made available to them. And we haven't fully introduced agents So I would say think of cursor

00:24:55.000 --> 00:25:07.000
Agent for now where it's like it can write code, it can run shell commands and like run shell command is a pretty general tool, right? There's a lot of things you can do with that.

00:25:07.000 --> 00:25:21.000
As is write code. Exactly. The decision making is the, but exactly the LLM, instead of having any sort of logic that says, oh, we've got edges on the graph and there's a condition that will make the edge fork based on such and such.

00:25:21.000 --> 00:25:42.000
Instead of that, it's entirely the LLM making the decision. And you might say, oh, well, the LLM is kind of making the decision here too. But it's subtly different in that we're making the decision a single yes, no decision based on the output of the LLM. And that is different than an agent, which is not just single yes, no decisions guiding a graph.

00:25:42.000 --> 00:25:47.000
But is actually like gets to choose between all the options every time.

00:25:47.000 --> 00:26:01.000
Whereas with graphs, you can structure and you can basically direct it more. So I would say ultimately agents are actually the most flexible, the least not the least structured exactly, but like the least pinned down. Chains are a subset of graphs and graphs are

00:26:01.000 --> 00:26:14.000
A useful representation of all sorts of things. You can represent social relationships, you can represent knowledge, you can represent flow charts and processes. There's lots of things you can represent with graphs. And so if you could represent something with a graph.

00:26:14.000 --> 00:26:26.000
And if some of those things can be automated by LLMs, that becomes a candidate for a LLM system to automate And to have a useful product of some sort.

00:26:26.000 --> 00:26:35.000
So that's the way I would start seeing this. It's like, think about graphs as they apply to the world and then think about, hey, what if I could automate it?

00:26:35.000 --> 00:26:40.000
All right. Keep the questions coming, but I'm going to get to these few examples.

00:26:40.000 --> 00:26:47.000
And there's also some code if we have time for all of it. We'll get to at least some of it, I think.

00:26:47.000 --> 00:27:02.000
And we're getting here to the part that I would say are the design patterns of graphs that we're showing. And if you're unfamiliar with what I mean by design patterns, if you've never heard of the of a factory, right? Which is a factory

00:27:02.000 --> 00:27:13.000
Function or method that can instantiate objects. That is a software design pattern from a book of software design patterns from like the 80s or something, right?

00:27:13.000 --> 00:27:21.000
So what are the patterns? What are the typical things that we might do to organize our graphs?

00:27:21.000 --> 00:27:27.000
Since they are fairly open-ended. Well, one of them is to have multiple agents.

00:27:27.000 --> 00:27:36.000
And the graph essentially mediates their contributions and interactions. So the user asks their question.

00:27:36.000 --> 00:27:50.000
Generate a chart of average temperature in Alaska. And that is set up in the graph to first go to one agent a researcher and this researcher will have particular tools of like search the internet.

00:27:50.000 --> 00:28:01.000
Get some data, these sorts of things. But they're an agent, so they can kind of just make their own decisions to solve this problem within that within what they have.

00:28:01.000 --> 00:28:14.000
Then the router here is what will take their request and they're either going to say, hey, I need to run a tool and use that. That'd be like search the internet.

00:28:14.000 --> 00:28:28.000
And use that to continue working, in which case we loop back or, hey, I'm done. I've done all my research. I've got everything. In which case it gets routed to the other agent. And the other agent is the one that will actually try to

00:28:28.000 --> 00:28:35.000
Resolve the user query to generate the chart but also has the information, the output from this agent.

00:28:35.000 --> 00:28:41.000
And they too can say, hey, I need to run a tool. I need to run some code or see something.

00:28:41.000 --> 00:28:50.000
And then when they're happy, they'll say, hey, I'm done. And final answer. And if they're done, done, then it goes back to the user.

00:28:50.000 --> 00:28:56.000
Or they could say, hey, there's not enough data. Send it back to the researcher and it goes back to the researcher.

00:28:56.000 --> 00:29:15.000
And part of the purpose of this sort of pattern, you could imagine how with more nodes and edges you could have more agents and more conditions for how things are passed between them. And we'll see examples like that. But we've been talking about how general agents are and how powerful and flexible they are as well.

00:29:15.000 --> 00:29:25.000
But that actually is a source of danger, really. And I don't mean in the AI safety sense. I mean more like just in the software engineering, your code doesn't work sense.

00:29:25.000 --> 00:29:42.000
If you have something that is extremely capable and you try to give it every tool, like why do we have two agents here? Why don't we just have one agent and we give it search and we give it code. And I mean, honestly, this problem is simple enough. That would probably work okay.

00:29:42.000 --> 00:30:04.000
But the more tools you give an agent. And the more options you give an agent, the more likely it is to be confused. Like imagine if the cursor agent wasn't just writing code and running command line tools But it could also do like five other things, right? Then it has all the a larger surface area of decisions and tasks to focus on

00:30:04.000 --> 00:30:19.000
And it's more likely to mess up. I mean, a human's more likely to mess up in that situation too. All decision-making agents are going to function better when there is more focus, when there's a more limited problem space and splitting up multiple agents

00:30:19.000 --> 00:30:38.000
With a graph structure allows the agents to be more focused, to have narrower problem spaces And then it's up to the graph structure and whatever other logic you're coding to actually combine it all, put it together in a way that makes sense for the problem you're trying to solve. And again, this is just a particular example of that.

00:30:38.000 --> 00:30:52.000
But the general idea is it can be very useful to encapsulate several agents in a graph rather than like saying, oh, the system is an agent and it's just a single agent and this agent somehow does everything.

00:30:52.000 --> 00:30:58.000
That's in many situations just not going to be a very reliable or good system.

00:30:58.000 --> 00:31:05.000
And you'll want to add a little bit more structure to be able to get it where you want to get it.

00:31:05.000 --> 00:31:15.000
Um… All right. There are some questions coming. Yeah, agents also get analysis paralysis. Yes, that's a good way to put it.

00:31:15.000 --> 00:31:23.000
And I think the other question in there, Ash, is probably answering. So I'll let him do that. Thank you.

00:31:23.000 --> 00:31:30.000
So what's a general pattern if we want to scale to even more agents in our graph?

00:31:30.000 --> 00:31:37.000
Well, there's the supervisor pattern. And this looks almost like an org chart, doesn't it?

00:31:37.000 --> 00:31:44.000
So it's just an interesting parallel, again, be careful anthropomorphizing your agents.

00:31:44.000 --> 00:32:00.000
But we have here multiple agents. And in this case, they each have different specializations. This could be a research agent and this could be a coding agent and this could be a documentation agent. I don't know. You can build your whole team here, right?

00:32:00.000 --> 00:32:08.000
And then you have a supervisor. So the supervisor's job is to be the interface.

00:32:08.000 --> 00:32:15.000
You see the user only ever interact with the supervisor. And by the way, the end-to-end interface that the user is going to see here It's basically going to be the same.

00:32:15.000 --> 00:32:21.000
As the one they saw here. They give a question, stuff happens.

00:32:21.000 --> 00:32:33.000
They get an answer. The LLM interface at the top level for the user is pretty pretty much usually pretty constant, right? You could have different systems but A lot of the patterns are going to look like this.

00:32:33.000 --> 00:32:47.000
But in this case, what that black box is doing is the user gives their request. And again, say these agents are specialized to be different roles of a software development team, like a technical writer and a coder and a QA.

00:32:47.000 --> 00:32:56.000
Something like that, right? And the user gives a software development task, develop and test uh tic-tac-toe application.

00:32:56.000 --> 00:32:59.000
I'm not sure why you'd want that, but you could ask for whatever.

00:32:59.000 --> 00:33:09.000
It's up to the supervisor then to basically take this task and figure out which agent it should go to first.

00:33:09.000 --> 00:33:31.000
Right. And then route the task to the agent and the agent works on the task and updates whatever else. We're not picturing the state dictionary here, but there would be a state dictionary that would probably have for the purposes of software development, it would have like files or code or directories with files and code in them and maybe some other state that

00:33:31.000 --> 00:33:41.000
That is relevant to whatever particular problem you're solving. And the agent that it's assigned to, it assigns to the coder agent first, say, would write some code, update stuff.

00:33:41.000 --> 00:34:00.000
And when they're done, return it to the supervisor. The supervisor then looks at the updated state and decides what to do next, and their decisions could be routed to an agent again or give it back to the user. And they might say, oh, well, there's code, but it needs to be tested. So they pass it to the QA and then they get it back. No, there's code.

00:34:00.000 --> 00:34:23.000
But it needs some documentation, so they pass it to the technical and they get it back. And by the way, what's nice about this pattern versus this pattern This one… If you were going to add a new agent, you'd have to think a little bit about your routing rules or it would change the graph, basically. This is a two agent graph, the way it's written and anything else would just be a different graph.

00:34:23.000 --> 00:34:29.000
This pattern scales horizontally pretty well in the number of agents, because if you add another agent.

00:34:29.000 --> 00:34:35.000
It would just be another leaf. Agents are always leaves. And they always have two edges back and forth to the supervisor.

00:34:35.000 --> 00:34:45.000
And that's it. That's it for this supervisor pattern. That's the way you would expand it. So you could potentially add arbitrarily many agents.

00:34:45.000 --> 00:34:59.000
And the supervisor would be the one mediating all these connections. And eventually when the agents have done their work, again, within the judgment of the supervisor, and when I say the judgment, what I really mean is within the prompting you gave the supervisor.

00:34:59.000 --> 00:35:21.000
Within how this is set up in terms of whatever system prompt and whatever information, whatever others context it's being given eventually this will reach the decision that, okay, we're done the agents have updated the state to a satisfactory condition. I'm going to pass the results back out to the user.

00:35:21.000 --> 00:35:42.000
And this is, again. Really a pretty general pattern. I talked through it as a software development thing, but anything that a team works on could potentially, you know, you could try to automate it with this. And that's Pretty cool. That's pretty powerful. It's interesting how this looks like a human team structure.

00:35:42.000 --> 00:35:51.000
So… A few other questions coming in.

00:35:51.000 --> 00:36:11.000
Yeah, so exactly. A lot of the magic here, frameworks are going to handle things like timeouts or recursion limits. You might need to actually look up and set some details if you're really building production grade systems. So you can't just completely ignore it. But in terms of developing it, there are accommodations for that.

00:36:11.000 --> 00:36:23.000
What does perplexity do? I'm not… I mean, we don't work for Perplexity and they're not open source, but I feel like it's more of a rag than a supervisor agent pattern.

00:36:23.000 --> 00:36:42.000
And maybe there are some tools for particular things. Certainly if I was building an information retrieval system, which is what perplexity is, I would want it to have tools that handle things that LLMs are bad at. So it should be able to recognize when something's a math problem and just go use Wolfram Alpha.

00:36:42.000 --> 00:36:47.000
Just don't try to solve it yourself, LLM. You're not meant to do this.

00:36:47.000 --> 00:36:55.000
That kind of thing. So I would think it's probably just a single agent, but If they could have a graph going on over there.

00:36:55.000 --> 00:36:59.000
All right, I'm going to go on to the next example.

00:36:59.000 --> 00:37:05.000
So, you know, I was saying, hey, how do you scale this out? You can just add more agents.

00:37:05.000 --> 00:37:34.000
Horizontally here and add more lines. But at a certain point you know, just like human managers can't really effectively manage more than pick what number you want, but let's say 10 direct reports these LLM supervisors, if you give them way too many agents as options, they're also going to suffer from some of that analysis paralysis or at the very least, just there'll be more randomness, more

00:37:34.000 --> 00:37:56.000
Stochastic behavior and less reliable output in terms of the assignment of tasks simply because there's more options, right? Like, okay, if you have 100 agents reporting to you, then assigning your tasks, splitting them out, getting the results back and forth just becomes a more random griddle, difficult process. So if you have if you have

00:37:56.000 --> 00:38:02.000
Really wanted to build a system with that many agents or even say 20 or 15 or something.

00:38:02.000 --> 00:38:09.000
You might want to split it out. And that's what this is showing. And it's just hierarchical. And again, we're looking even more like an org chart here.

00:38:09.000 --> 00:38:26.000
Where the user is still end-to-end text to text. Interact with the supervisor. The supervisor, though, does not route to the individual contributors, to the leaf agents. The supervisor routes to what we could call team managers, if you will, or team leads and these

00:38:26.000 --> 00:38:31.000
Function a lot like the supervisor did. Back in the prior pattern.

00:38:31.000 --> 00:38:38.000
But they have a particular domain. This domain is research, this domain is documents and writing.

00:38:38.000 --> 00:39:03.000
You can have one with the domain to software development, et cetera. And then that team lead is responsible for the routes back and forth to try to solve the problem they're given with the actual leaf agents here that actually do the work. And by the way, I've been saying leaf, if you're not familiar with that, a leaf node is a bottom node on a graph like this. It's a graph node without any children.

00:39:03.000 --> 00:39:10.000
That's what it's referring to. We're not going any further down here.

00:39:10.000 --> 00:39:33.000
So this is just another pattern. I wouldn't jump to this one, but it's good to be aware of as an option if you're building a system that is complicated enough that it might warrant this. And it's again in a real world situation, would you use exactly these graphs, exactly as they're pictured here? Well, probably not. You're going to be solving

00:39:33.000 --> 00:39:44.000
Whatever specific problem you're solving, and there might be little edge cases here and there or other special inputs or other little things, right?

00:39:44.000 --> 00:39:59.000
But that's just like, you know, that's why I'm referring to these as design patterns. Design patterns are blueprints, but not exact recipes. So you would use this as a starting point, but if you end up tweaking it some for whatever problem you're solving.

00:39:59.000 --> 00:40:07.000
That's fine. So…

00:40:07.000 --> 00:40:25.000
Yes, to the definition of agent. Well, I would say that um Ash, we probably will have other lectures where we dig more into that. So I'm going to try to get through more of this material rather than lose the last 10 minutes on that. But I definitely hear you that that's something to discuss.

00:40:25.000 --> 00:40:36.000
I do think that part of the reason that it's difficult to define rigorously is it's still something that is so new that at least the casual public usage of it isn't necessarily rigorous.

00:40:36.000 --> 00:40:47.000
But again, I do think that the cursor agent is a reasonable representation. I'll try to give a concise definition that I would consider correct.

00:40:47.000 --> 00:41:03.000
Which is that an agent is a system is a system Where an LLM can guide can make decisions and use tools.

00:41:03.000 --> 00:41:14.000
And, uh. Tools being other code, other things. So again, think of the cursor agent. It can literally run shell script, right? That is a tool.

00:41:14.000 --> 00:41:21.000
Or a lot of these other examples here. A searcher would use a search tool, which is exactly what we're going to get to in the code here.

00:41:21.000 --> 00:41:26.000
I don't think this slide is relevant. Ash.

00:41:26.000 --> 00:41:28.000
It's if anybody wants to do it.

00:41:28.000 --> 00:41:35.000
Okay, if you want to practice thing, here is an example graph that you could make.

00:41:35.000 --> 00:41:47.000
Where you actually have a loop, you give user input and you ask to write a report and you reflect on the report and improve the report. And this is a reflection pattern that can improve the quality of the output.

00:41:47.000 --> 00:41:55.000
So that said, what we'd like to do here First, just look at at least some code.

00:41:55.000 --> 00:42:02.000
This is… MVP land graph code.

00:42:02.000 --> 00:42:07.000
And all it does, and this is truly MVP, which is to show the imports of the constructs.

00:42:07.000 --> 00:42:17.000
We have our setup and our connections. We need a model We instantiate a graph and we're going to instantiate it as a message graph. A message graph is a special sort of graph and lane graph.

00:42:17.000 --> 00:42:36.000
Where the state is assumed to be messages like LLM messages. And this means that we don't have to define a schema for the state being passed between nodes because we just assume it's LLM like open AI format chat messages, which is kind of convenient. Then we're going to add a single node

00:42:36.000 --> 00:42:57.000
Which we're going to call Oracle and a single edge from that node to the exit point, which is a special thing called end, which we also imported up here. So this is the end state. So the question of state diagrams, yeah, you could consider estate machines. You could write a state diagram or consider graphs a state machine as well but

00:42:57.000 --> 00:43:14.000
We set the entry point as Oracle, we compile it, and that gives us a runnable. And a runnable is the same really as the model itself, the same as a chain. It is something in laying chain that we're able to give input to and get output it gives

00:43:14.000 --> 00:43:23.000
Gives us this interface here. We can run this. We'll give it our question. It will give us a response.

00:43:23.000 --> 00:43:30.000
And then we're going to just ask it, what is one plus one, which is kind of silly, but just to prove that it's actually doing something.

00:43:30.000 --> 00:43:37.000
And if we run this.

00:43:37.000 --> 00:43:49.000
One plus one equals two. All we really did here was give a single prompt to chat gpt for Omini. And despite me saying that LLMs aren't good at math.

00:43:49.000 --> 00:43:53.000
They are able to do one plus one. And it said one plus one equals two.

00:43:53.000 --> 00:44:11.000
And we printed that response here. But what actually happened was that got passed in the entry point to the Oracle node, the Oracle node sent it to the model. The model got the response for the Oracle node, and then that went to the end node, which is what gave us the actual output.

00:44:11.000 --> 00:44:21.000
And you can add other nodes and edges. Basically, you can structure graphs however you want, right? That's the power of it.

00:44:21.000 --> 00:44:31.000
But we will. Look at some of these notebooks here.

00:44:31.000 --> 00:44:41.000
Let me also see if they're pre-rendered.

00:44:41.000 --> 00:44:58.000
While this is coming up.

00:44:58.000 --> 00:45:08.000
All right. So what these are, by the way. The first notebook here, multi-agent collaboration is an implementation of this system.

00:45:08.000 --> 00:45:14.000
The supervisor for the supervisor and the hierarchical team for the hierarchical team.

00:45:14.000 --> 00:45:34.000
So I'm actually going to skip the multi-agent collaboration one in the interest of time. You can refer back to that one. I'm going to look at the agent supervisor one a bit And then leave the other two for you as the sort of more complicated and less complicated ones.

00:45:34.000 --> 00:45:44.000
So…

00:45:44.000 --> 00:45:54.000
Yeah, so you only need to specify one entry point and you only need to get to the end. Well, potentially your graph could have multiple exits, right?

00:45:54.000 --> 00:46:01.000
That might be confusing. I think that's typically not what you would do. But you could route to the end wherever you want.

00:46:01.000 --> 00:46:18.000
The entry point, you should have a unique entry point. And yes, so if you want to, it's not that you if you want to add metadata to the message, well, that's no longer a message. You should just use what's called a state graph, which is what we'll see in the other examples. And a state graph

00:46:18.000 --> 00:46:34.000
Lets you define your own schema with arbitrary key value pairs to be the message schema that's passed around.

00:46:34.000 --> 00:46:41.000
All right.

00:46:41.000 --> 00:46:46.000
So you'll see here we're getting set up and connected. We're importing some tools.

00:46:46.000 --> 00:46:53.000
So tools, again, are functions or code that are designed for LLMs to use.

00:46:53.000 --> 00:46:59.000
To further whatever task they're given. And we're getting a Python REPL tool.

00:46:59.000 --> 00:47:06.000
Which lets you run Python in a REPL. A Tavilli search tool, which lets you search the internet.

00:47:06.000 --> 00:47:20.000
And then just the general constructive tool. And then we're going to then we're going to make some helpers here. These are some soft, some reusable code that you could use when you're making your own system.

00:47:20.000 --> 00:47:26.000
It's helpful to, this is, I would say, basically a factory pattern, interestingly enough.

00:47:26.000 --> 00:47:37.000
To have a function to create agents. And we give it the LLM connection, the tools it should have, and the prompt that it should have. And it puts all those pieces together.

00:47:37.000 --> 00:47:46.000
So it puts the prompt into a system prompt and it also makes room for messages and what's called the agent scratch pad.

00:47:46.000 --> 00:48:00.000
The agent scratch pad are uh Also messages, but not like human messages that we would interact with their space for the agent to basically think out loud or to say what it's doing.

00:48:00.000 --> 00:48:20.000
And to plan. The agent then is based on this import create open AI tools agent from lane chain and we give it the LLM connection, the tools and the prompt. And then we have to make an executor Which will actually expose the interface we want to put

00:48:20.000 --> 00:48:27.000
In the graph. So the executor also has the agent and the tools and that's what we returned here.

00:48:27.000 --> 00:48:37.000
And then an agent node will take an agent We'll take a state And we'll take a name.

00:48:37.000 --> 00:48:48.000
And it will invoke the agent on the state And then it will return basically another state. It will return the messages and the name of the node.

00:48:48.000 --> 00:48:53.000
So we are adding some metadata here. We're adding what node we're on.

00:48:53.000 --> 00:49:03.000
And… this really is a note. It's kind of weird to think about But…

00:49:03.000 --> 00:49:08.000
The nodes are something are functions that take state and return state.

00:49:08.000 --> 00:49:13.000
That's, I think, one of the better ways to think about nodes. Takes state.

00:49:13.000 --> 00:49:25.000
Return state. Now it takes state and return state in a way that like it's taking state from a certain direction, returning state in a different direction, but they all take state and return state.

00:49:25.000 --> 00:49:30.000
And so this general construct, you have to, of course, pass the actual agent that's doing the processing.

00:49:30.000 --> 00:49:48.000
But this will let you make whatever nodes you want. The supervisor needs its own special prompt and so we give it And this can be even more engineered, but for a basic prompt, we're saying, hey, you're a supervisor, you're managing the conversation, we're going to tell it who it's managing it between.

00:49:48.000 --> 00:49:56.000
And then given their requests say which workers should act next if we're done, say finish.

00:49:56.000 --> 00:50:07.000
And then there's There is a bit of a structure here that I don't think I'll get to in just a few minutes in terms of fully summarizing.

00:50:07.000 --> 00:50:28.000
If you look up OpenAI function calling, what this is, is this is a structure that actually enforces the the format of the response from the LLN. It's saying that we expect it to give us any of a certain number of options, right? And that this needs to be

00:50:28.000 --> 00:50:42.000
In what it returns and that it's a required field, this next field so these options are going to be the agents, the members of the team right now, researcher and coder, but you could add other agents here.

00:50:42.000 --> 00:50:55.000
And that basically will enforce all this structure when we pass it to OpenAI will force the system to respond with one of these things because it's the required field and it has to be one of these things.

00:50:55.000 --> 00:51:03.000
And that's very, very useful rather than trying to prompt and argue with it to say like, please, please, please don't ever respond with anything else.

00:51:03.000 --> 00:51:08.000
You know, instead of that, we can actually enforce it. So that's what's going on here.

00:51:08.000 --> 00:51:16.000
And that way it's deterministic in its selection. Well, it's not deterministic in what it selects. It still gets to choose freely between these two.

00:51:16.000 --> 00:51:22.000
But it's deterministic in that it has to follow this schema.

00:51:22.000 --> 00:51:28.000
This is all put together. And then the intermediary, when it's not the very first question.

00:51:28.000 --> 00:51:38.000
We tell the supervisor, hey, here's the conversation so far, because basically that's what the work history is. It's a conversation of the past agents between each other and with themselves.

00:51:38.000 --> 00:51:42.000
And we asked the supervisor, hey, given this conversation, are we done?

00:51:42.000 --> 00:51:47.000
Or should we route it to somebody else? Put it together as a chain.

00:51:47.000 --> 00:51:52.000
So even though we're using graph structures, the supervisor can be thought of as a chain.

00:51:52.000 --> 00:51:59.000
And then to actually make the graph, we need to represent the state. The state is a type dictionary, which really is just messages.

00:51:59.000 --> 00:52:07.000
So this almost could be a message graph, but we're also keeping track of what agent should be passed to next. So that's what the next field is.

00:52:07.000 --> 00:52:16.000
And that's what keeps, that's what is being passed around by the different LLM nodes to each other.

00:52:16.000 --> 00:52:20.000
Then we can instantiate both the agents with the helpers we have.

00:52:20.000 --> 00:52:28.000
We can run create agent And say, hey, give it the web search tool and say you're a web researcher.

00:52:28.000 --> 00:52:33.000
And we can run create agent and give it the Python tool and say, hey, you write Python code.

00:52:33.000 --> 00:52:44.000
And that gives us these agents. And then there's a little bit of functional programming paradigms going on here where, well, what is a node?

00:52:44.000 --> 00:52:50.000
A node is

00:52:50.000 --> 00:53:10.000
Whereas the agent node A node is this agent node function partially evaluated so we're passing the agent to be the agent to be to do the execution and the name of it, but what we're not passing We're not passing the state.

00:53:10.000 --> 00:53:24.000
So the result of this partial evaluation is a function that has two of the three parameters hard coded, but the third parameter state is open. And so now this truly is a node. This is a function that takes state.

00:53:24.000 --> 00:53:43.000
And return state. Same thing for the coder. And then, similar to the MVP example, we've got to add the nodes to a workflow. The workflow, though, is a state graph based on the state schema And then we need to add edges. Everybody has an edge to the supervisor. And then there's conditional edges mapping from the supervisor

00:53:43.000 --> 00:53:52.000
To the other places. So everybody always goes back to the supervisor when they're done. Supervisor gets to decide what happens.

00:53:52.000 --> 00:53:57.000
Compile the graph and now we can invoke the team. And if we invoke the team.

00:53:57.000 --> 00:54:08.000
We can see here. That call hello world and print it to the terminal. The supervisor says, well, hey, that should go to the coder.

00:54:08.000 --> 00:54:27.000
And then the coder. Execute some arbitrary code and says that they printed it. And the output here is a little bit funny looking but mvp example and now we can actually set a recursion limit And say, write a brief research on PICAs.

00:54:27.000 --> 00:54:42.000
And that gets mapped to the researcher, right? And now the researcher. So the LLM is routing to different agents and then giving us the responses. And right now, both of these examples just go to a single agent. I leave it to you.

00:54:42.000 --> 00:54:52.000
To experiment with this more and come up with some prompt that maybe routes it more back and forth that requires collaboration of the agents, that kind of thing.

00:54:52.000 --> 00:55:02.000
But we are at or slightly past time. I'll do one last pass in the question thread here. And Ash, if you have anything you'd like to add.

00:55:02.000 --> 00:55:13.000
Yeah, so the only thing I'd like to add is that on Monday we'll be doing agents as our next class and then followed by getting into the nuances of different tooling for that as well.

00:55:13.000 --> 00:55:20.000
So this is obviously a completely new concept and I want to start off by saying when you're building your AI components.

00:55:20.000 --> 00:55:31.000
For your CRM, you don't have to use LineChain. We are using Langchain as an educational teaching tool because we feel it's a really nice way to illustrate what the possibility of a framework is.

00:55:31.000 --> 00:55:39.000
This means that you want to use llama index, go for it. If you want to use auto gen, go for it. If you want to use Langchain, go for that.

00:55:39.000 --> 00:55:47.000
You want to use line graph, go for that. And if you want to make your own framework for the individuals who are so inclined, you can do that as well.

00:55:47.000 --> 00:55:52.000
The goal is to understand that these are the design patterns of how agents are being built right now.

00:55:52.000 --> 00:55:57.000
And the goal of today's class was, do I understand those design patterns and can I make the basic agent?

00:55:57.000 --> 00:56:10.000
That's my first point. My second point is going to be that using Langsmith is really, really, really important When it comes to Langgraph or using Langfuse and some of the other frameworks.

00:56:10.000 --> 00:56:14.000
You really need to see how your agent is working in the background.

00:56:14.000 --> 00:56:18.000
Otherwise, it's really hard to picture and like troubleshoot where it went wrong.

00:56:18.000 --> 00:56:30.000
So that's the other tip I'll give. Use your Langsmith wisely. You will be able to see exactly the decisions your supervisor is making And you'll be able to see what's going wrong and what's working.

00:56:30.000 --> 00:56:36.000
And we had our traces going here too. Um.

00:56:36.000 --> 00:57:03.000
There are a couple of questions in the thread that I'll tackle real quick. Do you tell the agent what tools it has in its prompt? That's something we'll discuss more on Monday, but The tools are along, certainly the agent does have to know. I don't think you necessarily describe them in the prompt, but it could be wise if the tools are a little confusing. But you can also, there are certain approaches where the tools, because the tools are functions.

00:57:03.000 --> 00:57:10.000
The functions have names and doc strings that can also inform the agent what the tool does.

00:57:10.000 --> 00:57:15.000
And default case is state just grows and grows over time.

00:57:15.000 --> 00:57:27.000
Possibly. I mean, it can also just update, right? It doesn't have to grow that much. And realistically, I'd say it's the message history that grows far faster than the state in most of these examples.

00:57:27.000 --> 00:57:51.000
Although, I mean, the state has message history in it, but like, it's really we're talking about message history handling right which um the good news is that's a problem a lot of people have worked on. And so there are some good existing solutions to that for the most part. And usually the state outside of the message history is either not that big or is something that maybe you can also store elsewhere if it really is big.

00:57:51.000 --> 00:57:56.000
Like you could have a tool that lets your agents access a database, right?

00:57:56.000 --> 00:57:58.000
Anyway. Sure.

00:57:58.000 --> 00:58:02.000
Thanks, Aaron.

00:58:02.000 --> 00:58:14.000
I think I caught all the questions in the thread. Are there any other questions?

00:58:14.000 --> 00:58:17.000
Again, I do encourage looking at these other notebooks as well.

00:58:17.000 --> 00:58:28.000
And I will say that a large part of this is also you guys will have to study the syntax on your own. It's not easy. You're not just going to pick up line graph like You know, like this, we'll be going through it multiple times.

00:58:28.000 --> 00:58:32.000
But feel free to use AI. I always like to start with So AI is a very powerful tool to get at least a gist of what you're building out there.

00:58:32.000 --> 00:58:37.000
Hmm.

00:58:37.000 --> 00:58:49.000
And that's what your hiring partners expect of you, right? Feel free to turn on work smart and start studying Landgraph. Like that is something that we fully expect you having to do.

00:58:49.000 --> 00:58:55.000
And obviously we'll be adding more and more classes and getting more and more complex as the weeks go on.

00:58:55.000 --> 00:58:58.000
That's it for me. I mean, Aaron, did you have anything to add?

00:58:58.000 --> 00:59:02.000
That's it for me too. So thanks, everybody.

00:59:02.000 --> 00:59:06.000
Thanks, everybody. I'm going to just call out some of the scheduled events for today.

00:59:06.000 --> 00:59:14.000
There's a project to check in today with me. Just for anybody who's having trouble understanding like what AI components to build.

00:59:14.000 --> 00:59:26.000
Issues that you may be facing more from a design perspective Not from an engineering perspective. And then we have an office hours tomorrow to handle more of your full stack questions, errors that you're facing, et cetera.

00:59:26.000 --> 00:59:32.000
And then we have our logistics meeting tomorrow. To handle your questions about moving to Austin, all those areas.

00:59:32.000 --> 00:59:43.000
And then we have a talk tonight. With the founder of every his name is Kieran. He's awesome. So I just want to call out these events, make sure everyone's on the same page regarding these events.

00:59:43.000 --> 00:59:48.000
And if there's any questions. Please, please let me know.

00:59:48.000 --> 00:59:53.000
But I hope everyone has a great day. I hope you guys get a lot of coding done today. And I hope you guys study line graph a lot.

00:59:53.000 --> 01:00:08.000
So thank you again. And I'll post the recording shortly. Bye, guys.

