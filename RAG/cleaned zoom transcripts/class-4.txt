%START_METADATA%
Class_Name="Class 4 - RAG Metrics and Optimizations"
Class_Date="2025-01-15"
%END_METADATA%

Speaker A

All right, so first and foremost, two weeks till Austin. Kind of crazy or two and a half weeks till Austin. I. I do want to say that we are closer than all of you think. So we have that in mind. We'll have a meeting on logistics and move and all that stuff next week. So a lot of people have been asking me this question. How many people are going to make it to Austin? And my answer is as many as possible. So if you submit your assignments, if you complete what you're supposed to complete, then there should be no reason why you don't make it to Austin. What is P? I have no idea what P is. Maybe there's a. What's the conditional probability of us going to Austin, given that we're so now, you know, Wilson, that my statistics is terrible. The probability, I would say, is pretty high as long as you submit your assignments. I mean, I'm not going to put a number on it because I'm not grading all the assignments. Obviously there's Austin involved and a couple other graders involved, but we'll be very clear and everyone will reach out to anybody who's in jeopardy of losing their spot in Austin. And we have done that for previous students as well. If you haven't gotten any feedback from Austin directly or me, that means you're doing pretty okay. I mean, we were pretty like for week one and for submission yesterday. We just want to see that people are meeting deadlines. Right? So if we're going to be giving you a job right after this program, it's the first and foremost thing as an engineer is to meet the deadlines that are set for you. And so if anybody did not meet those deadlines, half of the people didn't even show up to the first class. That was pretty easy. Cut. Right now with the next project, the guidelines are going to become more and more stringent. We're going to ask you to sort of hit metrics on production grade or industry grade applications. We're going to ask you to use quantifiable numbers for all four projects. There are 160ish people left. Right. So. But that number is dwindling day by day. Pretty sure you guys are gonna have a really nice group of people that can work together in Austin. And I'm really hoping to and excited to see all the projects that come out, especially like having that experience and being together for so long. I feel like you guys will really build some connections. The only other thing I'll say about that is in terms of like, logistics and in terms of like who's gonna get in? We want everybody to come. Right. So if you do the work, if you're able to submit something, step one, my recommendation, submit. If a deadline is about to hit, submit whatever you have. Do not wait. Like we will understand like the whatever circumstances that led to maybe you not finishing the submission, but not submitting something at all is actually a very bad idea. And then we'll talk through next week. Yes, you can submit on non deadline days. Thank this great question, Ryan. If you want to submit earlier, that's totally fine. Starting for next week's project, we're also going to be sending out deadlines. Not deadlines, metrics. So I'll include in the project talk on Monday just like harder and harder guidelines to ensure that we're sort of moving in the right direction. Yeah, you can put the MVP submissions on the same submissions page. I think they're resolving a caching error because somebody was like navigating away from the page and then their submission would disappear. So they're resolving that now. But if you just submit, everything should be saved. Fine. In terms of next week's project, I'm not sure if I should tell you. I've been thinking about that a lot. Like should I reveal next week's project or should I wait till Monday? The hint is. Let me give you a hint. CRM. How about that? That's a big hint. Hey, Paul. We changed the some of the projects based on the hiring partners. And so we will change the. You're not going to be building Salesforce. We will change the sort of group of projects or the order of projects based on just how we feel the cohort is going and just how we feel like we can sort of challenge you guys more. Yes. We can build a common farm. Okay. We gotta, we gotta get sort of started. Oh yeah. I want somebody to build something better than Ilias's app. I'd love that. And then we can get raise some funding from him. Okay, so I'm going to start today's lecture because we have a lot to cover and then I'll come back to more questions in general. The other thing I wanted to talk about with you guys was imposter syndrome. So a lot of you might be feeling imposter syndrome right now because you've been put into an environment where everything was intentionally not well defined. You have to sort of figure your way out. You have to be resourceful. I will give you advice that my sort of manager gave me when I first started out, which was that if you're Feeling Imposter syndrome. That means you're headed in the right direction for your development. Because if you stayed within your comfort zone and you continue to just do the things that you are good at, you wouldn't. You're not actually developing, you're just honing in on skills you already have. The whole purpose of Gauntlet is to ensure that each one of you excels and becomes amazing engineers for getting something zero to one out there really fast using AI, Right? And to ensure that you have that engineering sense that Zach was talking about in office hours yesterday. So a lot of you are going to feel this way, especially as time goes on and the projects get harder. I just want to keep. I just want to start this class off by saying, keep pushing, keep pushing. The only way you get through Imposter syndrome is going right through it. So I wanted to say that and I wanted to direct that to everybody. Okay, today's class. Today's class is going to be on retrieval metrics and optimizations. So we've essentially created a simple RAG pipeline, right? But how do we take it to the next level? And there is a lot out there that we could be doing to make our rag pipelines better, but we want to give you guys essentially the starting line. What are some things that you can do out of the box to take your rag pipeline and maybe make it 2x better? Right. So we're going to be talking through some metrics on how to measure that, and then we're also going to be talking through some optimization skills on how to ensure that we can sort of use this to ensure that we can use this to sort of stay up to date with techniques that we can use to make the pipeline better within your Slack applications. So I'm going to be making a thread on Slack, so let me just do that now, or you guys can sort of post your questions. Actually put that question. Okay, so today's learning objectives are we're going to be understanding three retrieval metrics, mrr, ndc, ndcg and Recall at K. I'll walk through what each one of them are. I'll show you how to calculate them using some basic Python code, and then I'll show you the scenarios in which they are applicable. Next, I'll talk about two methods of ensuring that you might get better results in general for your rag pipeline, something called contextual embeddings and something called summary techniques. And then we also have the repo that will walk through. I intentionally haven't set up I intentionally haven't set up the repo at all. So I'm going to actually walk through the entire local setup because there's a lot of questions about like, what do you put the keys and how do you set up Pine Cone? So I want to do that live in class today. So what are retrieval metrics? So if you guys remember, the first sort of class that we sort of started out with was a class that Austin was highlighting a couple of sort of spiky POVs that members of the hiring team have. One of them was something called QC or quality control. And so QC for AI is a big concept where we're ensuring that the AI output matches certain criteria, right? We're ensuring that we have metrics associated with this output and that we want to continue pushing and ensuring that AI is producing output on whatever category, whatever criteria that it's supposed to consistently, right? And so building this level of a QC system for your different AI components is very important because this lets us track, hey, is this really working in the direction that we want it to work in? Or is this, does this need to be upgraded? Does this need to be changed? Does this need to be iterated on. I think a lot of people think of AI as a black box. You put things into the black box and, you know, output comes out. But we need a real nice way to measure, hey, what was the output? Was it accurate, was it reliable, and is it consistently producing this? When you think about production grade or industry grade applications, consistency and accuracy are extremely important, right? You can't have AI randomly hallucinate whenever it's trying to do something, a certain task that you give it. So if we were to think about this QC system, there's three areas that we can sort of walk into, and those are the three metrics that we're going to explore. But they're going to give you concrete scores to tie your retrieval pipeline to. So I'll know that my recall is at this, I'll know that my MRR is at this. So this way I can say, okay, maybe this is one implementation of my RAG pipeline, maybe this is a second implementation of my RAG pipeline, maybe this is my third implementation of my RAC pipeline. I have quantifiable numbers for each of those implementations on how to decide what the best route forward is. So there's three key retrieval metrics. So the first one is called mean, reciprocal, rank, which essentially means how quickly can I find the first correct answer. So there's, let's say there's like a factual tidbit, like a date or historical value or if there's some sort of information that is specific to some type of user, how quickly can I retrieve that information from a vector database? And this is really nice for like, hey, I want to do some sort of Q and A agent or Q and a support bot or Q and a support chat. And I want to figure out key information from documents. How can I grab the right piece of information as quickly as possible? So that's what MRR measures, how quickly your first response is the exact response you're looking for. Next is the normalized discounted cumulative gain. So it's a huge name, but we can just call it ndcg. It sort of evaluates like, hey, this is a ranking of the results that I'm expecting, but this is the ranking that resulted from the actual retrieval. How close it. How close was it to the ideal? Right. So I want to ensure that. Let's say I'm getting the top reviews on Yelp, right? I want to ensure that ranking comes accurately, but let's say it doesn't. So what can I do to make that better? So it associates a score with getting the ranking correctly. Next one is called recall. And at K. So at K is just the number of items that you're pulling from your vector database, right? So this could be 5, this could be 10, this could be 20. So within those 5, 10, 20 items, how many are actually pertinent to the query that we're getting? So this is very important because it shows the coverage, right? How much is my retrieval pipeline grabbing across all these documents that is related to the specific topic? Okay, so I just checked. Let me just check messages. Great question, Joshua. Mr. Can be time. But in this scenario that I'm going to be show you that I'm going to show you, it's just going to be like accuracy. So it's going to be like, how were we able to get the first result at the right spot? Is this metric subjective? I'm not sure, Sebastian, which metric you're talking about because you didn't say which one. But I'm. They shouldn't be subjective. They should be based on the sort of the calculation we're doing. And I'll walk through specifically what's happening in the background. So I think we'll come back to your question. Okay, here's where my disclaimer comes. There are multiple ways to measure rag. You guys can sort of find 80 some odd metrics. You know, I just made that number up. But there's a lot of metrics out there for you to start measuring your rag pipelines. And you can even do it with user side metrics, right? How quickly is the user getting the answer? Did the user like the answer they were getting? Right now we're going to be talking about retrieval specific metrics, but there's no single perfect metric out there. It's up to you as the engineer to make a decision on what metric should I start with what is a nice way for me to measure the type of retrieval that I'm doing? And what metrics can sort of imp. Sort of complement my mvp? Right. It's really important not to choose too many at once. So a lot of the questions I've been getting is like, hey, I'm stuck here, I don't know what to do next. You do not want to be stuck in analysis paralysis. Go to ChatGPT, go to AI and just ask it to pick one. Right? It's really important for you to start implementing a solution and trying to see if it's headed in the right direction then to decide between x odd number of metrics. I'm going to show you three today that you can start using now. But in the future, let's say you find another one. Let's say you want to try something else. Let's say it's these three metrics are not working for you. We'll talk through other metrics that you can use for your QC system to ensure that the retrieval pipeline is working perfectly. But again, I want to make the disclaimer, there are so many metrics out there, and you as the engineer will sort of build up that sense on which metrics to choose from. Right now we will start with these three. So mean reciprocal rank. When should you actually use this? So let's say you have a correct answer. There's one piece of fact that you need to find in your vector database and you want it to go searching for that. Mean reciprocal rank is a really nice way to figure out if that's working properly. So that could be a fact, it could be a tidbit, it could be a specific passage, it could be a specific point that you're looking for. And what it does is it calculates how many tries does it take for the first correct answer, right? So let's say we're doing a bunch of retrievals and I'll show you a visual in just a second. How long does it take for how many tries does it take for me to get to that first correct answer? You can also do how long instead of how many tries? But today we're going to be talking about how many tries. And so let me walk you through the calculation of that and come back. So here you'll see on the right side is a visual. The first on the right side is a couple of queries that I've made, and it represents a query we're making to our vector database. So let's say we're trying to pull five items. The answer we're looking for is the green box, and then you have the other blue boxes. So in this scenario, what we're essentially doing is we're taking the number one and dividing it by the position of the correct answer. So in the first query, it took us two tries. In the second query it just took us one try. And in the last query it took us four tries. Then what we do is we average these numbers out to then get our mean reciprocal rank. What this means is we get the average number of tries that it's taking across multiple queries to get how quickly we're getting to the correct or the most viable answer. The goal for this, as I outlined in the slide previously, is could we get this number to be 0.8 or higher? Right? So right now in my example, I'm doing three queries. That's not enough queries for you to measure. But let's say you were doing 100 some odd queries. Let's say you're doing 200 queries. You can calculate the score for each one and say, hey, which one is it? You can also do this with another sort of ground truth data set. Right? So you know, hey, this is what I'm looking for. This is the doc I'm looking for. And you can test to see if your retrieval pipeline is actually hitting an mrr. That's high enough. Okay, let me go back to Slack. Yes, you can use multiple methods. That's no problem.
Speaker B

Yep.
Speaker A

Lucas, I'll talk about how to improve it. How is the relevancy? So you are the ones that you can create a test database to decide whether to figure out the ones that are relevant. So you usually have a human decide, hey, the, the second one is supposed to be the most relevant. The fourth one is supposed to be, yes, you need a ground truth data set for, for Mr. A ground truth data set is just a data set that's created by a human being that tells you, hey, this is the, the documents that are relevant. And so this way you can compare to the output that's coming out. Yeah, Brett, you'd have humans in the loop to determine what, what Is what is relevant? A great question, Joshua. For the next project we will actually be using agents. We might not be using rag, but for this project, if you wanted to do this, what would happen is we would give you a ground truth data set to compare. Great question, aj. The MRI is a metric for retrieving out of the vector database. It is not for the LLM or in tandem. Back to the slides for a bit and then I'll come back. Okay, next one is. So when should you use mrr is probably the great question. I'll come to your question in just a second, AJ. Mr. Is a really nice way if you want to get the right answer fast, right? So let's say you have a QA support bot. Let's say you have an FAQ bot. Let's say you have something that where you need to pull a specific piece of information quickly and you want to do that in less amount of tries. Then MRR is a really nice way to measure that. How do you make this better? Is a question that I got and I'll show optimizations in just a second or two or three ways on how to make this number better. But this is a really nice way to also compare. Let's say you have three or four different RAG implementations. Maybe one is using a framework, maybe one is natively in Python, maybe one is using a third framework. How do I compare and figure out which RAG pipeline is working best? Okay, let me see if there's more questions before I move on. In real testing, should the human selected truth also be conferring in Canada? Yes, great question, Anthony. You should have multiple human beings figure out what your data set is. Again. It doesn't have to be. It doesn't have to be super like straight, like you know, like too many people involved. Like, you know, getting 100 people to review something can actually become crazy. You can even have something generated by AI and it could just be fake synthetic data. And you can just have that as a way to ensure that your retrieval pipeline is heading in the right direction. I don't want all of you to sort of get caught up in the setup of everything. It's more about understanding how this works in the background and then seeing which ones you can apply to your RAG pipeline. Yes, I'll get into major ways in just a second. Let me get through all the metrics. Okay. All right, so the next metric is ndcg. So this is important when the ranking of something is important to the output. Right? So let's say we were using reciprocal, reciprocal rank Fusion. This is something we introduced in class on Monday where we're ranking the results from multiple queries and we care about the order in which it's being ranked. Right? So this could be. Imagine it's like restaurant reviews and we're giving five stars, four stars, three stars, two stars and one star. We want the five star reviews to come up first. Is that working properly? Is that not working properly? Are they sort of put in different areas of the ranking? Are they missing the mark? So it's important to understand, like, hey, if I'm doing a sort of a retrieval pipeline where the order of things matters, then I can use this as a way to see whether or not I'm close to the expected order. Okay, so for example, here, let's say you have like a five star review and then you have four star reviews and three star reviews and two star reviews. This is the similar to the restaurant scoring system I just talked about. It would be incorrect if the five star review lands at the end of the list. Right? Another way to think of this is let's say we're searching for data science courses in our vector database and the first course that pops up is Introduction to Data Science. Okay, that's irrelevant. That should be the highest on the list. And then on what should be the least highest on the list? Well, that should be web development with the relevant score of zero. The way you calculate this score is you have an expected ranking. You can check that expected ranking with AI, you can make the expected ranking with human beings and see, hey, when I'm retrieving these things, are they coming back in the right order? Whether that order is relevant score, whether that order is something else. It's important to understand are these things coming back in the order that I expect them to come in? And so what scenarios would it be important for us to use this score? So let's say we have a recommendation system. Let's say we have a system where it matters the order in which something is coming. So the movie streaming, for example, maybe it comes in Ash's most favorite movies or the ones that we expect him to like most. Maybe it's some sort of research database where we're trying to collect topics, right? Which topics are associated with which papers. In this scenario, when we're pulling from the vector database, the ranking matters. The ranking is an important way to understand, like this is the most relevant, this is the least important. And so that ranking in these scenarios is a really nice way to sort of be tested using ndcg. Okay, last one. Is called recall. Recall just means that hey, I am retrieving k number of documents, right? So I have my vector data store and I'm going to grab 10 documents, I'm going to grab 15 documents, I'm going to grab 20 documents. Of those five, 10, 15 or 20 documents, how many are relevant? How many of those documents actually correspond to the query that I have? This is important to sort of understand coverage. Let's say I have a ton of data in my vector database and I'm pulling like 100 vectors, right? Of these hundred vectors, which ones are actually usable? And if they're not usable, then I'm just pulling for no reason, right? Then I'm associating data that shouldn't be associated and then I'm doing extra legwork to figure out what are the relevant vectors. So this is a really nice way. I have a vector database. I grab k number of vectors. Can I figure out how many of those vectors are actually truly relevant? And the goal would be that you want this number to be high as possible. So here's an example. Let's say you have, and I've just used like simple images, but obviously this would be text based documents and vectors. But let's say you have all these images and there's two that are very important. So two out of the actual k value would make it that there's only 1/3 or 0.33 recall at K because only 0.33 of the vectors that you retrieved were actually corresponding with the initial query. So this is very important in a lot of aspects. We don't want to be sort of just building a system that pulls willy nilly, just pulling random items that may not be relevant or may not be too relevant to what we're searching for. So another nice way to figure out or associate a quantifiable number is recall. Okay, I will go to questions. The last thing I'll say is, let's say you're thinking about medical research. Let's say you're thinking about patent searches, legal documents. This is a nice way to figure out if a nice place for recall to sort of fit in. Because you have so many legal documents, you have so much medical research, right? But you don't want to just be pulling willy nilly across like 200,000 documents. You want, you want to figure out how can I make this search more precise? And a really nice way to sort of do that is with recall. Okay, let me just look at Slack. Yes, there is a whole business around some of these metrics that's true. That's a good point. Sometimes arbitrarily, some of these metrics can be assigned incorrectly. So it's important to have humans in the loop to ensure. I'm confused. For instance, why not just maintain. Great question, Aidan. Vector databases are important for when you're searching through a lot of unstructured data, documents, messages, et cetera. But that doesn't mean you can't pair it with a regular database. So let's say you have some document or message that you found, but instead of having to go in and go get the vector database because it's not working properly, you can definitely connect it to just a regular SQL query. Great question, Sebastian. Like, obviously, different number, different queries have different numbers of documents. I think that's a sort of a. You have to go out and try it yourself in terms of the scenario you're using it in. So in this scenario, I what I would do is I would try to get ensure that more than half of the retrieved vectors were actually relevant with the query to start out. And if you believe that you need to do that, increase that more, you could do that as well. Great question. The scores on the metrics are associated with a couple of things. For mrr, the score is associated with how quickly can you get a relevant answer. So, meaning, like how many vectors does it take for you to get an answer that is relevant to the query that you have? The next one is like the ranking, right? So NDCG is the ranking of the actual responses that are coming back. So can you rank them by relevance, can you rank them by other key criteria? And if you're able to rank them, are they coming back with the associated rank you had before? The next one for recall is just to make sure that everything that you're retrieving is in the same sort of problem space industry area, or it's actually related to what you're searching for. This way, like more than half of the vectors that we're doing should, in my opinion at least be a little bit relevant to what you're trying to search for. You shouldn't just be getting random vectors all over the place. Yes, you can attach metadata to vectors. User is spot on metadata. So there's a huge metadata object that is associated with every vector. In fact, we'll go into the Pinecone vector today and I'll show you the metadata object. You can add more fields to this as well. And another way to do it, there's other popular vector databases, MongoDB Quadrant, where you can have like A ton of vectors. You can even search by the parameters or you can even filter. Could be wrong, but I believe the photo shows the relevancy score to how much. Yes, yes. So every time you do retrieval and do the vector search, it is all compared to the query. So, Anthony, vector queries and SQL queries are pretty much the same price. I mean, there's not a significant difference in price. But again, that could be something we check and it depends on what you're using. So, aj, great question. The similarity search algorithm is ranking each of the vectors that are retrieves based on how similar they are based on the cosine angle. It also obviously data quality is always foremost. Like if you're putting terrible data into your vector database, the output will be bad. So one of the easiest way to sort of make a rag pipeline better is to add better data to your vector database. That's obviously always going to be true. But it's also important that we understand as you're saying, you can change the embedding model, you can change what vector database you're using. And we want to give you methods to quantifiably measure how to figure out which of the configurations should be the one you move forward with. Okay, we are running out of time. Okay, so before I move forward, I want to say there is a repo where I've taken some time to, well, I've taken some time to use AI and to some, some example code I had in the past and some code that we've used in sort of other consulting projects and create some sort of calculators using Python for each of these. So I'm going to walk through them real quick and then this is available to you in the gauntlet repo. So as we talked about, when you're, when you're calculating mean reciprocal rank, there is some sort of ground truth, meaning this is the expected. These are the relevant documents, I expect. So here you'll notice that in our function we have something called ground truth where we're saying that these are the relevant documents. And then you'll see that you can feed in the retrieved documents that we're getting for each query and figure out when was it able to finally get one of the documents in the ground truth. So that's what I walk through and it does the calculation as well, the same thing. One divided by the number of tries and then averaged out to the number of tries. Okay, so this one what we're doing is we're listing the number of sort of relevance scores based on some information that we give it already. So for example, I have a dictionary called Relevance. And the relevance dictionary has the doc with some scoring metric, right? So 3, 2, 1, 0. And what we're able to do is compare the output from a vector database and see if it matches the relevant scores and if it's ordered correctly with those numerical values. I think this is just to illustrate how you can go about calculating this and how you can do it programmatically. Obviously you can feed this into AI and sort of get it a more applicable version for your applications. But this is a really nice way to understand the concept, right? The concept is I know the relevancy of things and documents. I sort of outline that in a dictionary and then I compare it with what's being retrieved from my retriever. Finally, it's recall. And so for recall again, it's pretty similar to what we did previously where we defined the relevant documents first and then we compared to see how many of the retrieved documents were in the relevant documents. So this is a nice way to figure out, okay, I'm getting X number of vectors back from my retrieval pipeline how many of them are actually relevant to what I'm actually trying to do. Another thing I'll add to this is I don't have any LLM calls in here, so these are just example calculators for you to use. But you could also send some of this to an LLM. So instead of having a relevant dictionary where it shows that humans have picked all the relevant documents, you can have an LLM decide whether or not this is actually relevant. And you can use an O1 reasoning model, you can use higher grade models to decide whether or not your retrieval pipeline is functioning properly.
Speaker B

Can you repeat that last bit about using Hired?
Speaker A

Yeah, thanks. Yeah, great question. So let's say, Marcus, that I have, you know, some, just some retrieved vectors from a series of queries that I have. And I saved that somewhere, maybe in a CSV or whatever. What I could do is maybe because right now we're using an embedding model. Usually ADA002 is what people usually use for OpenAI. And so, or if you're using one of the ones out of Pinecone, there's still not as powerful as 01, right? So not as powerful as 4.0. So what you could do is send it, send the list of retrieved documents and what you're trying to look for, how you're determining relevance and ask the LLM to say, hey, is this really the order in which these documents should be relevant? Or what is the number of documents that are relevant in the retrieved documents that I have. So this way the LLM or the reasoning model can be the human in the loop or take over a lot of those tasks. Obviously that'd be a little bit more advanced and I would start with a human being. But in the future you can probably use a reasoning model as well. You can even store your vectors. Yes, you can store your vectors in a postgres database fairly quickly. You can do that. Inside of us, there's also something called PG Vector. And so it's really important to understand that you can easily do this in a no SQL or a SQL environment. Oh, great question, aj. Those three are three things that you can do. So I'll say that clearly. Let's say. So you've picked. I wouldn't do all three scores at once, but let's say you picked a recall, right? You want to make sure that your recall score is above a certain threshold. Let's say it's 0.5. What I would do is these are all the components that you can configure to ensure that your recall gets better. The quality of data 100%. The embedding model. There are so many embedding models out there. There's open source embedding models like Nomic, there's foundational embedding models like OpenAI. So you can switch those out and see if they're going to be performing differently. You can change the vector database, as AJ is saying, Quadrant Pinecone, a regular postgres database. You can even use Clickhouse on aws. Right? There's so many options for configurations. And so as engineers, you need a way to figure out which configuration is the best. After this, I'll give you two more examples of ways to sort of make this better, which is contextual embeddings and summary lookup. But the one thing I'll also talk about, and I'll share some articles and stuff on this, because it's different from for each vector database, but the metadata object that is associated with every vector is very powerful. Let's say I have a huge vector space and each vector has a metadata object with a certain series of parameters. I can actually reduce the size of that vector space using one of those parameters to make my search faster and more accurate. So let's say I have a bunch of Harry Potter books in my vector database and each of the filters have the title of the book. Each of the metadata objects have the title of the book and the chapter that they're associated with what I could do is use the metadata object to filter to just the Harry Potter and the Sorcerer's Stone and ensure that I'm only searching that book of vectors to get a response back. So it's really powerful in reducing latency increase and increasing accuracy. Prompt engineering is also a great lever. So Mikael, that is spot on. Like if you're getting context, you need to frame the context correctly and you need to make sure that once the context is framed correctly that the stuff that you're putting into that is proper. So I think it's important to understand that what are, what's everything I can do to make my RAG pipeline better that is not some, you know, new optimization. Change the database, change the embedding model, do some prompt engineering, change the prompt engineering technique that I'm using and increase the quality of data that I have. Add metadata to each of the vectors to ensure that I can organize them properly. And then we'll talk about two more as well. Can these calculations be made in JS100 Adam? You can probably just feed it into cloud or chat GPT and do it that way. Kale, that is a great question. How do over optimization is always a problem. Over training is always a problem. How do you improve the MRR without, you know, so what if it doesn't actually improve in actual life? These are just test demo scenarios. I think when it comes to like production grade industry applications where RAG is being used, you sort of will also have the assistance of a data scientist, a data engineer and you sort of have to sort of make different data sets, like a verification data set, you make a training data set and you make an evaluation data set. All of these have to be distinct and there's a process that you follow and you almost have to conduct an experiment with each of the configurations. But again, I don't want you guys to play in that right now. Maybe that is something you guys get to at your jobs, but right now it's an introduction, right? These are the metrics and the values the goals would be. Do you understand them? Can you at a basic level implement one of them? And do you understand how you can change the configurations to make your RAG pipeline better? Yes, we will be in future weeks using re ranker models. How can you. Should we make K large and filter off rollings Curry squares? Okay, so Drew is asking how can we optimize K or the number of chunks that you. You sort of grab? Frankly speaking, Drew, it's trial and error when you're first starting out. Like I Remember we were doing a project once and we were starting out with 5, then we tried it with 10, then we tried it with 20, and then each one of them we would test out and see with the stakeholder on the project, like, is this actually relevant? So almost doing a recall calculation that is non programmatic. So at the time we were thinking of using recall, but we didn't have it ready programmatically. So we had human beings come in and do it properly. I think it's very important that you have some level of verification going on because that level of verification will ensure that what you're building is made for production, it's made to last, and it's consistent. So the answer to your question is trial and error. I'm sure there's better ways to do it, but that's the way I've seen work almost every time. Great question, Anthony. Why would the database change something if the embedding model is the same? Vector databases have a lot of features, a lot of search functionality, a lot of similar research algorithms. They have a lot of different algorithms that you can use, Euclidean cosine, et cetera. So changing the database can actually alter some of these scores because the databases are built differently. For example, Quadrant could be built on prem within a server. It doesn't have to be cloud based or using SQL versus no SQL. So if you're using PGvector versus Mongol, like what is going to sort of get you to the right result? So I would test it, in my opinion, especially in the future. But right now, let's say you start with something and you have a threshold for how well it needs to work. And maybe you want to make database the last thing you change, then that's okay. You can start with the embedding model and you can start with the data itself and the prompt. Yes, you could. Yeah. So Brett makes a really good point. It's not just about adding better data, it's also about removing bad data. So people always get that wrong. Like you need to remove the data that is sort of taking away from what you're actually trying to search for. That might be a really big issue when it comes to ensuring that your data quality is high. Okay, I'm going to come back to the questions and I will make sure each one of them is answered. But I want to make sure that I walk through the two optimization examples that we have. The first one is called contextual embeddings. This is a really, really straightforward way for you guys to sort of make your vectors a little bit Better, right? And perform at a better level. And so in this scenario, let's say I have two words, mouse and rodent, mouse and mouse. They both refer to two different things, but I don't have the context to associate which vector goes where. So a really nice way is add some context to the vector and people would say, does that mean I'm just increasing the size of the chunk? No, that means that you're adding actual text that corresponds with this. For example, let's say I was trying to make a vector for this line of code right here, this line of code. Code called calculate recall at K. Right. So what I would do is I would make a vector by saying the following is a line of code in the recall k py file in the class 4 retrieval metrics repo. This is what it says. And then the line of code I've added context to where the vector might be, what place it might be in, what it's referring to. This context is really valuable and can be synthetically generated by an LLM to ensure that each of your vectors isn't just random sentences in the cloud. Right? It could have parameters associated with it, it could have additional context associated with it. It can refer to a bigger document or a bigger piece of information. So if I were to talk about, how can you do this? You can talk about what chapter it's associated with, what section it's associated with on a document. For example, this contextual information is on slide number 17 of the Class 4 slide deck, right? All of this is important information and context. So when I come in as a user and I chat with your RAG system and say, hey, what's on slide 17? The rag system can actually grab the relevant vector and actually answer that correctly. And this works well with metadata, like the title of a project, the author of a book, timestamps. You can even do neighboring text. This is really powerful. So you can see like, oh, this was the text that came before and this was the text that came after. Now you're going to ask me, how do we know how much context to add? What I would do is start off with one or two pieces of information that you want to add to the vectors to make them more relevant. And I would see whether or not your recall score gets higher, see whether or not you're getting answers faster to the user. And so it's. It's definitely a trial and error here, right? You want to see and test, hey, if I add this to the system, does it get better? Okay, going back to questions and then finally, we'll do our summary lookup. Is there a reason to store vectors of different types of in. No, you don't. There's no reason to store vectors of different embedding types. If you're going to switch embedding types, you should switch it for all your vectors. The indexing algorithms are different for full text search. There's performance. Oh, you're just answering. Yeah, I agree. Yes, Joshua, the first thing I would focus on is recall. Just ensure when your RAG database is pulling things that more than half of the vectors are actually relevant. This is not okay, so are we expecting. Great question, Steven. I'm not going to specifically ask you to implement a lot of these features. The one thing I will say is the summary lookup feature, and I'll talk about this at the end of class, is a really, really nice way to sort of search across documents in your Slack workspace. So Whether that's images, PDFs or anything, using summary lookup across PDFs documents is a really nice way to figure out okay, I'm going to this document and grabbing this information. So the answer to your question directly what would be awesome for this week would be if you added summary lookup with the documents associated with your Slack workspace. So Amir, the Harry Potter example would be the actual books. I was actually referring to the books, but in your sense. Yeah. So for in terms of your Stack workspace, Amir, if you. What I would focus on is recall and the thing that I'm asking you guys to implement or I would suggest implementing is summary lookup and I'll walk through the code for that in just a second. When do you do search? When do you. When do your search Introduce me. Okay, Spencer, you don't need re ranker access. You could do a lot of this without that access on aws. Okay, how does this apply to our project? Seems the question that keeps coming up the only thing you have to do for your project is consider one of these metrics to measure. I'm not going to check that directly. The one thing I would suggest for your AI component is using summary lookup to find documents that are within your your Slack workspace. Because you should be saving documents and uploads and all that stuff and be able to search them through using the summary process. I don't. Where do you add this context in the same text? You can go to. The context can be added in the same text or it can be the metadata. It can be both. You can run these tests on your local machine using some of the code that we offer, but in terms of like actually doing it on LangChain, LangChain does not automatically calculate for you. Lang Smith does show you the output so you can, you know, actually do a very quick calculation of what's going on. So when do we add context to chunks about. Yes. So Gary, what I would do is I'd get an MVP working of your rag pipeline and if you feel that there's your recall score is too low, I would consider adding more metadata to reduce the vector space and add context or add more information to the actual vector that is being to the actual text that's being vectorized. AJ Great question. For the clarification, the chat avatar can connect to the document one, right? So if you guys completed MVP today, you should be able to do a basic chat with your vector database. But to take it to the next level, summary lookup is a really nice way to sort of say, okay, I'm gonna go grab this doc, the avatar or the Persona is going to go grab the document and grab the information and put it out there. And so the answer is you should. Your chat avatar can connect to your summary lookup and do the document for you. So we're trying to give you guidance on how to approach your AI components, but again, if a lot of you want to sort of do it your own way, you could do that as well. How do you add context to your metadata, Sebastian? That just depends on if you just. There's a bunch of pinecone documentation on this. If you just search pinecone metadata, they have a bunch of documentation you can use. Yes, you can you stick with the vector database story you've already chosen. Do you attach summary of each chunk to each vector related to the chunk? No. Gary. So the summaries would be for huge documents. I'll show you in just a sec. Using the so what we're doing for summary lookup is we're making another index of all the summaries and then using that index to then find specific vectors in a document. So summaries are for documents, huge documents, PDFs, in case of your chat app, in case of your chat up. The documents I'm thinking of are like let's say you've uploaded something to Slack. So if you have, if you don't have the upload feature working, then summary lookup might not be the way to go and you might want to just focus on the messaging. But there's a lot of students who are able to upload documents and then save them somewhere. So if you have that working. This is a really nice way to go and find it. It. You can also do summaries based on individuals and all the messages that they've had and then go look up based on those chat histories. How frequently should be adding new messages? That's totally up to you. But yeah, I. I think it's important to update the messages so you have enough data to actually get your rag pipeline working. Document Lookup is not required for Monday, but it's important for me to push you guys in a direction on what components to add, and that's my suggestion. Oh, well, speed is a. Is a big win, Joris. So. So, yeah, so yeah, I mean, speed is a huge advantage. But I also think that from a user experience perspective, instead of having to go through all these vectors and maybe get the wrong answer, I feel like Summary Lookup just has a better user experience because of that speed. Okay, I'm going to do one last thing and then go into the code because we're running out of time. Summary lookup, what is that? So let's say you have a document so you can create chapter summaries of that document and you can create even section summaries and sort of pinpoint to the paragraph. So it's like I have API documentation, I can go into authentication methods and then I can go to OAuth implementation steps. That means I can sort of narrow down on the area, excuse me, on where I should be grabbing the information. So it lets me quickly get to the right portion of a document to ensure that I can grab that information and do something with it. In terms of your Slack applications, you guys have implemented messaging channels, threads, some of you even have the upload feature working. So if we think about all the AI features for your Persona, right? The AI Persona should be able to sort of chat with the entire workspace in terms of get me messages or give me a summary of all my messages with Gary. Give me a summary of all my messages with Marcus. When me and Marcus last talked, what did we talk about? All of these things are stuff that is out of the box available if you're vectorizing all the messages inside of the Slack workspace. But what else can you do? You can actually pinpoint and take all the documents that you've uploaded and use summary lookup to sort of chat with the documents. You can also create huge documents with all the messages that you have in a channel and then say, okay, I'm looking in the channel I have with Callum, and in that channel I want to find X, Y and Z. So I think it's important that these methods are really powerful for your Slack application. We're trying to say, hey, these are potential AI features that you can add to your AI avatar. Obviously you might get. You guys might be thinking of something else, but we want to give you some level of direction on where to go. Okay. Finally, I'm going to go to the repo and this repo is very similar to the repo that we had yesterday. And since in terms of setup, in terms of all the keys that you need, and so intentionally I haven't done anything, I'm going to walk through just how to do this from scratch. So I want to sort of resolve any questions that you guys might be having regarding initial setup. And also, it's looking like this class is going to run a little bit longer. So I do want to say, like you, I will be staying on to ensure all the material is done and completed. But if someone has to pop and then watch the recording later, that's okay. Okay, so I'm going to open up my terminal and I'm just going to go into my desktop and I'm going to clone the repo down. I'm not forking it, but you guys should be forking it. In fact, you know what, I'm going to. I'm going to fork it just to illustrate the importance of forking. All right, foreign. It's really important that you guys have your own forked version. Let's say you need this in the future. Let's say you want to reference this. I want to make sure that it's available to you on your personal account. Okay, so here we're going to copy this and we're going to clone it down. Oh, looks like I already had one saved from previous work. So let's go to Documents and let's come in there. Okay, awesome. And I'm going to open up cursor with the actual. Okay, this is the wrong repo file. Great. Question mark. When you're working in your team with your hiring partners, you, you will be able to do the branching, but in terms of best practices when it comes to open source software. So I guess that that's what this would be considered. You want to ensure that you're forking it onto your own. And then that doesn't mean you can't sort of open up a pull request with the original version. You can still do that through your forked version, but this just ensures that all the work that you're doing is confined to your Personal account. Yeah. So somebody did release the OpenAI key this morning, and that's okay. I did generate a new one. So I believe it should be working fine now, but we'll find out in just a second. Okay. All right, so I'm going to just follow exactly what is being said on the the readme. So I'm going to go to Pine cone and I'm I for anybody that's about to leave because they have to work on something else or whatever. We have another session with Zach today at 3et, I believe, and he's going to finish out the the live build using cursor and AI first development for the Hackalot project. So he shows his entire thought process. He's got to the point where you can make a submission for the hackathon, and I believe he's going to be adding timed competitions. And so the awesome part about this is we're then going to run a hackathon for you guys later on where we'll make some sort of prize, whatever, $500 or something, where you guys can sort of submit your things and sort of do like a side project that you might want to launch. So that's going to be great. Question. Can he. I mean, a lot of people were telling me they have a lot of side projects and they're entrepreneurs. We didn't do it the first week, Brett. This is just an idea that we had like yesterday as Zach just really wants to. So so far he's put in an hour and a half or so. He wants to put in a couple more hours and sort of launch it and show you guys. Okay. Can we implement RAG in the hack a lot? Yes, we can. So I'll ask Zach. So we'll probably finish up the app today and then we'll add rag to it later on. Later on, meaning probably tomorrow. Okay. So I'm just going to create an index here as instructed. Just. Just walking through it. I just want to make sure that everyone is, you know, doing this properly. And let's call it that it's going to copy this. Exactly. And normally I would use the cli, but, you know, I'm trying to show you guys how to do this properly. So here you'll notice I've used 1536. It's always going to be cosine. And I'm going to create the index. Okay, I've created the index here. I'm going to go to my API keys. Let's create a new key for class four. Okay. Notice how my ENV file is in the root directory of the folder. There's already a bunch of files here, but you want to make sure that your env file is in the root directory. So what I'm going to do is copy over my sample in the. Read me. Looks like I didn't add a sample file to this. I could fix that too. Okay, and let's put our key here again. For anybody who already knows how to do this, feel free to drop. But I want to make sure that I clearly show what you should be doing for project setup. And then we're going to create a second index here for chunk index. Then I'm going to grab the OpenAI key. Okay, let me just answer AJ's question directly. The doc is up to date. So when you're doing the Persona that matches you, I believe that requires a RAG pipeline. You guys can correct me if I'm wrong and use a different method, but in terms of grading, I'm looking for any and all AI features that you've added, and you will get credit for all of them. Right? So it's important for me to give you direction. So when you're building the Persona, you need a vector database of my messages. So if you don't have that, I'm not really sure how you're building that Persona, unless it's just based off a single prompt that you're using. But let's say you're using vector databases and RAG to build that Persona automatically using the messages for that individual, then you should automatically be able to chat with that Persona, answer questions, etc. The summary lookup is just an extension of that. Right now, I can have all the messages across everything, make it much faster. I can add documents to that. I can add documents that are associated across the board. And so, you guys, this is my suggestion to you that your Persona will become immensely powerful if it has all the messages associated with Austin Allred and all the documents Austin Allred has now uploaded onto Slack. In terms of how we grade it, right? First and foremost, it's graded based on is your AI component actually functional and working? Is your Persona answering me when I'm doing the Slack message? And is it going to actually search up something, give me relevant answers? So I wanted to answer that directly by saying the AI Persona is connected directly to rag. Yeah, can I jump in to clarify? I think the main point of confusion for me is sometimes when you're describing the Persona, you describe it as if we're talking to our own Persona and asking it to do stuff for us. Other Times it has been described as other people will DM literally my account on Slack, and it'll be an AI responding with all of my knowledge. Like, which of those two things are we talking about? I mean, they both. AJ if you implement either one of those things, you pass. Cool, cool, cool. So I want to, like, I want you guys to understand in, in terms of the AI components. We just want to see you guys actually try and get some of the AI components working. Right. At least for week two. I think the, the, the, the guidelines will become more stringent as the project gets on, the other projects come forward. But at least for this week, can you get your AI project from start to finish? There's some of you with ideas already, so feel free to go and push through on those ideas. But there's a bunch of you who have no idea what AI components to add. So I'm giving you the options. Right? So that's what's happening. I want to make sure that everybody has a direction. I don't want anybody to be stuck. And so whether you're chatting with all your messages, whether you're chatting with documents, we want to try to implement some level of AI components this week. But AJ I think it's a really good question and the answer directly, I would be happy with either this week. Awesome. Thank you. Sorry, I'm just reading the questions. Oh, like Joshua, I'm going to talk about setting up the summary lookup and actually running it and walking through the code. Yes, we can ask Austin to reach out to Han. Oh, Robert asked, in case of having all the user matches, why use rag? We have access to all messages running the rare. So, Robert, I would argue that RAG makes it. Makes it so that you can find meaning across messages and make a, a better answer than you would with without an LLM and without actually composing the messages together. But if you feel that that may not be necessary, I would still recommend using RAG as a way to sort of connect, make connections across all the documents that you have. Otherwise it'll just be sort of, here's everything that we have and here's what's coming up, like a simple search. Rather, we want the Persona to actually answer as if they, they were you. How's that? Go ahead.
Speaker B

Just to add on to that, a thought and something I've been looking at kind of myself. But rather than a just a slack onetoone message, imagine a team that's working together and they're using that as their, their main communication. More of a channel kind of environment. And then One of the team members is gone. If you've been in ragging the entire conversation and you rag the individual, then you have the entire conversation. However, back you know, it goes in history and you would be able to answer a lot of questions without having to do external research. Maybe there was a question six months ago about how did we apply this one particular thing and that person isn't even there today. Or maybe they're not with the company anymore. If you've ragged that knowledge, you have that. That Persona knowledge there from both as an individual and as a team. I think that would be super powerful as a team member, because I use Slack a lot. I mean, I was looking for a message from Ash just yesterday. That was only four days ago. And I have to scroll and scroll and scroll and scroll, right? So imagine you give them a. You have some kind of knowledge or a key or something or a trick, and you're like, what was that one thing? Well, rather than trying to scroll or trying to make a search that would work to get you that piece. Your AI already knows the answer and Ash is already in bed. But I can get his response because we have that rag. Just as a thought of where my mind is going on this kind of development.
Speaker A

No, I think, Zach, you're spot on. I think connecting messages across channels is going to be really powerful. Like, what if we have like a project gauntlet channel and like we lose all the messages in it? Well, then we can go search that channel and find that message quickly. So I think it's powerful using a vector database for this to answer Nicholas's question directly. Persona summary lookup, Document lookup. If you do any one of these things, any one of these AI components is working and functional in your application, you will pass this week. I'll be very clear about this. We are giving you so many avenues to explore here, and any one of the avenues that you get working and working properly will lead to passing. Stephen, I'll come back to the voice tips in just a second. I want to finish out the setup video and then I'll come back. Okay? Okay. I keep getting off track. I need to finish. Okay, here is OpenAI key again. This is for anybody who has any issues with local setup. I wanted to do this once and make sure that everybody had a path going forward. So you'll see I'm walking through each and every step, getting the keys, everything. Okay, Zach, where's the key button on Linksmith now? They changed it.
Speaker B

Oh, it's been a week since we looked. Of course they Changed it. I think it's under Settings now.
Speaker A

Okay. Oh, it is.
Speaker B

API keys. Yeah.
Speaker A

Yeah.
Speaker B

Don't get used to any stability in the Langsmith.
Speaker A

Yep. They're going to change it every time.
Speaker B

As soon as you know where it's at, guarantee it's changed. It was so bad. When we're doing our weekly classes, our other classes, I would go and check before class where it was at so we'd have it ready for class. That's how bad it is. Their service is great.
Speaker A

I feel like it also, like, with their library too. They'll change where all the libraries are located and yeah, everything across the board they'll say like, they'll change like specific methods on what classes they're in. And anyway, so everyone and community, you.
Speaker B

Write something really cool and people love it. You're using it today and tomorrow it'll be gone because they've absorbed it into their maintain it requires good research skills. It's a great tool to figure out what crash.
Speaker A

Just waiting for my Docker desktop to start working. Give me a sec. All right, we got the Docker desktop working here. And what we're going to do is just run some commands. These commands are already in there. The read me. So copy that. All right, so our containers are being built. You can see it in a second. I'm just going to keep following the guidelines here.
Speaker C

Can I ask a quick question about Docker?
Speaker A

Yep.
Speaker C

So we've got Docker running with this project. I've got my Supabase running in Docker. If I don't shut it down, is it just going to be running in the background on my computer forever?
Speaker A

Yes. You have to shut it down manually. Usually what happens is. Okay.
Speaker C

Any tips for that, like making sure. Yeah, I do. Okay, so on top Docker Desktop, you'll.
Speaker A

See this little icon on top which should indicate the Docker running in the background. I would click, right click on that icon and just quit it. And that should resolve most of your issues.
Speaker C

Okay, but what if I won, like Supabase to keep running, but some other ones to shut down?
Speaker A

Oh, no, no. So if you want Supabase to keep running, then what you should like, say.
Speaker C

I have different projects I'm doing Docker in. Is it. Is it easy in Docker Desktop to sort of see what's running and according to what projects?
Speaker A

Yeah. So if you just open up desktop projects here and you can just click the trash button for the project, you're to delete or.
Speaker B

Or stop if you just want to turn it off. But you don't want to delete it.
Speaker C

I see it. I see it. Okay, great.
Speaker A

Yep. Okay, so right now we're just setting up our vectors for each of the documents that we have stored. More questions. This is happening. Actually, let me scroll on the Slack thread.
Speaker B

Can I chime in real quick?
Speaker A

Ash? Go ahead. Spencer, what's up? Yeah, I just wanted to say I was adding. I was pulling down these notebooks and.
Speaker C

Adding them into as markdown documents in.
Speaker B

My code base to give, like, Claude.
Speaker A

Something to go off of. Yeah, and I wonder if other folks are doing that.
Speaker C

And yeah, I just wanted to mention it because.
Speaker B

Oh, sorry, you need to add those.
Speaker C

Markdown documents to your git ignore file. I just, as I was doing that.
Speaker A

I was like, oh, this is a.
Speaker C

Way that people could leak their keys.
Speaker B

So just wanted to add that, yeah.
Speaker C

The keys are stored in plain text in those notebooks. And so if you, like, convert those to markdown and like, use that as reference for coding, that could be a potential way to leak keys. Yeah, I don't. Yeah, I don't keep the keys in my docs, but I also have a docs folder that is an Obsidian vault and I put that in the git ignore. So that's my strategy. But I don't put the keys in there in the first place.
Speaker A

I. I think with the keys. Spencer, you make a really good point. Let's just check again. So one of the biggest things that might be happening is when you're doing git add, git commit, and git push, we want to make sure we do a git status before we do a git add. This will show all the files that have been updated, and you want to pick and choose the files that you're. You're actually sending to GitHub. That's the first step. The second step is, as Spencer said, have a git ignore. Put any file that may have a chance of the keys leaking inside the git ignore. It's really, really important that we highlight that. And a way to sort of make sure that cursor doesn't update your gitignore. There's a cursor ignore file so you can put the gitignore file in the cursor ignore file so your gitignore never gets updated. Updated. So these are just some steps to sort of go about this. But the reason I wanted to, I mean, Spencer makes a really good point. When your key goes live, because we're sharing keys right now, it now is unusable by the entire cohort and so then I get a message, I then delete it, I then make a new one. And so I've been trying to keep up with that. I apologize if I'm delayed sometimes, but it's really important that we sort of get this practice in hand and to ensure that we handle keys properly. Cal, go ahead. Yeah, I was just wondering maybe like if one person did this, then we could just copy it to everyone. We could set up a, like a pre commit hook so that it looks at every commit and looks at your dot ENV file and if there's any matches, it just rejects the commitment. Yeah, I feel like there's. Zach, do you think that already exists? We can just use something out there.
Speaker B

There is tooling for that. The thing is, it's, it's a real, it's a real hassle to get set up. But most importantly, this I feel is a grow up moment, not a helicopter engineer moment. That as good engineers you need to know this practice inside and out. It needs to resonate when you do anything. And so it's, it's a burden to have to reset these. And you know, it's not a hard thing, but you need to mess up in an environment that it's not really important because if you're not, if, if you don't ingrain these by the time you get to a job and you do it. I've seen lots of, I, I work at, you know, I get hired as like CTO at a lot of companies for interviews and I can't tell you how many people have been replaced and not hired because they, they did not ingrain this process. And so this is one of those like, you know, I, we're kind of babying you in the sense that we'll reset it and nobody gets yelled at, you know, too much. It's annoying and not kind of of thing, but from your perspective, get it into, into your, your mental framework and then you won't make the mistakes moving forward and it'll matter in your job. We can set up tools, but there are so many different tools and so many companies I've worked for will not use that tooling. And so it's back to you. So I would say everybody just has to learn how to do it right. And you know, that process now, in four more weeks or six weeks, if it's still happening, I'd be chewing on somebody myself. But you know, from a general perspective, you know, when we work on our other engineers, our zero to one where we're teaching, you know, to be Engineers, this is. We expect that this gets resolved by month three, which for you guys would be about the end of week two. So, you know, in time and work comparison, you know, it's. Everybody has done it and will do it in their lifetime. You know, we often joke that if you haven't wiped out the company database, you're not a real engineer yet. Because every engineer of any amount of time has done it or done some serious damage. Hopefully there's a backup. I've done it. And I will tell you this too. Part of what we want you to do is we are thinking through this. AI is really bad about moving keys around. And I had to happen to me just the other day, Claude went and just decided it was going to move it somewhere else. And I got notified, killed my keys. I had to go redo it. Okay, so it's going to happen in life, but we want you just to be very vigilant in your own mind frame from it. And so the answer to your question, you know, call them yes and no, but learn it and learn it well. Now, if you write something yourself, that does it for you and you want to share it, I have mixed opinions on that, but for yourself, absolutely. But just understand that this is real world and it is something you're going to deal with for potentially the rest of your life, especially where you will most likely end up. You're not going to end up at Amazon on a project that has 100 engineers that have been there forever. You're most likely going to end up in a company that says we want to work quick. You, you've learned this skill. We want to develop, we want these new things and they're not going to have those systems set up. So pretty good chance the better you are at it, just better employee you'll be in. The better engineer you'll be. My thoughts.
Speaker A

Okay, thank you guys for that. I want to. And thanks, Zach. I think that perspective really matters. And I will become harsher as the weeks go on because I could figure out who shared the key. So I want to make sure that right now I'm pretty lenient. I'm lenient with a lot of things, but as the weeks go on, I won't be.
Speaker C

You'll tell us if we leak the key, right? Because I want to know if somehow I did that.
Speaker A

Maybe I'll just put a. I'll put a pre commit hook and the pre commit hook sends me the GitHub of the individual that's about to leak a key.
Speaker B

What are some common ways Keys could.
Speaker A

Be leaked that aren't obvious because I think maybe I leaked my own personal.
Speaker B

Key because it seems it got regenerated. But I checked my database, everything, and. Or my GitHub and it's not there in the commit. But I'm just unsure like what may have. What might happen. Like I was looking online, maybe there's a commit history. Just any ideas or how to figure.
Speaker A

That out would be good too. I think we will, 100%. I'll answer your question, but I'm just going to walk through the code first because I keep putting it off. I'm going to finish the code and Marcus will come directly back to your question.
Speaker B

Sorry.
Speaker A

No, you're totally fine. You're totally fine. Don't be sorry. Okay, so what we've done here is we've created a summary index. So there's a bunch of documents in our. In our code base. And these documents correspond with large PDF files that correspond to books. And you'll notice that what we've done is create a summary of each of these books and vectorize it. Then we have a second database or a second index. Excuse me, which is the. Actually the chunks. So this, this is now chunks from the original books and that we've now vectorized and added. You'll notice that we've added metadata, which is the source. So it tells me which book the chunk or the vector is coming from. In terms of code, what we're doing is we're going to first search through the summary and find, okay, what's going to be the most relevant book to get my answer from? And then we're going to search through the chunks to find an answer directly to the query. So this is really powerful because instead of having to search through all of the vectors across all of the books, I now went to the exact document and then I was able to use the summaries to find where possibly my answer could be. And then I was able to then get an answer using an LLM and direct passages from that book to make my sort of life a little easier, a little bit faster. This should increase the quality of vectors that you're pulling and it should make the speed at which you're pulling stuff much faster. Okay, I just want to run through that. This was how we set it up. I'm going to now take questions, obviously. And the last thing I want to talk about is what are your expectations for the end of this week? The expectations for the end of this week are working AI features on top of your Slack application. This could be a Persona that answers for you. This could be a RAG implementation that is able to. You're able to ask questions through regarding any message across the workspace. This could be document lookup using summaries where you can ask questions to specific documents. Any of these AI features working seamlessly in your Slack application would give you a passing grade. If you want a higher grade, then adding multiple of these AI components would do that as well. My recommendation for the Persona is that you use rag. You RAG all the messages that you have across channels and your Persona is able to pull from this vector database and answer questions directly if you want to. Then also add in documents because a lot of you have this upload functionality. You can then add documents, vectorize those and include them a part of your vector store. And you can use summary lookup if you wish to make it more optimized. I think these are all options for you. These are directions that we're trying to give you. As long as we see a working AI feature, any one of those three or something else that is on that same level, you will pass for the week. And you know nothing is breaking. You're making the video. Everything's submitted on time. Okay, that's all I wanted to cover. I'll go to questions now and then. Zach, do you want to answer Marcus's question or I can go first.
Speaker B

Are we talking on the question of accidental ways to leak ENV stuff?
Speaker A

Yeah, yeah.
Speaker B

It's kind of threefold, generally speaking here because it's a learning environment. We use public repository and that's always a problem. Right. If it's a private repo, about 90% of that is not as much of an issue anymore. But in a public. Absolutely. Make sure your gitignore is ignoring your ENV file and use Your env files 90% of the times that it leaks. It's because one of those two things didn't happen. And I don't know, I haven't looked at all the ones Ash's head, but I've looked at a couple when Austin was talking about it the other day and that was the case in both of those. Just that the ignore was not properly ignoring ENV or the ENV wasn't even there. The third one would be AI likes to use inline variables, so never pass your variables as much. If you set through my deal, we did the variables manually because we didn't give cloud access. Now it is indexed and you can use the cursor ignore, which is great, but if Claude doesn't index it Directly when you pass it, it's much less likely that he's going to put it around. He, they, she, whatever Claude is. I don't know them, but whatever Claude's assumption is. And so don't pass it and then make sure you ignore it. And at least the first few times that you do your always use git status, double check that you don't see it and then just get good at looking at what Claude is doing, what you're doing in your AI, whichever one you're using, and just look because it's going to give you the plus minus differential between added code, removed code, and just keep an eye on it. If you do that 98% of the time, you'll be fine. There's a couple of times in life where it'll be really, you know, something will happen, but that will solve almost everything. Go ahead.
Speaker A

Yeah, I think in terms of like accidental ways, you can probably also ask AI, But I think what Zach mentioned is true. We just have a scenario in which we're also dealing with public repos because right now I've been trying to add everybody's GitHub to our GitHub. Org and I only get like 25 invitations a day. So I'm trying to figure out how to fix this. But in terms of like how do, how we, how we resolve this going forward, I think we'll just put all the repos in our Gauntlet AI organization, obviously that you can keep a version for it for yourself as well. I'm not saying that you can only keep it in Gauntlet AI, but this way we'll, we'll be able to manage keys much better. Okay, so I just want to answer some questions on the Slack Benji, the, the metrics that Pinecone gives you, those are automatic that from Pinecone, they're not connected to any code that we ran. So those are metrics that Pinecone automatically calculates and Gary. Yeah, it's a two, two step search in, in what I did, I searched through the summaries and then I searched through the chunks. Marcus.
Speaker C

So when did those metrics come up then? I'm. I'm a little confused about what it's showing.
Speaker A

So. Yeah, so those metrics are automatically. I didn't do anything for them. Those are automatically calculated by Pinecone.
Speaker C

I it a chunk. But I like. It's based on our, our docs though. It's based on our PDF. So somehow it got connected with those within the code base and when I, you know, like I successfully ran the rag on my. On my Supabase data. But when I go to the. The reg. The Pinecone databases that I connected to that data, I'm not seeing any metrics at all. So I'm just wondering, like, when. Or browser. I'm sorry, in the browser.
Speaker A

So I'm not sure why you're not seeing the tab, but this should. So what this. These metrics are.
Speaker C

I mean, the browser tab. I misspoke. If you go up next to metrics.
Speaker A

Oh, next to this one.
Speaker B

Correct. Yeah.
Speaker C

All of these records, you're not seeing.
Speaker A

That in your pine cone, you're saying.
Speaker C

Well, I'm seeing that in the pine cone I ran from your test project, but I'm not seeing it in the pine cone of my. My project. Like, but I was successfully running rag. So basically, like, Pinecone has the data from my database, I'm making queries to it. At what point are these records being generated and why are there so few of them? Is there something in the code itself that is generating this?
Speaker A

So I have no idea in terms of your code, but if you're using Supabase with Pine code, it might be on Supabase and Pinecone might just be connecting to that. It might be one idea that I have. In terms of what I'm doing, this is automatic. So in terms of what I'm doing, whenever I do an upload of vectors, that's automatic. I'm not. So Pinecone will do this automatically if the vectors are saved natively on Pinecone, I believe if you're using Supabase, I would check the Supabase database for the vector if I had to get.
Speaker C

So when we. When we ran the Docker compose upload file or whatever, that's when these records ended up in Pinecone.
Speaker A

Yep, that's.
Speaker C

And. And then why are there only 10? Because, like, it seems like there would be a ton.
Speaker A

Because there's more. It's just paginated. There's 2,000.
Speaker C

Okay. Okay. Okay.
Speaker A

Okay. Yeah.
Speaker C

Awesome. Thank you. Sorry, just a little unclear.
Speaker A

No, no, no. Don't be. Don't be sorry at all. There's only four summary vectors because there's four books, but the chunk vectors are over 2,000. Okay. No, but good question. I believe the reason you might not be seeing what I'm seeing is probably because of the super base connection, but that could be something we test. Yes. Grades are pass fail. So if you have AI components working, that probably be the primary sort of way you can sort of pass this week. We would like to see your Slack application functioning properly in terms of the functions that we highlighted in the project document. And then we would like to see one or more. More AI features working properly in the Slack. Yes. So, Stephen, we'll have our AWS workshop tomorrow where we'll talk about a standardized, standardized DevOps flow and what you should be using and what services at what time. But I didn't want to ruin anybody's applications or what they had for deployment so far. So that's not something I'm going to be introducing now. We'll talk through it tomorrow, but it'll be for the next project. And some of the services, Steven, will be just to give you a head start, Amplify. And then I believe that for the next project we're just going to be using Firebase or Supabase, so the only AWS service you'll really need is Amplify. Okay, that's all the Slack questions and I will do the zoom questions then call it because we were running super late. What was the code Ashran in the terminal, John? I just copied the code off the readme of the project. It was just some docker commands, so they're all on the readme of the project and copy them. Will we be able to improve upon. Yes, you will be able to improve upon your submissions. You have until Sunday to do another submission. So if you feel that you want to continue to make your AI features better, then yes. And Ben, great question. Is the next project going to be for weeks three and four? Yes. So what happens is the first week we always rebuild, in the second week we always add AI. We talked about optimizing Rectangle. How much of this expected from Array? Great question, Palmer. In terms of your week two stuff, I would like you guys to try, if you have some time to to implement recall, to see how many of the vectors you're pulling are actually relevant to the query. But in terms of grading, at the end of this week, the only thing I'm looking for is functioning AI features on top of a functioning Slack rebuild. Okay, awesome. Thank you. What is the best way to easily deeply apart from aws? My app works perfectly. Logo Rishabh. You can probably just deploy on Netlify or Vercel. I know. And amplify. Frankly, they're all really easy. You just click buttons and connect to your GitHub repo. What's the point of having two indexes on Pinecone? The reason that we have two indexes is because the first one is the indexes of the summary and the second one is the chunks. So indexing a smaller vector database with just the summaries is really quick. It helps me find where to look and then I could just quickly look in that specific book's vectors to find the answer. With week one still not back where I was, should I just focus on week two and Sengo? Yes, Paul, focus on week two. I love this week's main theme is AI features. When I was trying to set up stuff on AWS over the weekend, I was an search. If you're deploying just your front end, I would just try to use something else. But if there is more issues happening then I would bring them to the AWS workshop tomorrow. We'll just walk through it. I think we just need dedicated time for AWS and that's what we'll do tomorrow. Okay, I believe that's all the questions. Today Zach will be finishing out his part two of the Hackalot project, Build, Live Build. That's going to be at 3 Eastern. So I'll give Zach a chance to say something about that and what to expect. And then beyond that, I'm going to call class today and thank you guys for joining me and giving me your attention for such a long time.
Speaker B

Yeah, so we're going to continue. Man, you don't know how hard it was for me to sit all night. Good thing. I had plenty of unfun work to do to keep me busy. But I wanted to finish it yesterday. So we're gonna today we're going to add roles, reviewers so that we can take grades, that kind of thing. I mean, you know how if you attended the first one, you know how it went and we take a little bit off the cuff. I do like the idea of doing rag, but I will say this. I'm not going to do rag. While it's your week of rag work, I'm not going to give you the answers. So if we do rag, we'll do that next week. But I would really like to. I think we can get mo most of it finished out in another hour, hour and a half. So we might have three hours of work into it. Maybe we might need to touch base again just a little bit to finalize our first event. And then this weekend we run a run, a timed hackathon just for fun and we'll get some reward. Somebody can, you know, win some money or gift card or I don't know, whatever it comes out to. But I think that's a pretty cool testament to what it is that you're learning for us to be able to build an app, deploy it, use it, and have a winner, all within five days. Sounds like a lot of fun. So that's what we'll be doing today. We'll just continue. So, yeah, we'll see you then.
Speaker A

All right, see you guys at 3 Eastern. And I hope you have a great rest of your day. Thank you for your time. This was awesome. And thank you for engaging so much with all of us. Okay, see you. Bye.
Speaker C

Bye.
Speaker B

Thank you.
