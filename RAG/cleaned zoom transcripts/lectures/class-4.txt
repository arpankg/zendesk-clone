%START_METADATA%

Class_Name="Class 4 - RAG Metrics and Optimizations"

Class_Date="2025-01-15"

%END_METADATA%



---

Speaker A

All right, so first and foremost, two weeks till Austin. Kind of crazy, or two and a half weeks till Austin. I do want to say that we are closer than all of you think. So we have that in mind. We'll have a meeting on logistics and move and all that stuff next week. So a lot of people have been asking me this question: How many people are going to make it to Austin? And my answer is as many as possible. So if you submit your assignments, if you complete what you're supposed to complete, then there should be no reason why you don't make it to Austin. What is P? I have no idea what P is. Maybe there's a... What's the conditional probability of us going to Austin, given that we're so now, you know, Wilson, that my statistics is terrible. The probability, I would say, is pretty high as long as you submit your assignments. I mean, I'm not going to put a number on it because I'm not grading all the assignments.

I mean, I'm not going to put a number on it because I'm not grading all the assignments. Obviously, there's Austin involved and a couple of other graders involved, but we'll be very clear, and everyone will reach out to anybody who's in jeopardy of losing their spot in Austin. And we have done that for previous students as well. If you haven't gotten any feedback from Austin directly or me, that means you're doing pretty okay. I mean, we were pretty lenient for week one and for the submission yesterday. We just want to see that people are meeting deadlines, right? So if we're going to be giving you a job right after this program, the first and foremost thing as an engineer is to meet the deadlines that are set for you. And so if anybody did not meet those deadlines, half of the people didn't even show up to the first class. That was a pretty easy cut. Right now, with the next project, the guidelines are going to become more and more stringent.

Right now with the next project, the guidelines are going to become more and more stringent. We're going to ask you to sort of hit metrics on production-grade or industry-grade applications. We're going to ask you to use quantifiable numbers for all four projects. There are 160-ish people left. Right. So. But that number is dwindling day by day. I'm pretty sure you guys are going to have a really nice group of people that can work together in Austin. And I'm really hoping to and excited to see all the projects that come out, especially like having that experience and being together for so long. I feel like you guys will really build some connections. The only other thing I'll say about that is in terms of logistics and in terms of who's going to get in? We want everybody to come. Right. So if you do the work, if you're able to submit something, step one, my recommendation, submit. If a deadline is about to hit, submit whatever you have. Do not wait.

If a deadline is about to hit, submit whatever you have. Do not wait. We will understand the circumstances that led to you not finishing the submission, but not submitting something at all is actually a very bad idea. And then we'll talk through it next week. Yes, you can submit on non-deadline days. Thank you for this great question, Ryan. If you want to submit earlier, that's totally fine. Starting with next week's project, we're also going to be sending out metrics. So I'll include in the project talk on Monday just like harder and harder guidelines to ensure that we're moving in the right direction. Yeah, you can put the MVP submissions on the same submissions page. I think they're resolving a caching error because somebody was navigating away from the page and then their submission would disappear. So they're resolving that now. But if you just submit, everything should be saved fine.

So they're resolving that now. But if you just submit, everything should be saved fine. In terms of next week's project, I'm not sure if I should tell you. I've been thinking about that a lot. Like, should I reveal next week's project, or should I wait till Monday? The hint is, let me give you a hint: CRM. How about that? That's a big hint. Hey, Paul. We changed some of the projects based on the hiring partners. And so we will change the... You're not going to be building Salesforce. We will change the sort of group of projects or the order of projects based on just how we feel the cohort is going and just how we feel like we can sort of challenge you guys more. Yes. We can build a common farm. Okay. We gotta, we gotta get sort of started. Oh yeah. I want somebody to build something better than Ilias's app. I'd love that. And then we can raise some funding from him.

I'd love that. And then we can raise some funding from him. Okay, so I'm going to start today's lecture because we have a lot to cover, and then I'll come back to more questions in general. The other thing I wanted to talk about with you guys was imposter syndrome. So a lot of you might be feeling imposter syndrome right now because you've been put into an environment where everything was intentionally not well-defined. You have to sort of figure your way out. You have to be resourceful. I will give you advice that my manager gave me when I first started out, which was that if you're feeling imposter syndrome, that means you're headed in the right direction for your development. Because if you stayed within your comfort zone and you continue to just do the things that you are good at, you wouldn't. You're not actually developing; you're just honing in on skills you already have.

You're not actually developing; you're just honing in on skills you already have. The whole purpose of Gauntlet is to ensure that each one of you excels and becomes amazing engineers for getting something from zero to one out there really fast using AI, right? And to ensure that you have that engineering sense that Zach was talking about in office hours yesterday. So a lot of you are going to feel this way, especially as time goes on and the projects get harder. I just want to start this class off by saying, keep pushing, keep pushing. The only way you get through imposter syndrome is by going right through it. So I wanted to say that, and I wanted to direct that to everybody. Okay, today's class is going to be on retrieval metrics and optimizations.

Okay, today's class is going to be on retrieval metrics and optimizations. So we've essentially created a simple RAG pipeline, right? But how do we take it to the next level? There is a lot out there that we could be doing to make our RAG pipelines better, but we want to give you guys essentially the starting line. What are some things that you can do out of the box to take your RAG pipeline and maybe make it 2x better? Right. So we're going to be talking through some metrics on how to measure that, and then we're also going to be talking through some optimization skills on how to ensure that we can sort of use this to stay up to date with techniques that we can use to make the pipeline better within your Slack applications. So I'm going to be making a thread on Slack, so let me just do that now, or you guys can sort of post your questions. Actually, put that question

Actually, put that question. Okay, so today's learning objectives are: we're going to be understanding three retrieval metrics, MRR, NDCG, and Recall at K. I'll walk through what each one of them is. I'll show you how to calculate them using some basic Python code, and then I'll show you the scenarios in which they are applicable. Next, I'll talk about two methods of ensuring that you might get better results in general for your RAG pipeline, something called contextual embeddings and something called summary techniques. And then we also have the repo that we will walk through. I intentionally haven't set up the repo at all. So I'm going to actually walk through the entire local setup because there are a lot of questions about, like, where do you put the keys and how do you set up Pinecone? So I want to do that live in class today.

So what are retrieval metrics? If you guys remember, the first class that we started out with was a class where Austin was highlighting a couple of spiky POVs that members of the hiring team have. One of them was something called QC or quality control. QC for AI is a big concept where we're ensuring that the AI output matches certain criteria, right? We're ensuring that we have metrics associated with this output and that we want to continue pushing and ensuring that AI is producing output on whatever category, whatever criteria that it's supposed to consistently, right? Building this level of a QC system for your different AI components is very important because this lets us track, hey, is this really working in the direction that we want it to work in? Or does this need to be upgraded? Does this need to be changed? Does this need to be iterated on? I think a lot of people think of AI as a black box.

I think a lot of people think of AI as a black box. You put things into the black box and, you know, output comes out. But we need a really nice way to measure, hey, what was the output? Was it accurate, was it reliable, and is it consistently producing this? When you think about production-grade or industry-grade applications, consistency and accuracy are extremely important, right? You can't have AI randomly hallucinate whenever it's trying to do something, a certain task that you give it. So if we were to think about this QC system, there are three areas that we can sort of walk into, and those are the three metrics that we're going to explore. But they're going to give you concrete scores to tie your retrieval pipeline to. So I'll know that my recall is at this, I'll know that my MRR is at this.

So I'll know that my recall is at this, I'll know that my MRR is at this. This way, I can say, okay, maybe this is one implementation of my RAG pipeline, maybe this is a second implementation of my RAG pipeline, maybe this is my third implementation of my RAG pipeline. I have quantifiable numbers for each of those implementations to decide what the best route forward is. So there are three key retrieval metrics. The first one is called mean reciprocal rank, which essentially means how quickly I can find the first correct answer. Let's say there's a factual tidbit, like a date or historical value, or if there's some sort of information specific to a type of user, how quickly can I retrieve that information from a vector database? This is really nice for, like, hey, I want to do some sort of Q and A agent or Q and A support bot or Q and A support chat. I want to figure out key information from documents.

And I want to figure out key information from documents. How can I grab the right piece of information as quickly as possible? So that's what MRR measures, how quickly your first response is the exact response you're looking for. Next is the normalized discounted cumulative gain. So it's a huge name, but we can just call it NDCG. It sort of evaluates like, hey, this is a ranking of the results that I'm expecting, but this is the ranking that resulted from the actual retrieval. How close was it to the ideal? Right. So I want to ensure that. Let's say I'm getting the top reviews on Yelp, right? I want to ensure that ranking comes accurately, but let's say it doesn't. So what can I do to make that better? So it associates a score with getting the ranking correctly. Next one is called recall at K. So at K is just the number of items that you're pulling from your vector database, right? So this could be 5, this could be 10, this could be 20.

So within those 5, 10, 20 items, how many are actually pertinent to the query that we're getting? This is very important because it shows the coverage, right? How much is my retrieval pipeline grabbing across all these documents that are related to the specific topic? Okay, let me just check messages. Great question, Joshua. It can be time-consuming. But in this scenario that I'm going to show you, it's just going to be like accuracy. So it's going to be like, how were we able to get the first result at the right spot? Is this metric subjective? I'm not sure, Sebastian, which metric you're talking about because you didn't say which one. But they shouldn't be subjective. They should be based on the sort of calculation we're doing. I'll walk through specifically what's happening in the background. So I think we'll come back to your question. Okay, here's where my disclaimer comes. There are multiple ways to measure RAG.

Okay, here's where my disclaimer comes in. There are multiple ways to measure RAG. You guys can sort of find 80 some odd metrics. You know, I just made that number up. But there are a lot of metrics out there for you to start measuring your RAG pipelines. And you can even do it with user-side metrics, right? How quickly is the user getting the answer? Did the user like the answer they were getting? Right now we're going to be talking about retrieval-specific metrics, but there's no single perfect metric out there. It's up to you as the engineer to make a decision on what metric should I start with and what is a nice way for me to measure the type of retrieval that I'm doing? And what metrics can sort of complement my MVP? Right. It's really important not to choose too many at once. So a lot of the questions I've been getting are like, hey, I'm stuck here, I don't know what to do next. You do not want to be stuck in analysis paralysis.

You do not want to be stuck in analysis paralysis. Go to ChatGPT, go to AI, and just ask it to pick one. Right? It's really important for you to start implementing a solution and trying to see if it's headed in the right direction than to decide between x odd number of metrics. I'm going to show you three today that you can start using now. But in the future, let's say you find another one. Let's say you want to try something else. Let's say these three metrics are not working for you. We'll talk through other metrics that you can use for your QC system to ensure that the retrieval pipeline is working perfectly. But again, I want to make the disclaimer, there are so many metrics out there, and you as the engineer will sort of build up that sense of which metrics to choose from. Right now, we will start with these three. So mean reciprocal rank. When should you actually use this? So let's say you have a correct answer.

So, mean reciprocal rank. When should you actually use this? Let's say you have a correct answer. There's one piece of fact that you need to find in your vector database, and you want it to go searching for that. Mean reciprocal rank is a really nice way to figure out if that's working properly. So that could be a fact, it could be a tidbit, it could be a specific passage, it could be a specific point that you're looking for. What it does is it calculates how many tries it takes for the first correct answer, right? Let's say we're doing a bunch of retrievals, and I'll show you a visual in just a second. How long does it take, or how many tries does it take for me to get to that first correct answer? You can also do how long instead of how many tries. But today, we're going to be talking about how many tries. So let me walk you through the calculation of that and come back. Here, you'll see on the right side is a visual.

So here you'll see on the right side is a visual. The first on the right side is a couple of queries that I've made, and it represents a query we're making to our vector database. So let's say we're trying to pull five items. The answer we're looking for is the green box, and then you have the other blue boxes. So in this scenario, what we're essentially doing is we're taking the number one and dividing it by the position of the correct answer. So in the first query, it took us two tries. In the second query, it just took us one try. And in the last query, it took us four tries. Then what we do is we average these numbers out to then get our mean reciprocal rank. What this means is we get the average number of tries that it's taking across multiple queries to get how quickly we're getting to the correct or the most viable answer.

The goal for this, as I outlined in the slide previously, is could we get this number to be 0.8 or higher? Right? So right now in my example, I'm doing three queries. That's not enough queries for you to measure. But let's say you were doing 100 some odd queries. Let's say you're doing 200 queries. You can calculate the score for each one and say, hey, which one is it? You can also do this with another sort of ground truth data set. Right? So you know, hey, this is what I'm looking for. This is the doc I'm looking for. And you can test to see if your retrieval pipeline is actually hitting an MRR that's high enough. Okay, let me go back to Slack. Yes, you can use multiple methods. That's no problem.


---

Speaker B

Yep.


---

Speaker A

Lucas, I'll talk about how to improve it. How is the relevancy? So you are the ones that can create a test database to decide whether to figure out the ones that are relevant. So you usually have a human decide, hey, the second one is supposed to be the most relevant. The fourth one is supposed to be, yes, you need a ground truth dataset. A ground truth dataset is just a dataset that's created by a human being that tells you, hey, these are the documents that are relevant. And so this way you can compare to the output that's coming out. Yeah, Brett, you'd have humans in the loop to determine what is relevant. A great question, Joshua. For the next project, we will actually be using agents. We might not be using RAG, but for this project, if you wanted to do this, what would happen is we would give you a ground truth dataset to compare. Great question, AJ. The MRR is a metric for retrieving out of the vector database.

Great question, AJ. The MRR is a metric for retrieving out of the vector database. It is not for the LLM or in tandem. Back to the slides for a bit, and then I'll come back. Okay, next one is. So when should you use MRR is probably the great question. I'll come to your question in just a second, AJ. MRR is a really nice way if you want to get the right answer fast, right? So let's say you have a QA support bot. Let's say you have an FAQ bot. Let's say you have something where you need to pull a specific piece of information quickly, and you want to do that in fewer tries. Then MRR is a really nice way to measure that. How do you make this better? Is a question that I got, and I'll show optimizations in just a second or two or three ways on how to make this number better. But this is a really nice way to also compare. Let's say you have three or four different RAG implementations.

Let's say you have three or four different RAG implementations. Maybe one is using a framework, maybe one is natively in Python, maybe one is using a third framework. How do I compare and figure out which RAG pipeline is working best? Okay, let me see if there are more questions before I move on. In real testing, should the human-selected truth also be confirmed in Canada? Yes, great question, Anthony. You should have multiple human beings figure out what your data set is. Again, it doesn't have to be super strict, like, you know, too many people involved. Like, you know, getting 100 people to review something can actually become crazy. You can even have something generated by AI, and it could just be fake synthetic data. And you can just have that as a way to ensure that your retrieval pipeline is heading in the right direction. I don't want all of you to sort of get caught up in the setup of everything.

I don't want all of you to sort of get caught up in the setup of everything. It's more about understanding how this works in the background and then seeing which ones you can apply to your RAG pipeline. Yes, I'll get into major ways in just a second. Let me get through all the metrics. Okay. All right, so the next metric is NDCG. So this is important when the ranking of something is important to the output. Right? So let's say we were using Reciprocal Rank Fusion. This is something we introduced in class on Monday where we're ranking the results from multiple queries and we care about the order in which it's being ranked. Right? So this could be. Imagine it's like restaurant reviews and we're giving five stars, four stars, three stars, two stars, and one star. We want the five-star reviews to come up first.

We want the five-star reviews to come up first. Is that working properly? Is that not working properly? Are they sort of put in different areas of the ranking? Are they missing the mark? So it's important to understand, like, hey, if I'm doing a sort of retrieval pipeline where the order of things matters, then I can use this as a way to see whether or not I'm close to the expected order. Okay, so for example, here, let's say you have a five-star review and then you have four-star reviews and three-star reviews and two-star reviews. This is similar to the restaurant scoring system I just talked about. It would be incorrect if the five-star review lands at the end of the list. Right? Another way to think of this is let's say we're searching for data science courses in our vector database and the first course that pops up is Introduction to Data Science. Okay, that's relevant. That should be the highest on the list.

Okay, that's irrelevant. That should be the highest on the list. And then what should be the least highest on the list? Well, that should be web development with the relevant score of zero. The way you calculate this score is you have an expected ranking. You can check that expected ranking with AI, you can make the expected ranking with human beings and see, hey, when I'm retrieving these things, are they coming back in the right order? Whether that order is a relevant score, whether that order is something else. It's important to understand if these things are coming back in the order that I expect them to come in. And so in what scenarios would it be important for us to use this score? So let's say we have a recommendation system. Let's say we have a system where it matters the order in which something is coming. So the movie streaming, for example, maybe it comes in Ash's most favorite movies or the ones that we expect him to like most.

Maybe it's some sort of research database where we're trying to collect topics, right? Which topics are associated with which papers. In this scenario, when we're pulling from the vector database, the ranking matters. The ranking is an important way to understand, like this is the most relevant, this is the least important. And so that ranking in these scenarios is a really nice way to sort of be tested using NDCG. Okay, last one. It's called recall. Recall just means that, hey, I am retrieving k number of documents, right? So I have my vector data store and I'm going to grab 10 documents, I'm going to grab 15 documents, I'm going to grab 20 documents. Of those 5, 10, 15, or 20 documents, how many are relevant? How many of those documents actually correspond to the query that I have? This is important to sort of understand coverage.

Let's say I have a ton of data in my vector database and I'm pulling like 100 vectors, right? Of these hundred vectors, which ones are actually usable? And if they're not usable, then I'm just pulling for no reason, right? Then I'm associating data that shouldn't be associated, and then I'm doing extra legwork to figure out what the relevant vectors are. So this is a really nice way. I have a vector database. I grab k number of vectors. Can I figure out how many of those vectors are actually truly relevant? And the goal would be that you want this number to be as high as possible. So here's an example. Let's say you have, and I've just used simple images, but obviously, this would be text-based documents and vectors. But let's say you have all these images, and there are two that are very important.

But let's say you have all these images and there are two that are very important. So two out of the actual k value would make it so that there's only 1/3 or 0.33 recall at K because only 0.33 of the vectors that you retrieved were actually corresponding with the initial query. This is very important in a lot of aspects. We don't want to be sort of just building a system that pulls willy-nilly, just pulling random items that may not be relevant or may not be too relevant to what we're searching for. Another nice way to figure out or associate a quantifiable number is recall. Okay, I will go to questions. The last thing I'll say is, let's say you're thinking about medical research. Let's say you're thinking about patent searches, legal documents. This is a nice way to figure out if a nice place for recall to sort of fit in.

This is a nice way to figure out if a nice place for recall to sort of fit in. Because you have so many legal documents, you have so much medical research, right? But you don't want to just be pulling willy-nilly across like 200,000 documents. You want to figure out how can I make this search more precise? And a really nice way to sort of do that is with recall. Okay, let me just look at Slack. Yes, there is a whole business around some of these metrics, that's true. That's a good point. Sometimes, arbitrarily, some of these metrics can be assigned incorrectly. So it's important to have humans in the loop to ensure accuracy. I'm confused. For instance, why not just maintain? Great question, Aidan. Vector databases are important for when you're searching through a lot of unstructured data, documents, messages, et cetera. But that doesn't mean you can't pair it with a regular database.

But that doesn't mean you can't pair it with a regular database. So let's say you have some document or message that you found, but instead of having to go in and get the vector database because it's not working properly, you can definitely connect it to just a regular SQL query. Great question, Sebastian. Obviously, different queries have different numbers of documents. I think that's a sort of a situation where you have to go out and try it yourself in terms of the scenario you're using it in. So in this scenario, what I would do is try to ensure that more than half of the retrieved vectors were actually relevant to the query to start out. And if you believe that you need to increase that more, you could do that as well. Great question. The scores on the metrics are associated with a couple of things. For MRR, the score is associated with how quickly you can get a relevant answer.

For MRR, the score is associated with how quickly you can get a relevant answer. So, meaning, like how many vectors does it take for you to get an answer that is relevant to the query that you have? The next one is like the ranking, right? So NDCG is the ranking of the actual responses that are coming back. So can you rank them by relevance, can you rank them by other key criteria? And if you're able to rank them, are they coming back with the associated rank you had before? The next one for recall is just to make sure that everything that you're retrieving is in the same sort of problem space, industry area, or it's actually related to what you're searching for. This way, like more than half of the vectors that we're doing should, in my opinion, at least be a little bit relevant to what you're trying to search for. You shouldn't just be getting random vectors all over the place. Yes, you can attach metadata to vectors. User is spot on metadata.

Yes, you can attach metadata to vectors. The user is spot on about metadata. So there's a huge metadata object that is associated with every vector. In fact, we'll go into the Pinecone vector today, and I'll show you the metadata object. You can add more fields to this as well. Another way to do it is through other popular vector databases like MongoDB and Quadrant, where you can have a ton of vectors. You can even search by the parameters, or you can even filter. I could be wrong, but I believe the photo shows the relevancy score. Yes, yes. So every time you do retrieval and do the vector search, it is all compared to the query. So, Anthony, vector queries and SQL queries are pretty much the same price. I mean, there's not a significant difference in price. But again, that could be something we check, and it depends on what you're using. So, AJ, great question.

So, AJ, great question. The similarity search algorithm is ranking each of the vectors that are retrieved based on how similar they are based on the cosine angle. Obviously, data quality is always foremost. Like if you're putting terrible data into your vector database, the output will be bad. So one of the easiest ways to sort of make a RAG pipeline better is to add better data to your vector database. That's obviously always going to be true. But it's also important that we understand, as you're saying, you can change the embedding model, you can change what vector database you're using. And we want to give you methods to quantifiably measure how to figure out which of the configurations should be the one you move forward with. Okay, we are running out of time.

Okay, we are running out of time. Okay, so before I move forward, I want to say there is a repo where I've taken some time to, well, I've taken some time to use AI and some example code I had in the past and some code that we've used in other consulting projects and create some calculators using Python for each of these. So I'm going to walk through them real quick and then this is available to you in the Gauntlet repo. So as we talked about, when you're calculating mean reciprocal rank, there is some sort of ground truth, meaning this is the expected. These are the relevant documents, I expect. So here you'll notice that in our function we have something called ground truth where we're saying that these are the relevant documents. And then you'll see that you can feed in the retrieved documents that we're getting for each query and figure out when it was able to finally get one of the documents in the ground truth.

So that's what I walk through, and it does the calculation as well, the same thing. One divided by the number of tries and then averaged out to the number of tries. Okay, so in this one, what we're doing is we're listing the number of sort of relevance scores based on some information that we give it already. For example, I have a dictionary called Relevance. The relevance dictionary has the doc with some scoring metric, right? So 3, 2, 1, 0. And what we're able to do is compare the output from a vector database and see if it matches the relevant scores and if it's ordered correctly with those numerical values. I think this is just to illustrate how you can go about calculating this and how you can do it programmatically. Obviously, you can feed this into AI and sort of get a more applicable version for your applications. But this is a really nice way to understand the concept, right? The concept is I know the relevancy of things and documents.

I sort of outline that in a dictionary and then I compare it with what's being retrieved from my retriever. Finally, it's recall. And so for recall again, it's pretty similar to what we did previously where we defined the relevant documents first and then we compared to see how many of the retrieved documents were in the relevant documents. So this is a nice way to figure out, okay, I'm getting X number of vectors back from my retrieval pipeline, how many of them are actually relevant to what I'm actually trying to do. Another thing I'll add to this is I don't have any LLM calls in here, so these are just example calculators for you to use. But you could also send some of this to an LLM. So instead of having a relevant dictionary where it shows that humans have picked all the relevant documents, you can have an LLM decide whether or not this is actually relevant.

And you can use an O1 reasoning model, or you can use higher-grade models to decide whether or not your retrieval pipeline is functioning properly.


---

Speaker B

Can you repeat that last bit about using Hire?


---

Speaker A

Yeah, thanks. Yeah, great question. So let's say, Marcus, that I have, you know, just some retrieved vectors from a series of queries that I have. And I saved that somewhere, maybe in a CSV or whatever. What I could do is maybe because right now we're using an embedding model. Usually ADA002 is what people usually use for OpenAI. And so, or if you're using one of the ones out of Pinecone, they're still not as powerful as 4.0, right? So what you could do is send the list of retrieved documents and what you're trying to look for, how you're determining relevance and ask the LLM to say, hey, is this really the order in which these documents should be relevant? Or what is the number of documents that are relevant in the retrieved documents that I have? So this way the LLM or the reasoning model can be the human in the loop or take over a lot of those tasks. Obviously, that'd be a little bit more advanced, and I would start with a human being.

Obviously, that'd be a little bit more advanced, and I would start with a human being. But in the future, you can probably use a reasoning model as well. You can even store your vectors. Yes, you can store your vectors in a Postgres database fairly quickly. You can do that. Inside of us, there's also something called PG Vector. And so it's really important to understand that you can easily do this in a NoSQL or a SQL environment. Oh, great question, AJ. Those three are three things that you can do. So I'll say that clearly. Let's say. So you've picked. I wouldn't do all three scores at once, but let's say you picked a recall, right? You want to make sure that your recall score is above a certain threshold. Let's say it's 0.5. What I would do is these are all the components that you can configure to ensure that your recall gets better. The quality of data 100%. The embedding model. There are so many embedding models out there.

The quality of data is 100%. The embedding model. There are so many embedding models out there. There are open-source embedding models like Nomic, and there are foundational embedding models like OpenAI. So you can switch those out and see if they're going to perform differently. You can change the vector database, as AJ is saying, Quadrant, Pinecone, or a regular Postgres database. You can even use ClickHouse on AWS. Right? There are so many options for configurations. And so, as engineers, you need a way to figure out which configuration is the best. After this, I'll give you two more examples of ways to sort of make this better, which are contextual embeddings and summary lookup. But the one thing I'll also talk about, and I'll share some articles and stuff on this, because it's different for each vector database, is the metadata object that is associated with every vector, which is very powerful.

Let's say I have a huge vector space and each vector has a metadata object with a certain series of parameters. I can actually reduce the size of that vector space using one of those parameters to make my search faster and more accurate. So let's say I have a bunch of Harry Potter books in my vector database and each of the filters has the title of the book. Each of the metadata objects has the title of the book and the chapter that they're associated with. What I could do is use the metadata object to filter to just the Harry Potter and the Sorcerer's Stone and ensure that I'm only searching that book of vectors to get a response back. So it's really powerful in reducing latency and increasing accuracy. Prompt engineering is also a great lever. So Mikael, that is spot on. Like if you're getting context, you need to frame the context correctly and you need to make sure that once the context is framed correctly, the stuff that you're putting into that is proper.

So I think it's important to understand what everything I can do to make my RAG pipeline better is, that is not some, you know, new optimization. Change the database, change the embedding model, do some prompt engineering, change the prompt engineering technique that I'm using, and increase the quality of data that I have. Add metadata to each of the vectors to ensure that I can organize them properly. And then we'll talk about two more as well. Can these calculations be made in JS100, Adam? You can probably just feed it into Claude or ChatGPT and do it that way. Kale, that is a great question. Over-optimization is always a problem. Over-training is always a problem. How do you improve the MRR without, you know, so what if it doesn't actually improve in actual life? These are just test demo scenarios.

I think when it comes to production-grade industry applications where RAG is being used, you will also have the assistance of a data scientist and a data engineer, and you have to make different datasets, like a verification dataset, a training dataset, and an evaluation dataset. All of these have to be distinct, and there's a process that you follow, and you almost have to conduct an experiment with each of the configurations. But again, I don't want you guys to focus on that right now. Maybe that is something you will get to at your jobs, but right now it's an introduction, right? These are the metrics and the values the goals would be. Do you understand them? Can you, at a basic level, implement one of them? And do you understand how you can change the configurations to make your RAG pipeline better? Yes, we will be using re-ranker models in future weeks. How can you

How can you? Should we make K large and filter off rolling Curry squares? Okay, so Drew is asking how can we optimize K or the number of chunks that you sort of grab? Frankly speaking, Drew, it's trial and error when you're first starting out. Like I remember we were doing a project once and we were starting out with 5, then we tried it with 10, then we tried it with 20, and then each one of them we would test out and see with the stakeholder on the project, like, is this actually relevant? So almost doing a recall calculation that is non-programmatic. So at the time we were thinking of using recall, but we didn't have it ready programmatically. So we had human beings come in and do it properly. I think it's very important that you have some level of verification going on because that level of verification will ensure that what you're building is made for production, it's made to last, and it's consistent. So the answer to your question is trial and error.

So the answer to your question is trial and error. I'm sure there are better ways to do it, but that's the way I've seen work almost every time. Great question, Anthony. Why would the database change something if the embedding model is the same? Vector databases have a lot of features, a lot of search functionality, a lot of similar search algorithms. They have a lot of different algorithms that you can use, Euclidean, cosine, et cetera. So changing the database can actually alter some of these scores because the databases are built differently. For example, Quadrant could be built on-prem within a server. It doesn't have to be cloud-based or using SQL versus NoSQL. So if you're using PGvector versus Mongo, like what is going to sort of get you to the right result? So I would test it, in my opinion, especially in the future. But right now, let's say you start with something and you have a threshold for how well it needs to work.

And maybe you want to make the database the last thing you change, then that's okay. You can start with the embedding model and you can start with the data itself and the prompt. Yes, you could. Yeah. So Brett makes a really good point. It's not just about adding better data, it's also about removing bad data. So people always get that wrong. Like you need to remove the data that is sort of taking away from what you're actually trying to search for. That might be a really big issue when it comes to ensuring that your data quality is high. Okay, I'm going to come back to the questions and I will make sure each one of them is answered. But I want to make sure that I walk through the two optimization examples that we have. The first one is called contextual embeddings. This is a really, really straightforward way for you guys to sort of make your vectors a little bit better, right? And perform at a better level.

And so in this scenario, let's say I have two words, mouse and rodent, mouse and mouse. They both refer to two different things, but I don't have the context to associate which vector goes where. So a really nice way is to add some context to the vector, and people would say, does that mean I'm just increasing the size of the chunk? No, that means that you're adding actual text that corresponds with this. For example, let's say I was trying to make a vector for this line of code right here, this line of code. Code called calculate recall at K. Right. So what I would do is I would make a vector by saying the following is a line of code in the recall_k.py file in the class 4 retrieval metrics repo. This is what it says. And then the line of code I've added context to where the vector might be, what place it might be in, what it's referring to.

This context is really valuable and can be synthetically generated by an LLM to ensure that each of your vectors isn't just random sentences in the cloud. Right? It could have parameters associated with it, it could have additional context associated with it. It can refer to a bigger document or a bigger piece of information. So if I were to talk about how you can do this, you can talk about what chapter it's associated with, what section it's associated with on a document. For example, this contextual information is on slide number 17 of the Class 4 slide deck, right? All of this is important information and context. So when I come in as a user and I chat with your RAG system and say, hey, what's on slide 17? The RAG system can actually grab the relevant vector and actually answer that correctly. And this works well with metadata, like the title of a project, the author of a book, timestamps. You can even do neighboring text. This is really powerful.

You can even do neighboring text. This is really powerful. So you can see like, oh, this was the text that came before and this was the text that came after. Now you're going to ask me, how do we know how much context to add? What I would do is start off with one or two pieces of information that you want to add to the vectors to make them more relevant. And I would see whether or not your recall score gets higher, see whether or not you're getting answers faster to the user. And so it's definitely a trial and error here, right? You want to see and test, hey, if I add this to the system, does it get better? Okay, going back to questions and then finally, we'll do our summary lookup. Is there a reason to store vectors of different types of embedding? No, you don't. There's no reason to store vectors of different embedding types. If you're going to switch embedding types, you should switch it for all your vectors. The indexing algorithms are different for full-text search.

The indexing algorithms are different for full-text search. There's performance. Oh, you're just answering. Yeah, I agree. Yes, Joshua, the first thing I would focus on is recall. Just ensure when your RAG database is pulling things that more than half of the vectors are actually relevant. This is not okay, so are we expecting. Great question, Steven. I'm not going to specifically ask you to implement a lot of these features. The one thing I will say is the summary lookup feature, and I'll talk about this at the end of class, is a really, really nice way to sort of search across documents in your Slack workspace. So whether that's images, PDFs, or anything, using summary lookup across PDF documents is a really nice way to figure out, okay, I'm going to this document and grabbing this information. So the answer to your question directly, what would be awesome for this week would be if you added summary lookup with the documents associated with your Slack workspace.

So Amir, the Harry Potter example would be the actual books. I was actually referring to the books, but in your sense. Yeah. So in terms of your Stack workspace, Amir, what I would focus on is recall, and the thing that I'm asking you guys to implement or I would suggest implementing is summary lookup, and I'll walk through the code for that in just a second. When do you do search? When do you? When do your search. Introduce me. Okay, Spencer, you don't need re-ranker access. You could do a lot of this without that access on AWS. Okay, how does this apply to our project? Seems the question that keeps coming up. The only thing you have to do for your project is consider one of these metrics to measure. I'm not going to check that directly. The one thing I would suggest for your AI component is using summary lookup to find documents that are within your Slack workspace.

Because you should be saving documents and uploads and all that stuff and be able to search them through using the summary process. I don't. Where do you add this context in the same text? You can go to. The context can be added in the same text, or it can be the metadata. It can be both. You can run these tests on your local machine using some of the code that we offer, but in terms of actually doing it on LangChain, LangChain does not automatically calculate for you. LangSmith does show you the output, so you can, you know, actually do a very quick calculation of what's going on. So when do we add context to chunks about? Yes. So Gary, what I would do is I'd get an MVP working of your RAG pipeline, and if you feel that your recall score is too low, I would consider adding more metadata to reduce the vector space and add context or add more information to the actual text that's being vectorized. AJ, great question.

AJ, great question. For clarification, the chat avatar can connect to the document one, right? So if you guys completed the MVP today, you should be able to do a basic chat with your vector database. But to take it to the next level, summary lookup is a really nice way to sort of say, okay, I'm gonna go grab this doc, the avatar or the persona is going to go grab the document and grab the information and put it out there. And so the answer is you should. Your chat avatar can connect to your summary lookup and do the document for you. So we're trying to give you guidance on how to approach your AI components, but again, if a lot of you want to sort of do it your own way, you could do that as well. How do you add context to your metadata, Sebastian? That just depends on if you just. There's a bunch of Pinecone documentation on this. If you just search Pinecone metadata, they have a bunch of documentation you can use.

If you just search Pinecone metadata, they have a bunch of documentation you can use. Yes, you can stick with the vector database story you've already chosen. Do you attach a summary of each chunk to each vector related to the chunk? No, Gary. So the summaries would be for huge documents. I'll show you in just a sec. What we're doing for summary lookup is we're making another index of all the summaries and then using that index to find specific vectors in a document. So summaries are for documents, huge documents, PDFs, in the case of your chat app. The documents I'm thinking of are like, let's say you've uploaded something to Slack. So if you don't have the upload feature working, then summary lookup might not be the way to go, and you might want to just focus on the messaging. But there are a lot of students who are able to upload documents and then save them somewhere. So if you have that working...

So if you have that working, this is a really nice way to go and find it. You can also do summaries based on individuals and all the messages that they've had and then look up based on those chat histories. How frequently should you be adding new messages? That's totally up to you. But yeah, I think it's important to update the messages so you have enough data to actually get your RAG pipeline working. Document Lookup is not required for Monday, but it's important for me to push you guys in a direction on what components to add, and that's my suggestion. Oh, well, speed is a big win, Joris. So, yeah, I mean, speed is a huge advantage. But I also think that from a user experience perspective, instead of having to go through all these vectors and maybe get the wrong answer, I feel like Summary Lookup just has a better user experience because of that speed. Okay, I'm going to do one last thing and then go into the code because we're running out of time.

Okay, I'm going to do one last thing and then go into the code because we're running out of time. Summary lookup, what is that? So let's say you have a document, you can create chapter summaries of that document, and you can create even section summaries and sort of pinpoint to the paragraph. So it's like I have API documentation, I can go into authentication methods, and then I can go to OAuth implementation steps. That means I can sort of narrow down on the area, excuse me, on where I should be grabbing the information. So it lets me quickly get to the right portion of a document to ensure that I can grab that information and do something with it. In terms of your Slack applications, you guys have implemented messaging channels, threads, some of you even have the upload feature working.

So if we think about all the AI features for your Persona, right? The AI Persona should be able to sort of chat with the entire workspace in terms of getting me messages or giving me a summary of all my messages with Gary. Give me a summary of all my messages with Marcus. When Marcus and I last talked, what did we talk about? All of these things are features that are out-of-the-box available if you're vectorizing all the messages inside of the Slack workspace. But what else can you do? You can actually pinpoint and take all the documents that you've uploaded and use summary lookup to sort of chat with the documents. You can also create huge documents with all the messages that you have in a channel and then say, okay, I'm looking in the channel I have with Callum, and in that channel, I want to find X, Y, and Z. So I think it's important that these methods are really powerful for your Slack application. We're trying to say, hey, these are potential AI features that you can add to your AI avatar.

We're trying to say, hey, these are potential AI features that you can add to your AI avatar. Obviously, you might be thinking of something else, but we want to give you some level of direction on where to go. Okay. Finally, I'm going to go to the repo, and this repo is very similar to the repo that we had yesterday in terms of setup, in terms of all the keys that you need, and so intentionally I haven't done anything. I'm going to walk through just how to do this from scratch. So I want to sort of resolve any questions that you might have regarding the initial setup. Also, it's looking like this class is going to run a little bit longer, so I do want to say, like you, I will be staying on to ensure all the material is done and completed. But if someone has to pop out and then watch the recording later, that's okay. Okay, so I'm going to open up my terminal, and I'm just going to go into my desktop, and I'm going to clone the repo down.

I'm not forking it, but you guys should be forking it. In fact, you know what, I'm going to. I'm going to fork it just to illustrate the importance of forking. All right, foreign. It's really important that you guys have your own forked version. Let's say you need this in the future. Let's say you want to reference this. I want to make sure that it's available to you on your personal account. Okay, so here we're going to copy this and we're going to clone it down. Oh, looks like I already had one saved from previous work. So let's go to Documents and let's come in there. Okay, awesome. And I'm going to open up Cursor with the actual. Okay, this is the wrong repo file. Great. When you're working in your team with your hiring partners, you will be able to do the branching, but in terms of best practices when it comes to open-source software. So I guess that that's what this would be considered. You want to ensure that you're forking it onto your own

You want to ensure that you're forking it onto your own. And then that doesn't mean you can't sort of open up a pull request with the original version. You can still do that through your forked version, but this just ensures that all the work that you're doing is confined to your personal account. Yeah. So somebody did release the OpenAI key this morning, and that's okay. I did generate a new one. So I believe it should be working fine now, but we'll find out in just a second. Okay. All right, so I'm going to just follow exactly what is being said on the readme. So I'm going to go to Pinecone and I'm I for anybody that's about to leave because they have to work on something else or whatever. We have another session with Zach today at 3 ET, I believe, and he's going to finish out the live build using cursor and AI-first development for the Hackalot project. So he shows his entire thought process.

So he shows his entire thought process. He's got to the point where you can make a submission for the hackathon, and I believe he's going to be adding timed competitions. And so the awesome part about this is we're then going to run a hackathon for you guys later on where we'll make some sort of prize, whatever, $500 or something, where you guys can sort of submit your things and sort of do like a side project that you might want to launch. So that's going to be great. Question. Can he? I mean, a lot of people were telling me they have a lot of side projects and they're entrepreneurs. We didn't do it the first week, Brett. This is just an idea that we had like yesterday as Zach just really wants to. So, so far he's put in an hour and a half or so. He wants to put in a couple more hours and sort of launch it and show you guys. Okay. Can we implement RAG in the hackathon? Yes, we can. So I'll ask Zach. So we'll probably finish up the app today and then we'll add RAG to it later on.

So I'll ask Zach. We'll probably finish up the app today and then we'll add RAG to it later on, meaning probably tomorrow. Okay. So I'm just going to create an index here as instructed. Just walking through it. I just want to make sure that everyone is, you know, doing this properly. And let's call it that. It's going to copy this exactly. Normally, I would use the CLI, but, you know, I'm trying to show you guys how to do this properly. So here you'll notice I've used 1536. It's always going to be cosine. And I'm going to create the index. Okay, I've created the index here. I'm going to go to my API keys. Let's create a new key for class four. Notice how my ENV file is in the root directory of the folder. There's already a bunch of files here, but you want to make sure that your ENV file is in the root directory. So what I'm going to do is copy over my sample in the README. Looks like I didn't add a sample file to this. I could fix that too.

Read me. Looks like I didn't add a sample file to this. I could fix that too. Okay, and let's put our key here again. For anybody who already knows how to do this, feel free to drop. But I want to make sure that I clearly show what you should be doing for project setup. And then we're going to create a second index here for chunk index. Then I'm going to grab the OpenAI key. Okay, let me just answer AJ's question directly. The doc is up to date. So when you're doing the Persona that matches you, I believe that requires a RAG pipeline. You guys can correct me if I'm wrong and use a different method, but in terms of grading, I'm looking for any and all AI features that you've added, and you will get credit for all of them. Right? So it's important for me to give you direction. So when you're building the Persona, you need a vector database of my messages.

So when you're building the Persona, you need a vector database of my messages. If you don't have that, I'm not really sure how you're building that Persona unless it's just based on a single prompt that you're using. But let's say you're using vector databases and RAG to build that Persona automatically using the messages for that individual, then you should automatically be able to chat with that Persona, answer questions, etc. The summary lookup is just an extension of that. Right now, I can have all the messages across everything, making it much faster. I can add documents to that. I can add documents that are associated across the board. And so, you guys, this is my suggestion to you: your Persona will become immensely powerful if it has all the messages associated with Austin Allred and all the documents Austin Allred has now uploaded onto Slack.

In terms of how we grade it, right? First and foremost, it's graded based on whether your AI component is actually functional and working. Is your Persona answering me when I'm sending a Slack message? And is it going to actually search for something and give me relevant answers? So I wanted to answer that directly by saying the AI Persona is connected directly to RAG. Yeah, can I jump in to clarify? I think the main point of confusion for me is sometimes when you're describing the Persona, you describe it as if we're talking to our own Persona and asking it to do stuff for us. Other times it has been described as other people will DM literally my account on Slack, and it'll be an AI responding with all of my knowledge. Like, which of those two things are we talking about? I mean, they both. AJ, if you implement either one of those things, you pass. Cool, cool, cool. So I want you guys to understand in terms of the AI components.

So I want you guys to understand, in terms of the AI components, we just want to see you guys actually try and get some of the AI components working, right? At least for week two. I think the guidelines will become more stringent as the project progresses and other projects come forward. But at least for this week, can you get your AI project from start to finish? There are some of you with ideas already, so feel free to push through on those ideas. But there are a bunch of you who have no idea what AI components to add, so I'm giving you the options, right? So that's what's happening. I want to make sure that everybody has a direction. I don't want anybody to be stuck. And so whether you're chatting with all your messages or chatting with documents, we want to try to implement some level of AI components this week. But AJ, I think it's a really good question, and to answer directly, I would be happy with either this week. Awesome.

Awesome. Thank you. Sorry, I'm just reading the questions. Oh, like Joshua, I'm going to talk about setting up the summary lookup and actually running it and walking through the code. Yes, we can ask Austin to reach out to Han. Oh, Robert asked, in case of having all the user matches, why use RAG? We have access to all messages running the RAG. So, Robert, I would argue that RAG makes it so that you can find meaning across messages and make a better answer than you would without an LLM and without actually composing the messages together. But if you feel that that may not be necessary, I would still recommend using RAG as a way to sort of make connections across all the documents that you have. Otherwise, it'll just be sort of, here's everything that we have and here's what's coming up, like a simple search. Rather, we want the Persona to actually answer as if they were you. How's that? Go ahead.


---

Speaker B

Just to add on to that, a thought and something I've been looking at myself. Rather than just a Slack one-to-one message, imagine a team that's working together and they're using that as their main communication. More of a channel kind of environment. And then one of the team members is gone. If you've been archiving the entire conversation and you archive the individual, then you have the entire conversation. However far back it goes in history, you would be able to answer a lot of questions without having to do external research. Maybe there was a question six months ago about how we applied this one particular thing and that person isn't even there today. Or maybe they're not with the company anymore. If you've archived that knowledge, you have that. That personal knowledge there from both as an individual and as a team. I think that would be super powerful as a team member because I use Slack a lot.

I think that would be super powerful as a team member because I use Slack a lot. I mean, I was looking for a message from Ash just yesterday. That was only four days ago, and I had to scroll and scroll and scroll, right? So imagine you give them some kind of knowledge or a key or something or a trick, and you're like, what was that one thing? Well, rather than trying to scroll or trying to make a search that would work to get you that piece, your AI already knows the answer, and Ash is already in bed. But I can get his response because we have that rag. Just a thought of where my mind is going on this kind of development.


---

Speaker A

No, I think, Zach, you're spot on. I think connecting messages across channels is going to be really powerful. Like, what if we have a project Gauntlet channel and we lose all the messages in it? Well, then we can go search that channel and find that message quickly. So I think it's powerful using a vector database for this to answer Nicholas's question directly. Persona summary lookup, Document lookup. If you do any one of these things, any one of these AI components is working and functional in your application, you will pass this week. I'll be very clear about this. We are giving you so many avenues to explore here, and any one of the avenues that you get working and working properly will lead to passing. Stephen, I'll come back to the voice tips in just a second. I want to finish out the setup video and then I'll come back. Okay? Okay. I keep getting off track. I need to finish. Okay, here is the OpenAI key again. This is for anybody who has any issues with local setup.

Okay, here is the OpenAI key again. This is for anybody who has any issues with local setup. I wanted to do this once and make sure that everybody had a path going forward. So you'll see I'm walking through each and every step, getting the keys, everything. Okay, Zach, where's the key button on Linksmith now? They changed it.


---

Speaker B

Oh, it's been a week since we looked. Of course, they changed it. I think it's under Settings now.


---

Speaker A

Okay. Oh, it is.


---

Speaker B

API keys. Yeah.


---

Speaker A

Yeah.


---

Speaker B

Don't get used to any stability in Langsmith.


---

Speaker A

Yep. They're going to change it every time.


---

Speaker B

As soon as you know where it's at, guarantee it's changed. It was so bad. When we're doing our weekly classes, our other classes, I would go and check before class where it was at so we'd have it ready for class. That's how bad it is. Their service is great.


---

Speaker A

I feel like it also, like, with their library too. They'll change where all the libraries are located and, yeah, everything across the board. They'll say, like, they'll change specific methods on what classes they're in. And anyway, so everyone in the community, you know.


---

Speaker B

Write something really cool, and people love it. You're using it today, and tomorrow it'll be gone because they've absorbed it into their system. Maintaining it requires good research skills. It's a great tool to figure out what crashed.


---

Speaker A

Just waiting for my Docker Desktop to start working. Give me a sec. All right, we got the Docker Desktop working here. And what we're going to do is just run some commands. These commands are already in the README. So copy that. All right, so our containers are being built. You can see it in a second. I'm just going to keep following the guidelines here.


---

Speaker C

Can I ask a quick question about Docker?


---

Speaker A

Yep.


---

Speaker C

So we've got Docker running with this project. I've got my Supabase running in Docker. If I don't shut it down, is it just going to be running in the background on my computer forever?


---

Speaker A

Yes, you have to shut it down manually. Usually, what happens is... Okay.


---

Speaker C

Any tips for that, like making sure? Yeah, I do. Okay, so on top of Docker Desktop, you'll...


---

Speaker A

See this little icon on top, which should indicate Docker running in the background. I would right-click on that icon and just quit it. That should resolve most of your issues.


---

Speaker C

Okay, but what if I want, like Supabase to keep running, but some other ones to shut down?


---

Speaker A

Oh, no, no. So if you want Supabase to keep running, then what you should say is.


---

Speaker C

I have different projects I'm using Docker in. Is it easy in Docker Desktop to sort of see what's running and according to what projects?


---

Speaker A

Yeah. So if you just open up desktop projects here, you can just click the trash button for the project you want to delete.


---

Speaker B

Or stop if you just want to turn it off, but you don't want to delete it.


---

Speaker C

I see it. I see it. Okay, great.


---

Speaker A

Yep. Okay, so right now we're just setting up our vectors for each of the documents that we have stored. More questions? This is happening. Actually, let me scroll through the Slack thread.


---

Speaker B

Can I chime in real quick?


---

Speaker A

Ash? Go ahead. Spencer, what's up? Yeah, I just wanted to say I was adding. I was pulling down these notebooks and...


---

Speaker C

Adding them as markdown documents.


---

Speaker B

My code base to give, like, Claude.


---

Speaker A

Something to go off of. Yeah, and I wonder if other folks are doing that.


---

Speaker C

And yeah, I just wanted to mention it because.


---

Speaker B

Oh, sorry, you need to add those.


---

Speaker C

Markdown documents to your .gitignore file. I just, as I was doing that.


---

Speaker A

I was like, oh, this is a.


---

Speaker C

Way that people could leak their keys.


---

Speaker B

So, I just wanted to add that, yeah.


---

Speaker C

The keys are stored in plain text in those notebooks. And so if you, like, convert those to markdown and, like, use that as a reference for coding, that could be a potential way to leak keys. Yeah, I don't. Yeah, I don't keep the keys in my docs, but I also have a docs folder that is an Obsidian vault, and I put that in the git ignore. So that's my strategy. But I don't put the keys in there in the first place.


---

Speaker A

I think with the keys, Spencer, you make a really good point. Let's just check again. So one of the biggest things that might be happening is when you're doing git add, git commit, and git push, we want to make sure we do a git status before we do a git add. This will show all the files that have been updated, and you want to pick and choose the files that you're actually sending to GitHub. That's the first step. The second step is, as Spencer said, have a .gitignore. Put any file that may have a chance of the keys leaking inside the .gitignore. It's really, really important that we highlight that. And a way to sort of make sure that Cursor doesn't update your .gitignore. There's a .cursorignore file so you can put the .gitignore file in the .cursorignore file so your .gitignore never gets updated. So these are just some steps to sort of go about this. But the reason I wanted to, I mean, Spencer makes a really good point.

But the reason I wanted to, I mean, Spencer makes a really good point. When your key goes live, because we're sharing keys right now, it becomes unusable by the entire cohort, and so then I get a message, I then delete it, I then make a new one. And so I've been trying to keep up with that. I apologize if I'm delayed sometimes, but it's really important that we sort of get this practice in hand and ensure that we handle keys properly. Cal, go ahead. Yeah, I was just wondering maybe if one person did this, then we could just copy it to everyone. We could set up a pre-commit hook so that it looks at every commit and checks your .env file, and if there are any matches, it just rejects the commit. Yeah, I feel like there's. Zach, do you think that already exists? We can just use something out there.


---

Speaker B

There is tooling for that. The thing is, it's a real hassle to get set up. But most importantly, this, I feel, is a grow-up moment, not a helicopter engineer moment. As good engineers, you need to know this practice inside and out. It needs to resonate when you do anything. And so it's a burden to have to reset these. And you know, it's not a hard thing, but you need to mess up in an environment where it's not really important because if you don't ingrain these by the time you get to a job and you do it, I've seen lots of, I work at, you know, I get hired as a CTO at a lot of companies for interviews, and I can't tell you how many people have been replaced and not hired because they did not ingrain this process. And so this is one of those, like, you know, we're kind of babying you in the sense that we'll reset it and nobody gets yelled at, you know, too much.

It's annoying and not the kind of thing, but from your perspective, get it into your mental framework, and then you won't make the mistakes moving forward, and it'll matter in your job. We can set up tools, but there are so many different tools, and so many companies I've worked for will not use that tooling. And so it's back to you. So I would say everybody just has to learn how to do it right. And you know that process now. In four more weeks or six weeks, if it's still happening, I'd be chewing on somebody myself. But you know, from a general perspective, when we work with our other engineers, our zero to one where we're teaching to be engineers, we expect that this gets resolved by month three, which for you guys would be about the end of week two. So, in time and work comparison, everybody has done it and will do it in their lifetime.

Everybody has done it and will do it in their lifetime. You know, we often joke that if you haven't wiped out the company database, you're not a real engineer yet. Because every engineer of any amount of time has done it or done some serious damage. Hopefully, there's a backup. I've done it. And I will tell you this too. Part of what we want you to do is we are thinking through this. AI is really bad about moving keys around. And it happened to me just the other day, Claude went and just decided it was going to move it somewhere else. And I got notified, killed my keys. I had to go redo it. Okay, so it's going to happen in life, but we want you just to be very vigilant in your own mindset about it. And so the answer to your question, you know, call them yes and no, but learn it and learn it well. Now, if you write something yourself, that does it for you and you want to share it, I have mixed opinions on that, but for yourself, absolutely.

But just understand that this is the real world, and it is something you're going to deal with for potentially the rest of your life, especially where you will most likely end up. You're not going to end up at Amazon on a project that has 100 engineers who have been there forever. You're most likely going to end up in a company that says, "We want to work quickly. You've learned this skill. We want to develop, we want these new things," and they're not going to have those systems set up. So there's a pretty good chance that the better you are at it, the better employee you'll be, and the better engineer you'll be. My thoughts.


---

Speaker A

Okay, thank you guys for that. I want to. And thanks, Zach. I think that perspective really matters. And I will become harsher as the weeks go on because I could figure out who shared the key. So I want to make sure that right now I'm pretty lenient. I'm lenient with a lot of things, but as the weeks go on, I won't be.


---

Speaker C

You'll tell us if we leak the key, right? Because I want to know if somehow I did that.


---

Speaker A

Maybe I'll just put a pre-commit hook, and the pre-commit hook sends me the GitHub of the individual that's about to leak a key.


---

Speaker B

What are some common ways keys could be used?


---

Speaker A

Be leaked that aren't obvious because I think maybe I leaked my own personal information.


---

Speaker B

Key because it seems it got regenerated. But I checked my database, everything, and my GitHub, and it's not there in the commit. But I'm just unsure what might have happened. Like I was looking online, maybe there's a commit history. Just any ideas on how to figure it out?


---

Speaker A

That would be good too. I think we will, 100%. I'll answer your question, but I'm just going to walk through the code first because I keep putting it off. I'm going to finish the code and then Marcus will come directly back to your question.


---

Speaker B

Sorry.


---

Speaker A

No, you're totally fine. You're totally fine. Don't be sorry. Okay, so what we've done here is we've created a summary index. So there's a bunch of documents in our code base. And these documents correspond with large PDF files that correspond to books. And you'll notice that what we've done is create a summary of each of these books and vectorize it. Then we have a second database or a second index, excuse me, which is actually the chunks. So this is now chunks from the original books that we've now vectorized and added. You'll notice that we've added metadata, which is the source. So it tells me which book the chunk or the vector is coming from. In terms of code, what we're doing is we're going to first search through the summary and find, okay, what's going to be the most relevant book to get my answer from? And then we're going to search through the chunks to find an answer directly to the query.

So this is really powerful because instead of having to search through all of the vectors across all of the books, I now went to the exact document and then I was able to use the summaries to find where possibly my answer could be. And then I was able to then get an answer using an LLM and direct passages from that book to make my sort of life a little easier, a little bit faster. This should increase the quality of vectors that you're pulling and it should make the speed at which you're pulling stuff much faster. Okay, I just want to run through that. This was how we set it up. I'm going to now take questions, obviously. And the last thing I want to talk about is what are your expectations for the end of this week? The expectations for the end of this week are working AI features on top of your Slack application. This could be a persona that answers for you. This could be a RAG implementation that is able to

This could be a Persona that answers for you. This could be a RAG implementation that is able to. You're able to ask questions through regarding any message across the workspace. This could be document lookup using summaries where you can ask questions to specific documents. Any of these AI features working seamlessly in your Slack application would give you a passing grade. If you want a higher grade, then adding multiple of these AI components would do that as well. My recommendation for the Persona is that you use RAG. You RAG all the messages that you have across channels and your Persona is able to pull from this vector database and answer questions directly if you want to. Then also add in documents because a lot of you have this upload functionality. You can then add documents, vectorize those, and include them as part of your vector store. And you can use summary lookup if you wish to make it more optimized. I think these are all options for you.

I think these are all options for you. These are directions that we're trying to give you. As long as we see a working AI feature, any one of those three or something else that is on that same level, you will pass for the week. And you know nothing is breaking. You're making the video. Everything's submitted on time. Okay, that's all I wanted to cover. I'll go to questions now. Zach, do you want to answer Marcus's question, or I can go first?


---

Speaker B

Are we talking about the question of accidental ways to leak ENV stuff?


---

Speaker A

Yeah, yeah.


---

Speaker B

It's kind of threefold, generally speaking here because it's a learning environment. We use a public repository, and that's always a problem. Right. If it's a private repo, about 90% of that is not as much of an issue anymore. But in a public one, absolutely. Make sure your .gitignore is ignoring your .env file and use your .env files. 90% of the times that it leaks, it's because one of those two things didn't happen. And I don't know, I haven't looked at all the ones Ash has had, but I've looked at a couple when Austin was talking about it the other day, and that was the case in both of those. Just that the ignore was not properly ignoring .env or the .env wasn't even there. The third one would be AI likes to use inline variables, so never pass your variables as much. If you set through my deal, we did the variables manually because we didn't give cloud access.

If you sat through my deal, we did the variables manually because we didn't have cloud access. Now it is indexed, and you can use the cursor ignore, which is great, but if Claude doesn't index it directly when you pass it, it's much less likely that he's going to put it around. He, they, she, whatever Claude is. I don't know them, but whatever Claude's assumption is. And so don't pass it and then make sure you ignore it. At least the first few times that you do, always use git status, double-check that you don't see it, and then just get good at looking at what Claude is doing, what you're doing in your AI, whichever one you're using, and just look because it's going to give you the plus-minus differential between added code, removed code, and just keep an eye on it. If you do that 98% of the time, you'll be fine. There are a couple of times in life where it'll be really, you know, something will happen, but that will solve almost everything. Go ahead.


---

Speaker A

Yeah, I think in terms of accidental ways, you can probably also ask AI, but I think what Zach mentioned is true. We just have a scenario in which we're also dealing with public repos because right now I've been trying to add everybody's GitHub to our GitHub org, and I only get like 25 invitations a day. So I'm trying to figure out how to fix this. But in terms of how we resolve this going forward, I think we'll just put all the repos in our GauntletAI organization. Obviously, you can keep a version for yourself as well. I'm not saying that you can only keep it in GauntletAI, but this way we'll be able to manage keys much better. Okay, so I just want to answer some questions on the Slack. Benji, the metrics that Pinecone gives you, those are automatic from Pinecone; they're not connected to any code that we ran. So those are metrics that Pinecone automatically calculates, and Gary

So those are metrics that Pinecone automatically calculates. And Gary, yeah, it's a two-step search. In what I did, I searched through the summaries and then I searched through the chunks. Marcus.


---

Speaker C

So when did those metrics come up then? I'm a little confused about what it's showing.


---

Speaker A

So, yeah, those metrics are automatic. I didn't do anything for them. Those are automatically calculated by Pinecone.


---

Speaker C

I hit a chunk. But I like it. It's based on our docs, though. It's based on our PDF. So somehow it got connected with those within the code base, and when I, you know, like I successfully ran the RAG on my Supabase data. But when I go to the Pinecone databases that I connected to that data, I'm not seeing any metrics at all. So I'm just wondering, like, when. Or browser. I'm sorry, in the browser.


---

Speaker A

So I'm not sure why you're not seeing the tab, but this should. So what is this? These metrics are.


---

Speaker C

I mean, the browser tab. I misspoke. If you go up next to metrics.


---

Speaker A

Oh, next to this one.


---

Speaker B

Correct. Yeah.


---

Speaker C

All of these records, you're not seeing.


---

Speaker A

That in your Pinecone, you're saying.


---

Speaker C

Well, I'm seeing that in the Pinecone I ran from your test project, but I'm not seeing it in the Pinecone of my project. Like, but I was successfully running RAG. So basically, like, Pinecone has the data from my database, I'm making queries to it. At what point are these records being generated and why are there so few of them? Is there something in the code itself that is generating this?


---

Speaker A

So I have no idea in terms of your code, but if you're using Supabase with Pinecone, it might be on Supabase and Pinecone might just be connecting to that. It might be one idea that I have. In terms of what I'm doing, this is automatic. So in terms of what I'm doing, whenever I do an upload of vectors, that's automatic. I'm not. So Pinecone will do this automatically if the vectors are saved natively on Pinecone. I believe if you're using Supabase, I would check the Supabase database for the vector if I had to get.


---

Speaker C

So when we ran the Docker Compose upload file or whatever, that's when these records ended up in Pinecone.


---

Speaker A

Yep, that's.


---

Speaker C

And then why are there only 10? Because, it seems like there would be a ton.


---

Speaker A

Because there's more. It's just paginated. There are 2,000.


---

Speaker C

Okay. Okay. Okay.


---

Speaker A

Okay. Yeah.


---

Speaker C

Awesome. Thank you. Sorry, just a little unclear.


---

Speaker A

No, no, no. Don't be. Don't be sorry at all. There are only four summary vectors because there are four books, but the chunk vectors are over 2,000. Okay. No, but good question. I believe the reason you might not be seeing what I'm seeing is probably because of the Supabase connection, but that could be something we test. Yes. Grades are pass/fail. So if you have AI components working, that will probably be the primary way you can pass this week. We would like to see your Slack application functioning properly in terms of the functions that we highlighted in the project document. And then we would like to see one or more AI features working properly in Slack. Yes. So, Stephen, we'll have our AWS workshop tomorrow where we'll talk about a standardized DevOps flow and what you should be using and what services at what time. But I didn't want to ruin anybody's applications or what they had for deployment so far.

But I didn't want to ruin anybody's applications or what they had for deployment so far. So that's not something I'm going to be introducing now. We'll talk through it tomorrow, but it'll be for the next project. And some of the services, Steven, will be just to give you a head start, Amplify. And then I believe that for the next project we're just going to be using Firebase or Supabase, so the only AWS service you'll really need is Amplify. Okay, that's all the Slack questions, and I will do the Zoom questions, then call it because we were running super late. What was the code, Ashran, in the terminal, John? I just copied the code off the README of the project. It was just some Docker commands, so they're all on the README of the project, and copy them. Will we be able to improve upon? Yes, you will be able to improve upon your submissions. You have until Sunday to do another submission. So if you feel that you want to continue to make your AI features better, then yes.

So if you feel that you want to continue to make your AI features better, then yes. And Ben, great question. Is the next project going to be for weeks three and four? Yes. So what happens is the first week we always rebuild, in the second week we always add AI. We talked about optimizing Rectangle. How much of this is expected from Array? Great question, Palmer. In terms of your week two stuff, I would like you guys to try, if you have some time, to implement recall, to see how many of the vectors you're pulling are actually relevant to the query. But in terms of grading, at the end of this week, the only thing I'm looking for is functioning AI features on top of a functioning Slack rebuild. Okay, awesome. Thank you. What is the best way to easily deploy apart from AWS? My app works perfectly. Logo Rishabh. You can probably just deploy on Netlify or Vercel. I know. And Amplify. Frankly, they're all really easy. You just click buttons and connect to your GitHub repo.

Frankly, they're all really easy. You just click buttons and connect to your GitHub repo. What's the point of having two indexes on Pinecone? The reason that we have two indexes is because the first one is the index of the summary and the second one is the chunks. So indexing a smaller vector database with just the summaries is really quick. It helps me find where to look, and then I can just quickly look in that specific book's vectors to find the answer. With week one still not back where I was, should I just focus on week two and Sengo? Yes, Paul, focus on week two. I love this week's main theme: AI features. When I was trying to set up stuff on AWS over the weekend, I was in search. If you're deploying just your front end, I would just try to use something else. But if there are more issues happening, then I would bring them to the AWS workshop tomorrow. We'll just walk through it. I think we just need dedicated time for AWS, and that's what we'll do tomorrow.

I think we just need dedicated time for AWS, and that's what we'll do tomorrow. Okay, I believe that's all the questions. Today, Zach will be finishing out his part two of the Hackalot project, Build, Live Build. That's going to be at 3 Eastern. So I'll give Zach a chance to say something about that and what to expect. And then beyond that, I'm going to call class today and thank you guys for joining me and giving me your attention for such a long time.


---

Speaker B

Yeah, so we're going to continue. Man, you don't know how hard it was for me to sit all night. Good thing I had plenty of unfun work to do to keep me busy. But I wanted to finish it yesterday. So today we're going to add roles, reviewers, so that we can take grades, that kind of thing. I mean, you know how if you attended the first one, you know how it went, and we take a little bit off the cuff. I do like the idea of doing RAG, but I will say this: I'm not going to do RAG. While it's your week of RAG work, I'm not going to give you the answers. So if we do RAG, we'll do that next week. But I would really like to. I think we can get most of it finished out in another hour, hour and a half. So we might have three hours of work into it. Maybe we might need to touch base again just a little bit to finalize our first event. And then this weekend we run a timed hackathon just for fun, and we'll get some reward.

And then this weekend, we run a timed hackathon just for fun, and we'll get some reward. Somebody can, you know, win some money or a gift card or, I don't know, whatever it comes out to. But I think that's a pretty cool testament to what it is that you're learning for us to be able to build an app, deploy it, use it, and have a winner, all within five days. Sounds like a lot of fun. So that's what we'll be doing today. We'll just continue. So, yeah, we'll see you then.


---

Speaker A

All right, see you guys at 3 Eastern. I hope you have a great rest of your day. Thank you for your time. This was awesome, and thank you for engaging so much with all of us. Okay, see you. Bye.


---

Speaker C

Bye.


---

Speaker B

Thank you.
