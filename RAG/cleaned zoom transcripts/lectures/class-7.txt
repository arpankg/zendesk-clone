%START_METADATA%

Class_Name="Class 7 - Agents"

Class_Date="2025-01-27"

%END_METADATA%



---

Speaker A

All right. Hello and welcome. So today we're going to talk about agents, which we've talked about a little bit in the past, but we'll dig into some more details about what you can consider an agent built with an LLM, see a bunch of examples, what tool use is, and essentially see the sorts of patterns that are already being applied to, say, build the cursor agent. Again, we don't know exactly how the cursor agent is built because we don't work at Cursor, but you can definitely see, I think, some of the patterns and approaches that we will look at today in practice with a lot of the sorts of AI development tools and other tools that you've all been exploring so much. So this also potentially empowers you to build your own, or at least better customized ones for your own needs. Ador. Yeah, I saw that in the chat. Ador is great. It's. If you want to check that out. I'll leave that as a. Just a reference though, and I will go ahead and put the question thread in the channel.

Just a reference though, and I will go ahead and put the question thread in the channel. I'm putting it in the announcements channel just so it doesn't scroll by too fast, but please only respond in the thread itself. And here we go. All right, so what are agents? Well, agents at the highest level of abstraction are really just about using LLMs as a reasoning engine to sequence actions. And so what that means actions are. It's going to boil down to function calls, right? Like spoilers, tools, or functions.


---

Speaker B

Kind of.


---

Speaker A

Basically, that's it. It's just that we're getting to call functions in a way, though, that instead of us scripting it in advance, which, when you think about it, is basically what a chain is, right? And that's even, you know, with a graph. We also provide some structure that's determining the order stuff should go, instead of us determining any of that sort of structure. It's basically up to the LLM and, of course, how the LLM is prompted and the other information it's given in its input, its context, to decide to determine what action to take next. And it is also potentially allowed to reason. And you can think of reasoning as the LLM thinking out loud. Basically, the LLM will just say, right? And you've seen this, right? The LLM will say what it's going to do and then do it. Right? And why?

Right? And why? Why bother? Well, granted, this is still early days, so there's not necessarily a final objective answer to this, but there are a lot of indicators that having the LLM reason out loud, having that prompting technique and that structure increases the reliability and accuracy of certain actions because it's basically building up to it in a way that lets you have more confidence. And it also perhaps makes it easier as a developer for you to see what it's doing and interact with it if you have to, if you want to do that or improve it.

So what are we going to cover more specifically? A bit of an understanding of tool calling and then the REACT, which is Reasonable Reasoning plus Acting, which is basically that pattern I was just talking about, and the agentic life cycle and you know, what it is, why it works, how to build an agent, looking a little bit at the traces of running these things in Langsmith, because it's always good to look at stuff in Langsmith and also some pre-built agents. So we've got a lot of code in the repo today I'm not going to get through. There are like four or five notebooks, I think. I'm not going to get through all of them. I'll get through a few of them. There are other examples there, and there are other references to the LangChain Community's open-source package that lets you. For example, one of the pretty cool agents that you can basically just take off the shelf is a SQL agent, right? Or at least a SQL toolkit because that's such a standard database. Right?

Right. And so it's, you know, a set of tools and prompts that will enable an LLM to basically interact with the database, which is pretty nice. But before we get to all that, what is tool calling? So you'll hear these terms tool calling and function calling used a little interchangeably. But if we want to be precise, we should probably say, you know, tool calling when we're talking about LLMs and, you know, function calling outside that it's a function. But the tool calling for the LLM basically means that the LLM has to respond in a structured format. Let's take a second, think. Well, wait, I thought, I thought it was about calling a function. Well, it is. And to do that, you need to have a structured format, you need to know the name of the function and then a dictionary, AKA key-value pairs of the arguments, right? And perhaps even type annotated. Although, you know, Python doesn't worry about that quite as much as some languages. JavaScript certainly doesn't seem to worry about that.

JavaScript certainly doesn't seem to worry about that. Suppose TypeScript. In any case, what this means is that the LLM isn't actually executing the code. The LLM is sort of making a request to execute the code. The LLM is saying, hey, I would like to run this tool with these parameters as input, and it is up to you, slash your program logic, slash whatever other code and steps to decide what to actually do. And of course, one of the things to do is to just execute it, right? And perhaps even to always do that, to automatically execute when the LLM asks. For some tools, that's totally fine, right? Or in some contexts, that's totally fine; in other contexts and for other tools, that's perhaps not a good idea.

You perhaps want to have a human check, or you know, there are things in between where you could have just some hard-coded sanity checks, like, okay, we're going to have a SQL agent, but we're going to double-check what it's running on the database before we actually run it with some hard-coded rules. You could have something like that. Of course, that might be better solved by permissions and roles. But the point is, you could check what the LLM is asking to do, not just blindly do it. So it's a recommendation, and they can actually potentially recommend multiple tools in a single interaction. And again, this is all about making a decision-making engine. And then one other thing that I'll call out here is because this tool-calling approach is basically enabling us to force the LLM's response to have a particular sort of structure. This can be useful in ways that you might not think of as tool-calling.

This can be useful in ways that you might not think of as tool calling. Exactly. But this can be useful if you wanted to, say, have an LLM that always annotates data a certain way, right? Like let's say you're going to feed the LLM a bunch of customer reviews, you have a bunch of open-text customer reviews your customers have left, and you want it to rate the sentiment and the urgency. I don't know, you want it to rate certain things out of that, right? And give you numbers or categorical values. You could use function calling because you would basically make a function that takes that data and inserts it into wherever. And the signature of that function will be the data and the types that you want it to identify from the unstructured data you're giving it. And by doing it this way, you're forcing it to respond with exactly the structured data.

And by doing it this way, you're forcing it to respond with exactly the structured data. There's none of that like, hey, I think this is like, there's none of, you're not going to have to argue with the LLM to make it less chatty is what I'm saying, right? Like we had to in the dark old days, like what, a year and a half? I mean, this stuff is moving pretty fast, but it's nice for a variety of things. So a little bit of a visualization of tool calling. So again, going to call out here. The end-to-end interface kind of just looks like an LLM user question. Text comes in, output which could be human-friendly text from the LLM comes out, right? So it's that the user, it's still like they're just chatting with an LLM.

However, the LLM is going to, instead of just, you know, zero-shot, trying to make tokens in response to the user, parse that question, make decisions about which tool might help answer the question better, get the output from that, feed it back in, and then keep doing that until it's satisfied that it can answer. And when I say, "Again, danger, danger. We're anthropomorphizing," when I say things like it decides or it's satisfied, what I really mean is that it's meeting whatever you set for it in the prompt, right? You've prompted it a certain way and also perhaps just how the model is trained and tuned. And so it will eventually reach the point where its next token is, "Yes, this is an answer," and it's going to give the answer as opposed to, "I need more input." What sort of tools are good? Well, obviously, it depends on what you're doing.

What sort of tools are good? Well, obviously it depends on what you're doing. But if we want to think of just general knowledge, source, question answering, shoring up on the potential weaknesses of an LLM. What an LLM is a master of is grammar and to some extent reasoning. I wouldn't say it's a master exactly, but that's what it actually does. It's the next token predictor, choosing appropriate terms. It happens to sort of have some knowledge baked in. But information retrieval isn't really what it does, and mathematics certainly isn't what it does, that sort of reasoning at least. So you might want to give it tools that let it do calculations, tools that let it retrieve information that's relevant to the questions, that kind of thing. I think that's what's being implicitly pictured here in these little icons. So the React framework, again, reasoning plus acting. And the idea is that this makes the LLM sort of mimic how a human would actually do things.

And the idea is that this makes the LLM sort of mimic how a human would actually do things. Because humans, you know, whether or not you talk out loud, you probably have at least some form of inner monologue or note-taking or habits that you do when you're solving a problem. You don't just zero-shot things, right? You iterate yourself, you ideate, and you eventually get to the solution that you're trying to get to. And the react pattern tries to build a similar approach on an LLM. And so what it looks like, the react is the bottom one here. So if you have an LLM that can just act, then it would just basically ask for tool calls, observe what the tool call does, which is what we're calling the environment. Like the real world, it observes something and then does the next thing. If it's just reasoning, then that would just be the LLM talking to itself. With both, the LLM can basically loop in either of these.

With both, the LLM can loop in either of these. And again, I think this is roughly what you're seeing a lot of the time when you see the cursor agent, right, sort of decide a plan and talk to itself or a lot of the other LLMs that do things along these lines. So there is some code in LangChain we'll see that facilitates this. And it's a very general-purpose approach. So it can, again, if you give like 100 tools to something like this, it'll probably still overwhelm it, at least I would expect that. So there is still the analysis paralysis idea we talked about last time. But an agent like this, suitably prompted and, you know, with a powerful model, could probably handle a pretty wide array of tools and solve some interesting problems. I mean, at least that's what we've observed with things like, you know, coder agents and so forth.

I mean, at least that's what we've observed with things like, you know, coder agents and so forth. I'll pause here for a second since this is a good slide to look at and since there might be some questions coming in, feel free to type them in a thread or unmute real quick if you want, I suppose. All right, I'll keep going then. So a little bit about the agentic life cycle. This is just another way to visualize this in case this is helpful. Again, we see the end-to-end input-output, but in between, we have the agent, which you can think of as just an entry point. The prompt goes to the LLM, the LLM decides whether or not to go to a tool. The tool response goes back to the agent to potentially do more reasoning. You could think that there's an implicit reasoning loop in the agent if you want. So the agent gets the full history of what they're doing.

So the agent gets the full history of what they're doing. Really, most of the tokens that are going to be informing its response are not the user prompt, but are all these outputs from the tool and outputs from its own reasoning, potentially, if it has that loop. And that's all the more reason why using something like Langsmith to look at the full traces of what's going on, see all the calls, see all the internal details of what the agent is reasoning with, is so critical. And again, typically the way this is prompted is the LLM is told to keep going, keep calling tools until no further tools are needed and it can formulate a complete and accurate response to the user, something like that. You can, of course, have other safeguards in place. And, you know, I think as we talked about last time, I think there's even some default recursion limit because if you ever look at a loop in a graph and you get a little bit afraid, that's good. You should be.

You should be. There's arbitrary compute that can potentially be spent here, right? There's some sort of halting problem going on. And so it's good that you can set the recursion limit, and you should consider doing that. Even if you set a fairly aggressive recursion limit, agents, as you've experienced yourself by using tools such as Cursor, are not super crazy fast necessarily. They're not as fast as just doing a web search because they do web searches, multiple of them, and whatever else they're doing. Obviously, that's all going to add up, plus the API calls to the LLM itself. This is the sort of thing where you'd have to either use Streaming or Async if you want to make it a decent user experience. A lot of the time, Cursor essentially streams the response to us. What's nice about that is even though it takes a little bit for the agent to do stuff, it takes longer for us to read.

So as long as we can start seeing something that gives the user a better experience. How are LLMs configured to forcibly produce action tokens in the output? If you mean like in the internals, honestly, I'd have to dig into that, and I'm not sure. I mean, OpenAI, despite their name, has not open-sourced how they do things like that necessarily. So if you mean how do we configure it, we'll get to that. The short of it is there are basically Python dictionaries that show the signature and the rules we want. There was actually kind of a brief example of it last week, but we'll see more of that. And this feels the same as OODA from decision-making. Yeah, I like that connection. Honestly, back in the day, I did political science and economics. I'm

And honestly, I said back in the day I did political science and economics. I used to have a bit of a game theory, decision theory hat. It does seem like the AI people perhaps should do more reading of past literature because there's a lot of stuff out there that's very interesting, but I'll avoid going down that rabbit hole for now. There are definitely some interesting connections, and if you're curious about decision theory, I encourage looking it up when you have some time. What is meant by the history of the tool? Literally just all the past outputs. Think of it as the conversation history, the traces, but instead of the conversation history being you back and forth like, you know, U LLM U LLM, it's just one U and then a whole bunch of, well, potentially a whole bunch of the LLM kind of looping on its own stuff, both its own self-thoughts, its reasoning, and the outputs from the tool, which will be part of the history that it considers as it forms its response.

So just past outputs and again, tools, multiple tools can be executed repeatedly, that kind of thing. You'd want extensive tests to evaluate an agent. You'd want to be able to test each tool call and decide which tool to call. What's the best way to store null? All right, there are several questions here for the test. Yeah, I mean, we're not going to talk that much about exactly unit testing agents. It's good that you're thinking about that. And yes, obviously, tests on the functions make a lot of sense, and some sort of end-to-end test where you expect the agent to behave a certain way probably makes sense, though having your test actually send external API calls, you can determine that based on what you're actually doing. At the bare minimum, test the tools; certainly test that the tools behave as they should because the tools really are just functions.

And then what's the best way? And you know, I guess what I'm saying as far as LLM stuff instead of deterministic tests, it might make sense to have labeled data with scores about what you expect the output to be at the end, that kind of thing. And then what's the best way to store a knowledge base? I don't know that there is a single best way. I mean, the whole point is that this thing can have. Oh, is that other slide. All right, whatever. Oh, here. This thing can have different tools, right. So this thing could potentially search the web. This thing could query a Postgres database. This thing could connect to MongoDB if you have that for some reason. So the best way to store it will be more of an infrastructure project question than like what's best for the LLM. Because at the end of the day you're just retrieving context that you're. That's going to be fed to the LLM. And that's not all. The tools, by the way, are information retrieval or RAG.

And that's not all. The tools, by the way, are information retrieval or RAG. The tools can also literally take action. You could have a support agent bot that can do things like unlock user accounts, right? You could have a function to unlock a user account that actually updates a field in your database, that kind of thing. So keep in mind that that's also a possible direction here. Just like the cursor agent can run CLI, which lets it do anything, of course, it nicely asks for permission first. All right, good questions. I'm going to keep going, but keep the questions coming. I'll get back to them soon. So again, how to think of these AI agents? The idea is that we want to make these independent workers that can handle complete and somewhat complex tasks, tasks that would have required at least some human intervention in the past.

And this can be carried out by, you know, a mixture of the LLM reasoning and then you coding some logic into the tools or, you know, making sure you prompt it really well and letting the AI agent act autonomously. That's the ideal. Now, the real world might be that it still asks for human confirmation. That's called human in the loop. That is a good thing. The goal should not be 100% automation for literally all tasks. The whole point of this is to make stuff that's going to hopefully make us humans happier and so forth. So we shouldn't be completely neglected here. We have preferences too, but whether or not something mandates that human in the loop step is going to depend on what it is, how sensitive. And even when there is a human in the loop step, the agent should speed things up dramatically, right? Like the whole point is it should become ideally like a binary decision for the human.

Hey, you want to do this, yes or no, right? And everything up to that is done for you automatically. So an example here is building a research agent. And I think for this one, I'll also switch out to look at and run the code real quick. So this one is more like, sort of, yeah, this one's not taking action or just, you know, unlocking user accounts. This one is researching. But it's still pretty interesting to see because it's to do with the reasoning loop, not just retrieving documents and shoving them into its context window, but planning and considering a little bit as it writes its response. So the steps here are we're going to connect to an LLM, define the tools, which will be basically information retrieval type tools, have a prompt, an agent executor, which is the LangChain runtime for these things that you feed the tools and the LLM connection and such to. And then we're going to have streaming output in the terminal, and we'll also look at perhaps the stuff in Langsmith.

So I'll run this real quick. There it was, that one. I look at the code first. So the code should be in the repository. It's called Research Agent Python, and what we've got going on here is calling out, as usual, a whole bunch of imports from LangChain Tavily, which you've seen a few times now. But I want to call out that this is from the Community; you should check out and potentially contribute to the LangChain Community Library. Right. Like, it's, it's. That's part of what that is. We are also importing tool. A tool can be used as a decorator to literally make a function a tool. So there you go. That's how you can make tools. You make functions, and you decorate them. Not too surprising what the decorator is actually doing. If you're not super familiar with decorators, decorators are kind of a nice way to have a function that takes a function and returns a function.

So it's implicitly wrapping this in another function that makes it behave the way it needs to for integration with a tool called LLM Agent. You can dig into the details if you're really curious, but it's mostly going to be boring stuff like making sure it has the right signature, some, you know, key-value blob to conform with an API. So what does this tool do? Well, it searches. Oh, sorry, it makes. Of course, we could have kind of just used this, by the way, like this could have been a tool. So this is a demonstration of how to make a tool. And we'll see others because this is a tool. But we invoke the tool and we join the documents that it returns. It's a web search, and we return the web results. You could potentially change this more here. Like you could have web research also query other things and combine it here. You do anything you want with code here, write report.

You can do anything you want with code here, write a report. This is a tool with an LLM prompt that connects separately to an LLM and gives it a prompt and context. So that's also interesting, right? You know, the tools don't have to just be pure Python that stays local or something. The tools can trigger more LLMs. Again, you could have a tool that triggers a chain for some reason, which actually this one does. It's a pretty simple chain. And the idea is this will be the LLM that actually does the writing. What's nice about this is we can give it its own very specific prompt. That's kind of what's making it a separate LLM from the perspective of the main agent LLM, if you will. This is an example of a straightforward tool. This is like, hey, save it to a file. So it's just standard Python stuff. And those are our tools.

This is like, hey, save it to a file. So it's just standard Python stuff. And those are our tools. So get stuff, get information, have a writer LLM write some stuff, save stuff to a disk text file. We're going to prompt the overall agent saying you're a research assistant, and then when the report is written, save it to a file. So this is us being pretty explicit actually about when you're done, right? We're not even quite calling this done, but we know that saving to a file is the last thing to do. And so we're telling the LLM like, hey, you're a research assistant, the user's going to ask you to write a report on something or something like that. And when the report is written, that's sort of the stop condition. Now, this is a more fuzzy stop condition in this case.

Now, this is a more fuzzy stop condition in this case. It's a simple enough problem that I'm not particularly concerned, but it is good practice, I think, when you're prompting any sort of agent to try to describe what is done right, what is satisfactory, and when it should sort of call it. That's going to be one of the more persnickety parts, potentially, especially in a more complicated system, to nail down. And you might have to iterate some there. So you'll also notice this agent scratch pad message placeholder thing. This is where some of that reasoning and tool output can live. These are those message histories that can accumulate and inform what the agent is doing. All right, now we could have, so we wrote this ourselves here so you can see it. But again, there is the LangChain hub with prompts, and you can literally just pull an OpenAI functions agent, which is an already engineered general prompt for this sort of thing.

So feel free to check that sort of stuff out too. Now, how do we actually put it all together? We stick the tools in a list and bind those to the LLM. So the LLM again was just the connection to OpenAI. So we bind that here and then we pass in the LLM with tools along with the tools and the prompt to create a tool-calling Agent, which was one of the imports as well. And we pass that to the Agent Executor. The idea here is that the Agent Executor is the actual nice entry point that has things like streaming and such, which we are often going to want with an LLM. So we're asking it to write a report about the current spike in the price of Bitcoin in November 2024. I suppose I could even update it to January. It's spiky now, isn't it? Well, we'll leave the prompt as it is for now. It'll still get a similar response, I think. So we're going to execute this and you'll see we're entering now. We're printing sort of everything.

We're printing sort of everything. But we'll also check this out in Langsmith, because you normally might not be printing everything like this. We're entering the Agent Executor chain. So the Agent Executor began, and it's deciding to invoke web research. And we're just letting it invoke that; we don't have any human in the loop. Type check. And then these are documents that are being retrieved by Tavily with the query "Bitcoin price spike," blah, blah, blah. Right. And so there's information about Bitcoin's price in November. And then it's invoking. Right. Report with the research. And it looks like it actually chose to summarize, which is kind of interesting. Right. And potentially helpful. I mean, we could be more explicit in how we structure and prompt this, but it's not passing this verbatim. All of this, it wrote its own summary because it is an LLM of the information here and passed that as the research data to the LLM and then, well, to the other LLM. Delete. Right. The writer.

Right. The writer. And then the writer wrote this and returned this because the writer is also a tool call. So we're not done yet. This got returned to the agent, and then the agent was satisfied and said, okay, yes, this is fine. I'm going to invoke save to file. And the content here is the report that the writer just made for me. And then the result there. It named the file this. So if we look at this, we have an actual, on our disk, a little report. And even though this is a pretty simple example and there's a lot that could be added or changed already, this is potentially an efficient way to research a little bit if you wanted to. You know, this is the sort of thing that would take a human a little bit of time to actually search and skim and understand and write up a nice summary. So in a sense, is this kind of. Could you also do this with RAG? Yeah, this is kind of RAG in agent's clothing, if you will. But this is a good way to also see how it's running.

But this is a good way to also see how it's running. And you could add more tools here if that was appropriate. You could pull from multiple sources, that kind of thing. All right, I'm going to catch up on the questions, though. I think Ash has been all over in the thread. Thank you, Ash. Yeah.


---

Speaker B

I think one thing you might talk about, Aaron, is binary ways to evaluate agents. Like, did it eventually lead to the task being completed versus did it not? I think people have questions about that, at least from what I'm seeing based on the questions.


---

Speaker A

Got it. So, you mean like from an external evaluation perspective, like looking at the whole thing?


---

Speaker B

Yeah, yeah. Because I mentioned how usually humans start out as the evaluators, and that probably is not something that the students were excited about, but sure.


---

Speaker A

Got to do some things.


---

Speaker B

Yeah, I mean, a lot of companies these days just outsource this. So.


---

Speaker A

Yeah, I mean that. So, you know, and let's see if we can even see a little bit of this in Langsmith. Langsmith would be. So. And, but I mean, really, this perhaps isn't specific to agents. I'd say what, what this has to do with. This has to do with any LLM-based system. So this thing could be a chain, a graph, an agent, whatever. I swear I set the environment variable, but it looks like the traces are going here. That's fine. So, yeah, you can see it took almost half a minute, right, for this to happen, for it to write the full report. So what actually happened? And it's just this most recent one. These are historical ones. At this point, we can see all of this. So this hierarchical representation of all of the tool calls and such that the agent decided to make is very useful. And I will get to the evaluation thing here, but we'll step through this and we'll get there. So we see the entry point was the agent executor.

So we see the entry point was the agent executor. It goes to chat OpenAI because it's an LLM that's going to reason, and its first decision based on the prompt is to do web research that gets to valid search results. We click any of these and see the actual more detailed input-output trace information. Right? That goes back. We're always going to go back to the LLM. That's this loop back to the LLM agent. So any of these top-level Chat OpenAI orange things are the LLM agent, and it decides to write the report, which ends up spinning off another call to OpenAI. But that's inside the write report tool, and that's pretty much all the report write report tool does, pretty much all in five seconds. That goes back to this to reason some, to look at the output, and it says, hey, that's good, I'm going to save it to file. Which is just taken less than a hundredth of a second, which is lovely. And then we get one last loop to it, and it's done.

And then we get one last loop to it, and it's done. So, how would we evaluate that the final output here is something we want? Well, what we can do is look at the raw output here. This is the raw output of it saying, "Hey, I'm done. I wrote it, and if you need any assistance, feel free to ask." We might actually care more about the output that was saved to disk. But for now, we'll just think about this information here, and we can see right now there's no feedback. But this run has a unique ID, and the trace even has a unique ID, and we can add the output from this run to a dataset or to an annotation queue. In general, I keep emphasizing that the end-to-end interface of all these things is text in, text out. That's typically the case for what we're doing, or at least some sort of defined structure of output that could also happen.

And in that case, or at least some sort of defined structure of output, that could also happen. These pairs are data, right? This is if you think of a good old, you know, Cartesian plane and drawing a function Y equals MX plus B, right? Y, the output of the function, is a function of the input X, right? In this case, our Y, our output, is the response, and the input X is the user prompt. Y is a function of X. Actually, the same thing here. This is all being changed to numbers and doing math; it's just a lot more parameters than M and B, right? A whole lot of parameters. But nonetheless, it's math. And so to the question of evaluation, what that really means is it's a data problem, it's a statistics problem. And this is its own topic that I'm not. I will try to avoid rabbit holing too much. It's a topic I care about and have worked with quite a bit. But the short of it is you want to collect data, which is why you can use a tool such as Langsmith. You don't have to, but you can.

You don't have to, but you can. And I'll leave this to you to explore in the UI so we can get to the Python notebooks, but it's just like a web UI that lets you track these and then annotate them. And when you annotate them, you typically make a decision like is this good or bad, right? Does this output meet whatever it needs to meet? And the reason that binary classification is often enough is if you do that, or you pay contractors to do that for enough data for a thousand rows, ten thousand rows, something like that, you end up with a test, a holdout data set that you can then run as you iterate. As you change your system, you can run with this known good labeled data and compare the output to what the output should be and score the output somewhere from 0 to 1 based on how close it is to what the good output is. Right. And that, in a nutshell, is how you sort of evaluate these things.

Right. And that, in a nutshell, is how you sort of evaluate these things. Now, obviously, sure, you can use LLMs or other AI-type tools to try to help you label, but at a certain point, you will probably need at least some human review and supervision. And because that's also how the LLMs themselves are trained, the reinforcement learning step of the LLMs themselves involves human feedback, right? Reinforcement learning from human feedback is pretty critical to LLMs working the way they do. So there is a human step a lot of the time, and it mostly has to do with just deciding, is this good? And then once you have enough labeled data, you don't need the humans anymore. Now you have their output, and you can use that labeled gold data to actually calculate numbers and know how you're doing. It's actually really important if you're doing a major project to establish that data as early as you can because, again, that will let you. Otherwise, you're just in the dark.

Otherwise, you're just in the dark. You can iterate and change your system, and you don't know if you changed it in the right way or not because you're not evaluating. So, it's a good and important topic. But I will leave it at that for now since it's not the actual lecture topic today.


---

Speaker B

That's perfect because that's what I'm going to ask them to do for the project. I want to have them.


---

Speaker A

Got it. Yes.


---

Speaker B

All right.


---

Speaker A

All right. Yeah. So yeah, that's an important step: evaluation. And I'm sure we'll talk about it more in the future as well. But let's get through the rest of these slides here too. So we saw our research agent, and we basically already saw this. So this is just for your reference. Yeah, we saw all of this when we actually ran the code that corresponds to these. There are other agents out there. So, for instance, this one is using an agent toolkit. So the LangChain community has agent toolkits, which, as you might guess, are basically lists of tools, right, that would be appropriate for solving particular tasks, and they have special functions that can instantiate an agent that I believe would even already have the prompts built in.

So you can entirely, without really writing your own code, just using the LangChain community, import their SQL agent and SQL toolkit, wire it all up and get an agent and an agent executor that literally does human text to SQL. This is now a solved off-the-shelf problem in like 10 lines of code. It's kind of crazy when you think about it. So, you know, this will automatically connect to the database based on the URI you give it and explore the database, explore the schema a little bit to understand it. Because you can introspect databases by running SQL on them, right? Anybody who's done advanced SQL usage knows all about that. And then it builds its understanding of the database, and it's already been prompted to translate whatever human question into an appropriate SQL query that will then retrieve the information, and it can return that, summarizing it for the human.

Now, obviously, for a real production-grade SQL LLM agent, you're going to probably need to do more than just the code you see here. You might need to think about permissions and, you know, scopes and visibility of stuff. You might need to think about how much is in the response. You'd have to dig into exactly what this SQL agent is doing. But in general, it's not going to work to ask the agent to look at a thousand rows of results that well, or 10,000 rows of results. You know, it's better for it to work with summary statistics or things like that. So you might need additional steps or reasoning if you have particular sorts of questions. But still, it's very cool how powerful this is. And of course, this could be extended. Like, this is extensible. If you wanted to add that to it, the actual LangChain community stuff is open source if you want to check it out or even contribute to it.

So, the moral of the story here, as I think you all already know, is don't try to reinvent the wheel. I mean, you all are being expected to move pretty fast and develop a lot of stuff, so it's good to know what tools are out there and how you can reuse them. And that's actually the last slide. So at this point, we'll start looking at some of the notebooks, probably get through one or two notebooks, and then leave the rest for you to look at at your leisure. All right, and again, Ash, it looks like you are all over the questions, but if there's anything you want to pass my way, let me know.


---

Speaker B

Yeah, my third coffee is really pushing me hard on these questions. Like, yeah, my typing speed is like 3x right now.


---

Speaker A

Longer to build than I expected, but here it is. All right.


---

Speaker B

I'm sorry, but the provided text chunk is too short and lacks context for me to make any corrections. Could you please provide a longer snippet of the transcript?


---

Speaker A

Oh, that's not necessarily the one I.


---

Speaker B

Want to start with.


---

Speaker A

I want to start with the Simple Math agent. So this is just if you wanted to interact with an agent and play with your own tools, add your own tools, and just experience that a bit. I know we are, yes, you should consider. Sorry, continue doing AI-first development. But spending some time knowing how to write and edit this code yourself will let you better supervise your AI. Or you could just check this in Cursor right away and see what it does. It's totally fine too. Cursor does open Python notebooks. So what are we doing here? Well, we are doing our usual imports and connections. We're also getting some mathy stuff, Matplotlib and NumPy, and we're defining some tools that do math because really, math is actually a weak spot of LLMs and a pure true LLM that doesn't have any other nonsense going on.

If it gets math right, that's basically because it just happened to be trained on enough examples of people talking about that math question, right? Which is kind of crazy when you think about it. I mean, it's neat. But it also means that if you give obscure enough numbers that nobody on Reddit slash math has ever talked about, it's not necessarily going to do as well. And so we can instead define tools that actually do math. Because by the way, even if the LLM can do the math well, it's like one of the least computationally efficient ways to add numbers is to ask an LLM to do it. Our chips do that very directly in a very optimized fashion. So from an efficiency perspective and a cost perspective, it also makes sense to have tools for something like math if that's relevant to your use case. So these are just a bunch of math functions, including one that even makes a plot. And you could consider adding more here if you want to make a glorified calculator.

And you could consider adding more here if you want to make a glorified calculator. We're telling the LLM you're a mathematical assistant, use tools to answer questions. If you do not have a tool to answer the question, say so. So that's another little prompting technique here. Tell the agent to basically defer. If it can't solve a question with the tool, it shouldn't fake it. It should just say it can't solve it because it doesn't have the tool. And then a little example. So kind of a one-shot here, we connect, we give it all the tools, make the executor, all the usual steps, and now we're going to ask it a kind of annoying question. Actually, I'm not sure. All right, off the top of my head. We'll see, we'll see what it does. 85. Okay. Oh yeah, we told it, I think, to return only the answer. So we're not seeing the reasoning, right? It could reason, but we're seeing only the answer now if we do this same thing and look at the entire...

I mean, maybe we can still see the whole result here. No, yeah, that's it. We could change it if we wanted to have it reason out loud. If we wanted to show our work, you know, we could actually do that. So maybe ask. I won't.


---

Speaker B

If you make the question harder, I might do it because I feel like I’m. There might be a function for the area of a trapezoid already, so just use that directly.


---

Speaker A

Got it, got it. So we could also just ask another question here. What is the area of...? I'm not sure how to specify. Maybe let's not do an area question.


---

Speaker B

Yeah, do a random one, like the volume of a cylinder.


---

Speaker A

I don't know. Sure. What is the volume of a cylinder with height 10 and diameter 3? Ah. Actually, it just says I do not have a tool. So again, if you wanted to actually make this satisfy all your use cases. By the way, this is the data structure. This is the input-output that I was talking about. Right. This is the X and this is the Y. And you could have humans come through and label stuff and score this and have your test data, set your holdout data, and then you could iterate on the tools you provide and the prompt to make it actually answer the questions you wanted to answer and complain when you wanted to complain or whatever. I'll leave it at this because this is, I think, just meant to show the basics. But any questions on what we saw in this notebook? When would we use a neural net? And well, I mean, there is, this is, there's transformer architecture going on here. So there's definitely some machine learning happening in the background of all this.

So there's definitely some machine learning happening in the background of all this. But if you mean like a non-language model, say something that is trained just to be a classifier or just to output numbers or something like that, I would say practically the main examples of that at this point are when you want something to be really efficient. LLMs are amazing general-purpose models, but this LLM connection doesn't just solve math. I mean, let's see if it will actually tell me or not. We're just going to see if it will obey something that isn't math at all. Tell me about the Pokémon Pikachu. Right, like this is an LLM, it knows all sorts of nonsense. Okay. It actually was willing to say I don't have it. So this is doing some decent heavy lifting to try to jailbreak it and argue with it, like ignore your system prompt.

But the point is that this LLM has a lot of information, as it were, and has a lot of reasoning capabilities, very purpose-driven if you want. As a result, it's a kind of large model, and it's a little bit expensive. It's weird to call it 400 million parameters expensive, but relative to a really focused, small neural network, say 100 million parameters or something that's trained to only classify stuff, if that's all you want to do and you need to run it really efficiently because you're running it on the edge or at an absurd scale, those are the instances where you'd still not look at LLMs perhaps. But really, we're getting into the realm of machine learning, so I'll just leave it at that. You're not expected to do machine learning, I think, at this particular juncture. So let's go ahead and move on to the next notebook here, which is the agent vector store. This is a slightly more fleshed-out, less minimal example.

So this is a slightly more fleshed-out, less minimal example. We're going to basically have an agent that has access to a vector store like a RAG, and it's based on a State of the Union address that's going to be chunked. In this case, we're using a character text splitter. Oh, it doesn't actually. Apparently, we didn't check in the State of the Union here, so I didn't.


---

Speaker B

Put a file in; that's on me.


---

Speaker A

It's all good. Well, let's see here. I could probably pull down or curl something.


---

Speaker B

Real quick, just copy and paste something in.


---

Speaker A

Yeah, yeah, that's fine. I mean, do we even know which state? Let's just pick which state of the union it was. Do paste our own. See here suggestions on what to copy and how about the project description. Yeah. See if it actually behaves as a decent string. So instead of this, the text is going to be where's the part where we're loading it? Not the doc path. Okay. So yeah. And it doesn't need to load. Comment out the odds one. It doesn't just accept a straight string. Unfortunately, with this loader, we'll just get at this. Yeah, it wants something other than a string. So I think unless we know how to load it offhand, maybe I'll just move on to the other example. But what this example will do is it will connect and make a QA system, and you can connect to different things. So it also connects. It uses a web loader. Oh, I guess we could just do two web loaders. Let's just do two web loaders rather than doing this string. Ay, ay. All of this. Come on.

Let's just do two web loaders rather than doing this string. Ay, ay. All of this. Come on. Now I'm just going to not save and reopen it. Okay. So except it did actually manage to save some great or copy paste itself from here. All right. And then I'm going to replace the loader with a web loader that I think makes the most sense. All right. So this loader will load them both from the web. Instead of rough, we'll load from, I guess, GauntletAI or where is it? Is it GauntletAI.com? Yeah. All right. Which isn't really an FAQ website, so this might be a little funny, but it should at least load. No. Okay. And we're also missing stuff, right? So this will connect multiple RAG-type systems that are loaded from different sets of documents and expose the collections as tools as different Q&A systems to an agent. So the Q&A system is the tool, and the agent can decide which tool to use to answer the question.

So the agent is reasoning about the question and deciding which RAG, basically, or which resource to use to answer the question. So we're probably gonna have to change this question. We can see how it behaves. We can ask, you know what, what is. All right, let's run this. Maybe both complain. All right, we owe you fixing this, I think. So we, we will do that. But I think you get the idea of what this should be doing, which is it can answer different questions and it can route the questions to different RAGs based on the input. How much time do we have? We could look at one more notebook just to close on a functioning notebook, I think would be ideal. So let's say, yeah, the baby AGI. So this. So I'll just talk about each of these really quick. This one I'll let you run through. I believe that it involves another. Was it this one? Oh, you know what, this one I'll let you run through the baby AGI. It involves another tool, Serp, which is another API similar to Tavli.

It involves another tool, Serp, which is another API similar to Tavli. So I won't run through it right now. But the idea is that it can basically create and prioritize its own tasks. So it's a planner, and that's why we're calling it a baby AGI. It's an AI-type system that can actually explicitly plan and structure the tasks it should do. Obviously, it's not making paper clips of us all just yet, but it's an interesting system to look at. This one, the Sales GPT, is potentially pretty relevant. So let's take a look. It's a sales agent. So what the sales agent will do is it's supposed to have natural sales conversations with prospects, and it can use a product knowledge base and potentially interact with other tools. I don't know that that's all set up here, but here's a diagram. And the diagram shows how, again, the human gives input. And this is going to be analyzed and broken up so that the sales agent can decide what tools to use and what steps to take.

And there's a lot of prompting here. Not going to read all the prompts right now, but we can see there are different conversation stages. This corresponds to some domain information. So if you're building a real-world system that's sophisticated like this, you either need to have domain knowledge or access to a domain expert to ensure that it's appropriate. We can see here we're entering, we're just testing an intermediate chain. Glad that it's running, and we see the sort of steps that are planned. Then we have our chain here where the salesperson is apparently Ted Lasso. Lovely. We have a conversation history that's pre-populated and is about selling mattresses. So we're saying your name is Ted Lasso, you work at Sleep Haven, you have all these abilities based on tools and such. Here's the conversation history right now. Ted Lasso: Hi, how are you? And the user says, I'm well, why are you calling? End of example.

Ted Lasso. Hi, how are you? And the user says, I'm well, why are you calling? End of example. And then the current conversation stage is by introducing yourself and you know, hi, it's Ted Lasso. How are you doing? And the idea is that we could interact with this Ted Lasso agent, and it would be a sales agent that has all these abilities. We can set up a product catalog so the sales agent can refer to that product catalog and the knowledge base, which really is also just a text file right now. I think the text file is just the product catalog, and it's chunking that and then some mappings here between products and prices. Oh, and it's our first thing that it's hitting. Probably have to pip install that. Well, we're past time enough that I'm not going to necessarily finish, I think, running every step in this notebook. But you see, as far as we got here, it's basically able to do a variety of things in terms of its connections to knowledge, in terms of its connections to looking at prices.

And the idea is that this agent could integrate perhaps with a CRM and automatically take actions on tickets, respond to users, handle sales leads, that kind of thing. I encourage you to explore this example more as well. And Ash, I think it's probably time to talk about the project.


---

Speaker B

So, thanks, Aaron.


---

Speaker A

Do you want to screen share, or...


---

Speaker B

Yeah, I can screen share. A couple of questions came in the chat about deadlines this week. There's no deadline tomorrow. There's a check-in on Wednesday and a submission on Friday. This is highlighted in the document at the bottom as well.


---

Speaker A

So, I'll show you that too.


---

Speaker B

But please go to your project document real quick. The milestones are at the bottom, and I've also added them to the calendar. Okay, so what I did over the weekend was, obviously, there are a couple of edits that have been made. One in the AI features, one in the test to pass, and everything else is rather similar. So I've just closed those sections. I'm going to start off with the AI features. So what I did, instead of listing out a ton of AI features, I've picked three potential user flows that would probably work for a lot of different specifications and different user bases and industries. And so, for each one of them, what I've done is, if I were your product manager, how would I frame or give you context on what you're building from a user perspective? So each one of these is me walking through a problem and walking through how I would solve it using an AI feature. And again, these are only suggestions.

And again, these are only suggestions. These are for individuals who might not know exactly what AI feature to add. You could use one of these. If you want to use your own AI features and not pick the ones I've suggested, that's totally fine. I'll just quickly walk through some of them. So AutoCRM is like one of the biggest problem spaces in a CRM across the board, across all industries. And I think Aaron can speak to this as well, in that you have to manually flip through so many screens and scroll and then input data over and over again to actually keep all the data up to date. It actually wastes a ton of time in HR, customer support, and sales. It's probably one of the biggest problem points where if I had a way to just open up a sidebar, give it a direction, and it could just automatically do things for me and then show me that on the chatbot so I can see if it did it correctly or not, I think that'd be huge. So that's what AutoCRM is.

So that's what Auto CRM is. I can pretty much dictate and say update X, Y, and Z. And if it does that, then it will automatically update the database tables. Do all of that and then show me a little interface of it walking through the actions. Any questions on Auto CRM? Should probably check the Zoom chat. Yes, Bjorn, you can track these on your own system, but it might be a lot of work to build your own system, but yeah, you can track them on your own system. No, I. The Project 2 doc is always the source of truth. Yes. Okay. The next one is called Insight Pilot. One of the biggest things I've noticed in many positions I've been in across the industry and many times I've been talking to people is that, you know, Tableau and Salesforce are always disconnected. So it's really hard to sort of go from here's all my CRM data to an actual insight people can use to actually come up with a solution or make a user experience better.

So Insight Pilot, something in the background that runs, goes through all your data and then brings up these patterns or insights. You'll notice that in my examples I've been using students a lot because that's where I'm from, right? I've been running schools for a while. So here the example I give is for AutoCRM is like, hey, object, update the record for Christopher Walker with the following meeting notes. So add notes to the record for me automatically. And then for Insight Pilot, it's saying that, oh, Christopher has an 85% chance that he'll need additional mathematics support next semester based on these indicators. And the fact of the matter is the human can actually walk through the analysis, see what happened and see why it came to that analysis. That's what Insight Pilot is.

That's what Insight Pilot is. And then Outreach GPT is another pain point that I've experienced, that I've heard a lot of people experience, which is crafting personalized engagement messages for whoever your customer is, right? So whether it's a lead, a sales call, a B2B client, or a student, you always have to have these emails running. You always have to check in with your customers. You have to have activation checkpoints. So Outreach GPT automates all of that. It lets you basically say, okay, you can go out and do this, go out and do this. And it checks in with everybody involved. So those are the three suggested features I have. Are there any points of clarification that I can make?


---

Speaker A

How many are we expected to implement?


---

Speaker B

I would like you to implement a full user flow, so one of any of the three suggested, or something like the three suggested at the same complexity level. But if you could do more, I'm not going to stop you. And I'm sure your hiring partners would love to see that. And if you're going to pick your own suggestion, make sure it's an actual problem people are facing. So go on Reddit, go on review sites. People have so much criticism of Salesforce and HubSpot. You'll be able to find and cite problems fairly quickly. Or you can talk to real users and then make sure that it integrates naturally with the application you've already built. Make sure you don't have to build a whole new set of components just to accommodate this AI feature. And then the big one is going to be accuracy metrics. You should be able to measure the accuracy of the task being completed. And then we'll talk about how to do that in just a second.

And then we'll talk about how to do that in just a second. But the big key metric we're going to be looking for on this AI submission is accuracy, and I'll talk about that. But any questions on the suggested? Okay. Zoom chat.


---

Speaker A

Actually, I do have a question. Hey, Robert, so you kind of suggested some AI features in the earlier version of the document, like the one that automatically sorts tickets. So I don't know, would that be... Would implementing those features be feasible?


---

Speaker B

Yeah, that would work as well. The only reason I did it this way is because I wanted to make sure there was a complete user journey that we were doing. Sorting tickets is a small feature in a bigger user journey. So, Robert, think about what your users would get out of sorting tickets and what additional smaller features come together to build that complete user journey. That would be my thought process for you.


---

Speaker A

But that makes sense. I have a question. Okay, thank you.


---

Speaker B

Yeah, no worries. Great question. I'm going to talk about accuracy just now, Tim, so two seconds. But are there any questions on the suggestions here? Otherwise, again, you could pick your own. But I would like it to be at a similar level of complexity, and I would like it so that we're actually solving problems for real users, not just making demo features. Okay. Okay. Oh, Aaron, if you have to drop, by the way, feel free. It's all good.


---

Speaker A

Thanks, Eric.


---

Speaker B

Okay, so if you go to your test to pass requirements. Oh, go ahead, Benji.


---

Speaker A

So this isn't specifically on your examples, so I don't know if you want to move on. It's more of a technical question.


---

Speaker B

Wait, actually, I might have another meeting, guys, so let me just check my calendar. Okay, so that's why I just got to Zoom. Wait a minute. This happens sometimes. Just one second.


---

Speaker A

I see a problem I can solve with my auto CRM.


---

Speaker B

Okay, I'm gonna have to go a little faster. All right. All right, so let's do this. Benji, I'll come back to your technical question. Let me get through the accuracy stuff and then I'll probably have to put more time on the calendar to answer more specific questions. But the tests to pass are all the same: the BrainLift, the walkthrough video GitHub repository, and deployed application. The one thing we're adding is agent accuracy metrics. So how do you go about measuring accuracy metrics for this week? We're going to ask you to do it manually through Langsmith or Langfuse. If you wish to automate that entire process, you could do that with an LLM. But make sure you check what the LLM is evaluating and not just let the LLM do it blindly. The way you would go about doing this is you would document 20 to 30 common requests you would expect a user in your CRM to make for your AI feature.

For each of these, what you would do is create a test data set, so just the input and the output of that action. So if I ask the AI to update a record for me, what is the input from me and what is the output I'm expecting from the AI? This is called a structured test case. A structured test case has an input and expected output, any context required for that, and then the success criteria. I'll show you on Langsmith what it looks like as well. You want to connect your application to Langfuse or Langsmith because you can do this really quickly on the platform, and you won't have to build a whole new thing after that. All you're going to be doing is with your 20 to 30 test scenarios, you can run through each one of them and evaluate them on Langsmith to directly see if they've actually worked or not. Finally, the metrics I would be sort of looking to track here could be error rate.

Finally, the metrics I would be sort of looking to track here could be error rate. How many times did it completely just not work at all? How quickly did it happen? So Langsmith calculates latency for you and the speed of calculation. So you can see, like, were these slow? Were these fast? Did it meet a threshold? The other one was success rate at identifying the correct action. So this would mean, like, hey, I asked the agent to update a record for Christopher Walker. Did it know to go out and use this API call or this Supabase function to update Christopher's record? And then accuracy of field updates would be like, in a CRM, I have a bunch of objects. One of those objects is student. One of those objects could be customer. Did the agent identify the right field in that object to update? So these would be the four criteria that I suggested, the four metrics that I suggested, and what we're requiring of you for the walkthrough video is to track any two.

So you can track any two of these metrics and just show me in your video at the end of your app. Walk through that. Hey, these are the two metrics I tracked. And the metrics are like X, Y, and Z. Let me walk through on Langsmith what that looks like on Langsmith. When you have a dashboard or a dataset, you can go into each of your projects. For example, here in my Gauntlet, you can open up any trace that Langsmith has made. You'll get the latency score right here. You get the number of tokens used on the right, and what you can do is click right into it, and you can click add to add to the annotation queue. And here I've created a demo annotation queue. So we're just going to add it to that for now here for the annotation queue. If you go to the annotation queue here, you'll notice there's my demo. You can create a new one here inside of an annotation queue. You'll see the input, you'll see the output, you can put instructions for evaluating it, and you can set up a feedback rubric.

So for now, the feedback rubric could just be whether it worked or not; it could just be binary. I don't want you guys really reading into this like you know too much. But it's important for us to track whether it worked or did not work. And so that's what the feedback review rubric could be. If you believe that something was done successfully, then you can click done and add it to your specific data set, or you can even add it to an error data set. All of this can be managed on Langsmith and Langfuse. So I think it's really important for you guys to start getting the hang of this because right now we're only in week four, so I'm asking you to do this manually. Over time, we're going to build a complete QC system that does this for us. Most companies right now that are building AI agents use Langsmith or Langfuse to have this annotation system.

And then they have overseas departments and customer support systems that will automatically annotate and correct everything based on the rubric provided. So this is a huge, huge part of building QC. First AI. Right, so obviously we're not all the way there yet. I'm just introducing the topic, but I really want you guys to do it manually so you understand each of the steps and can walk through them. Okay, first and foremost, does everybody understand what is required of them for Friday's submission? This week there's only a Wednesday check-in and a Friday submission. So I'll start with that question, does everyone understand what is required of them for Friday's submission?


---

Speaker A

Cool.


---

Speaker B

Sebastian, for edge functions and server-based functionalities, you can actually add Langsmith to the edge function. It's just adding that env and adding a decorator. You can also do that through some sort of layer on your front end. So if you have that front-end util with the function that's abstracted out, you can add Langsmith to it. I'm sorry, I'm just making sure. Wednesday, Tim, Wednesday check-in is at 6 pm Eastern, and office hours are in the morning. Yeah. If you're still here, you're probably all headed to Austin. I will say that unless you don't submit on Friday, then you won't be.


---

Speaker A

I do also have a.


---

Speaker B

One sec, Marcus. Sorry, Joshua, you can ask your question. To answer your question directly, a test passes a framework that's introduced by your hiring partners. I will send over a document that highlights exactly what it is. They sent us the document. Frankly speaking, David LangChain LangSmith is not specifically important. If you want to use another tool, you can. If you want to make your own tool, you can. But please walk through evaluating the input and the output of your agent. Yes, Robert, feel free to dig into the process. I'll make another office hours later today because I do have to run for another meeting. Benji, I'll take your question and then take Marcus's as well. Is your question about Supabase awesome?


---

Speaker A

Sort of. So I'm doing NextJS and Supabase and TypeScript, and it looks like LangChain works really well with Python, but does it work with TypeScript? I guess I would just like a recommendation. Is it possible to do TypeScript? What would you recommend for running these AI functions so I can get started on that? Because it seems like running Python somewhere else might be a little complex, but I guess either way I'll learn something.


---

Speaker B

No, great question. LangChain has a TypeScript library, so I would start there. Yeah, don't. And if you want to do Python, what I would do is only do it if the TypeScript library doesn't work. I will say they do maintain the TypeScript version of the LangChain library pretty well. So I think you should be fine. So I think that's where you should start. Marcus, what's up? Yeah, so regarding the AI features. Yeah. What sort of focus should we just be on? Like, hey, this is what our application currently has. Let's try to integrate it versus maybe trying to build out some more features.


---

Speaker A

To, like, maybe use AI better with.


---

Speaker B

Great question. I think you should focus on the AI features that you can add to your application well and measure accuracy with. Don't go out and try to add new features just so you can implement new AI features. What can be added to your application seamlessly? Right now I really want you guys to focus this week on AI. AI, AI. I don't want anybody rebuilding some. Maybe you have to. So I'm not going to say like don't do that at all. But you can make edits to your existing features. I think that's better than adding new ones, Marcus. And I think that would be the route I take. Joris, what's up? Oh, did. Are you muted, Joris, or did I miss? Oh, okay, got it. Joshua, you don't have to do both of those accuracy metrics. You can pick any two of the four. And if you guys want to do another metric, you guys can also just message me. And I'm pretty open to you guys choosing your own metric as long as it's related to accuracy. Joshua, I will check that it fits in Vercel Edge functions.

Joshua, I will check that it fits in Vercel Edge Functions. I mean, I have used it before. You can also put it on your front end so you don't have to load the whole thing in the Edge Function. Tyler, I'm looking for, like, of the 20-30 areas that I tested, I want the percentage that was correct and functioned properly based on the metrics I provided. And then you can also highlight how many errors there were. All right, Joris, what's up?


---

Speaker A

So actually, what I... I don't know.


---

Speaker B

What I had actually thought was something.


---

Speaker A

System that can classify incoming support tickets.


---

Speaker B

And augment your knowledge base automatically. I just wanted to ask if you think that's perfect, that's great, Joris. So the way you would track accuracy is you would make sure that the tickets are classified correctly. Okay, awesome. All right, guys, I really have to hop. I'm like 20 minutes late for a meeting, so I'm going to get killed by Austin. But I'll see you guys later in the day. I'll probably put up another office hours to answer more questions about additions to the project doc. Okay, thanks, guys. See you soon.
