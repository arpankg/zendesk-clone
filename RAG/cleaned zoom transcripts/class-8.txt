%START_METADATA%
Class_Name="Class 8 - LangServe, LangSmith and LangFuse"
Class_Date="2025-01-29"
%END_METADATA%

Speaker A - Ash Tilawat

All right, so I just want to say that the flight emails haven't gone out for US based participants, so the international ones have. And yes, you will need to put your credit card down for incidentals. This is still, you know, has some of those hotel features, even though it's an apartment. And so that'll be a $50 hold just in case you damage any property for the duration of the program. And Rebecca is working on the flight details right now and should have more details on that by end of day. But yes, we are pushing on the logistics and I'm having Austin help out as well. But Aaron, back to you.
Speaker B - Aaron Gallant

Right, all right, well, we've got quite a bit to cover today, so I'm just going to dive in. All right, I'm starting the question spread as always and let's get to it. So today we are covering really a variety of tools, tooling topics. None of this is particularly heavy in the concepts, but it is definitely very practical and should help you build, build things, build more things, build, you know, better, stronger, faster, all of that good stuff. And in particular there's going, you've already seen one of the things on this slide a fair amount, Lang Smith. Langsmith, as you know, is a platform that's integrated with LangChain, which is where we're getting a fair number of our sort of imports from for LLM things. And so when something's instrumented with Lang Smith as LangChain code basically automatically is, you get to see those handy traces for debugging and you can even potentially interact with, you know, making data sets and such. On the Langsmith platform, Langfuse very similar to Langsmith in a lot of ways, a competitor. The main difference is that it's open source and you can self host it. You don't have to self host it. I will not be self hosting it. Today they have a cloud with a free user level that's acceptable for certainly basic use. But the ability to self host is obviously pretty nice for whatever use case or industry you might have where that might be important. And it's just good to have an alternative as well, especially if you end up perhaps not relying as heavily on the LangChain ecosystem and writing your code More directly against OpenAI's API or perhaps even something completely different. Langfuse is still there for you and yes, exactly, self hosting, it's often a data security thing. I mean it will even be regulated in some industries that like this is what you have to do. And then Lang Serve is a framework or really a bit of framework on top of what is already a framework known as fast API. Any of my Python backend folks out there. So what this basically lets you do is it lets you very quickly spin up a fairly useful and featureful API for any LLM runnable. What is an LLM runnable? Well, we'll see. But you've already, you've already been using them like it's already something that basically whenever you instantiate a LLM object that you can send chats to and get responses from that, that end to end interface I keep talking about, that is basically considered a runnable in most situations. And so a single connection to OpenAI is a runnable, but so is a chain, right? At least if you make it have that interface end to end. And so is sort of graphs and so are agents typically. You know, they're typically going to be runnables in that you interact with them as if they were an LLM, even though there's other stuff happening. You know, they might hit a rag, they might have tools, they might go in a loop, who knows. But it's still a runnable. And so obviously being able to serve quickly a little microservice API for that runnable is handy, right? Is something that will let you integrate that into other infrastructure, have it be part of your, your, your production environment and just generally be a useful tool in your toolkit. So that's what we're covering today. And we'll also touch on deployment on Render. Render is for anybody out there who remembers Heroku, which I guess is maybe still a thing, but it's different now. Anybody who remembers what Heroku used to be, Render is fairly similar to that. It's something that you can point at a repo and if the repo is set up correctly, which for the most part means does it have an obvious entry point with like a Docker file or something, Then it will pick up that repo and set up a CICD for you and just put it out there and deploy it. And it has a free tier that will work for at least experiments and some initial development stuff. And it's basically multiple orders of magnitude simpler than aws, I would say. It's also like, it's really good for prototyping and learning. It's not necessarily going to be the thing that would scale your company up into the millions and millions of dollars. So there are still reasons to learn things like AWS for sure. But Render can be a very handy platform to do some of that initial development experimentation, especially if you've Been frustrated with other platforms. So let's see here. It looks like. Yeah, all right, I think, I think the questions are being, being handled. Yeah, part of, part of. So this is all Python, but we'll show it including deploying it and it's pretty simple to get going. I don't believe Lang Serve specifically exists in JavaScript. I'm pretty sure it's, it's, it's built on Python and fast API and you can, you can still develop more on it. Of course you're not limited to what it does, but anyway. All right, so we're going to get an understanding of langsmith and well, I, I'll talk a bit about this one, but I might spend a little bit less time showing Langsmith just because you've already seen langsmith a number of times at this point. But we'll also get to laying fuse and observing the calls and then we will understand Lang Serve and deploying the render. So what's our motivation here? Well, I, I think you basically know it's observation, seeing traces, debugging and it's doing it in a way that's sort of LLM first or LLM native, you know, why not just shove all this in a normal observation platform? Well, this gives you tokens, this gives you, it's not on this screenshot but you know, it even gives you your cost summary, like how much, how many micro cents you paid for your, for your call. It's usually one of the few microsense, I guess millisense maybe that kind of thing. And it also has a built in data annotation sort of experience. Ash, you mentioned you already stepped through that some in office hours, is that correct?
Speaker A - Ash Tilawat

Yeah, I did yesterday and I did the day before, so.
Speaker B - Aaron Gallant

Okay then I'm not gonna, I'm not gonna spend as much time on it in this lecture, but there's material on it in the slides.
Speaker A - Ash Tilawat

Yeah, I went through like three or four times.
Speaker B - Aaron Gallant

Got it. Okay, then y'all have had opportunity to see it and that stuff is probably recorded too if you want to see it again. But to emphasize the point of doing that, basically in this brave world of LLMs, one of the purposes of humans is to provide high quality annotated data that we can use to at the very least validate and measure the LLMs. But also depending on your scale and what you're doing, potentially to tune or train them. That's beyond what we're talking about right now. So please focus on the idea of just measuring, having some data that you'd be able to validate. Because that way when you experiment, when you change the LLM, which can be as simple as just changing the prompt, then you can run it again on something that's consistent and see its behavior. Right? And a little bit about evaluation, since I'm guessing you all are curious, the sort of naive approach, especially if the input and output are both pretty much unstructured, is to basically just have humans rate it as acceptable or not. 01. You can potentially come up with finer score gradient than that, but that's usually not that valuable. But whatever you do, especially if you start working with other people and you've got multiple people going through and evaluating or such, have some sort of code book, have some sort of guidelines, and even if it's just you worth spending some time thinking, planning and writing down how you do this, because you need to be consistent because that will really affect the quality of what you're doing. And of course, there are, if you are able to, there are situations where if the output is structured enough, you might be able to just automatically calculate the score, which is obviously a lot nicer, right? If the output, if you know what the output should look like in a deterministic fashion, you can just do a direct comparison or some math on it. You can, you know, depending what problem you're solving. And then lastly, it is possible also obviously really to set up a system where you just have a different LLM do that subjective evaluation, right? Like you write your code book into a prompt and you tell the LLM to evaluate the output of this other LLM. Now don't start. There would be my advice, because building that prompt requires that that sort of code book and some of your own experience and expertise and judgment and audit it quite a bit at first. But it can potentially work, especially if, like, let's say you've got a simpler LLM in the system and a sort of more sophisticated LLM or at least a differently prompted LLM evaluating it. So yes, you, you can also potentially set that up and that allows you to automate or at least speed up the evaluation of something that might otherwise be slower and subjective. So, but that's, that's enough on that. I think that's been covered too. And if there are any other questions, we'll handle it offline or out of the lecture. So Lang Smith, again, you've seen Lang Smith. I don't know if you've seen this specific decorator or not, but what this does is if you have some function that you're just writing that isn't part of the LangChain framework and so it isn't automatically instrumented. You can slap this decorator on it and that basically instruments it. It's fairly straightforward. So if you're all in with LangChain and Langsmith, but you want a little bit of your own code, but you'd like it to show up in the, in the traces, you should absolutely be thinking about this and adding this because it'll just add more traces. More logging. All right now from Lang Smith, let's take a peek at Laying Fuse. So Laying Fuse and you can go to cloud.langfuse.com and set up. I'm already here and I've already created a project. So you have to start by creating a project and when you create it it will give you API keys. Really what it gives you is a key pair, public private key pair. You need both. You know, you can see this in dot sample for where to put them. But once you have that you'll see little different UI obviously than, than Lang Smith. And you know there's some different functionality but the, the high level is basically the same. It's, it's, it's a LLM first observation platform. It's going to have costs and tokens and stuff and it's also going to have. And then there's even human annotation and evaluation. And remember, see what I just said about how you could have another LLM evaluate. Looks like they added something. I don't think this was here last time I looked at this. This stuff develops pretty quickly. But they have an LLM as judge evaluation section so you can check that out if you're curious and make a new evaluator that is potentially just another LLM carefully prompted to evaluate the zeros and ones of what it gets. Now how do we actually use this? Well, let's look at the code. I think that'll be the most instructive here. So first off, end dot sample just calling out you need these two, the secret key and the public key. And then you need the host. The host is going to be cloud, the same as URL here cloud layingfuse.com assuming you're just using their cloud, which again free tier that works and I'm not sure their pricing model exactly but you can also self serve. So this could be like whatever ec2 something something inside your virtual private cloud theoretically. And then once you're instrumented, the way you use it in actual file is you see this Laying Fuse decorator for observe and that works kind of similar to the look to the decorator for the traceable decorator. Right. And so because Laying Fuse isn't providing like Laying fuse doesn't have as large of a framework as laying chain that, you know, has all sorts of stuff. And so there's less pre instrumented stuff going on. So you will need to explicitly add your observe annotator where you care about it. And they do have their own OpenAI thing. And so we are connecting using their OpenAI library, which might mean that this object is also automatically instrumented, but we're still instrumenting this function as a whole, which is probably good. So thank you for that. All right, so let's go ahead and run this and see some traces show up. So Lang Fuse demo is what it's called. Okay. So this is what I want and I want to be here, use demo. All right. Yeah. The other stuff in here is basically laying Smith. So I'm going to skip it for now and we'll come back to it at the end if there is time and interest. But you've already seen Langsmith a bit. There's several Langsmith example PI files and then there's a rag with Langsmith and a notebook. And what is prompting demo exactly While we're waiting for this? Ah, more dad jokes, few shots and so forth. So if you just want more code to play with connecting to LLMs and seeing traces. But at this point you've already seen that stuff. And while we're waiting, there's a. Yeah, I'm gonna have to dig in this just for that, for the dad jokes. Yes, there are dad joke generators in this repo if that's something that, that you are interested in. Okay, so it ran here. And what this actually did, if we look at the actual code here for Laying Fuse demo is it tells the AI it is a storyteller and starts it off with, you know, sci fi prompt, supposedly. And we get a story that there's a vibrant planet called Lumaria with bioluminescent flora and fauna. Okay. And it just cuts off here because we have max tokens. 100. Like that was the decision we put here. So it just goes up to 100 tokens and then it stops. But what we really want to see here, okay, there's our trace. There's our. Are these milli. Millipennies. It looks like we spent six and a half millipennies. I'm counting my zeros correctly. So there we go. And there's again similar information, different ui, different organization, different team making this competitor. But Similar purpose and you should check it out. There's a lot of emphasis on latency. I actually kind of like this landing page from a high level perspective. I think that with one trace it's not that significant. But if you had like a production thing with thousands and thousands of traces and more even this would give you a nice way to slice by, okay, how much you're spending by each model type. Right. And you know what your actual latency tendencies are and what your different percentiles are. So because what the killer with latency isn't your average latency, it isn't your median latency, it's usually like what's your 95th percentile latency? Because that's what happens often enough that a user will notice it. But you know, even though it only happens 5% of the time, it tends to have a disproportionate impact on their perception of your product. Everybody always remembers their slowest load. So, you know, it's nice to be able to break that down and look for what's. What your patterns are. And then of course we can just like actually look at the trace in more detail and see similar output to what we saw on Lane Smith. Keeping all my lang lang langs straight here. And we see, you know, the payload, Jason's. It looks like it's kind of hiding it, but I assume it's has. Well, no, this one really is just empty. Well, it should have similar information, a similar hierarchy on the side here. Of course, this is just a single one shot called 1 LLM. So there's not a lot to this, but if you did something like an agent or a graph, you'd get the same sort of nested hierarchical representation of it in this tree. And you can also see that there's annotation options and data set options and scores. And as we discussed, they even have a LLM as a judge feature which you can explore. It looks cool. And they also have, you know, you can maintain a little prompt library and have prompts that you can potentially evaluate and score differently. And there's a playground for just interacting with LLMs if you set an LLM API key. So there's, there's some nice functionality here, honestly. As I said, I think they've iterated this quite a bit since I looked at it like a couple of months ago. I'm trying to remember which of these features are new exactly. But yeah. Ash, what do you think?
Speaker A - Ash Tilawat

The LLM as a judge is really cool. So you can pretty much. Yeah, they didn't have this before, but essentially what you could do is you can set up a new evaluator. So what it will do is it'll look at the input output test case and based on the prompt that you're sending to the LLM, it can either score it or give you some sort of binary result or also rank it. And then the human annotation is exactly the same as the Langsmith annotating queue. They pretty much, you know, that's fairly similar. But the LLM as a judge is a really nice way in the future, when we are more confident with our evaluation framework to then just have the LLM do it.
Speaker B - Aaron Gallant

Yeah, and I would emphasize that I would not start with LLM as like, yeah, I'm sorry, but you got to do your time. Like, you gotta, you gotta do some human evaluation first, simply because if nothing else, that will inform you about how to best prompt and evaluate the whole system. And even, frankly, even in a situation where you're working with other people, you're going to, you're planning to hire contractors to be your annotators or something, you should still annotate a few cases yourself first because you want to know what you're talking about when you're training your.
Speaker A - Ash Tilawat

Contractors or whatever to piggy piggyback off what Aaron is saying. For Zendesk Project, you are required to manually.
Speaker B - Aaron Gallant

Yeah, right. Right now we're asking you to. I don't want to see any lo evaluations.
Speaker A - Ash Tilawat

I want you to walk through your process in the video. So I just wanted to make that very clear.
Speaker B - Aaron Gallant

All right. So, yeah, a lot of cool stuff here, but I, I think you probably get the idea and I think, you know, I'm assuming from what I've seen, we don't have a specific requirement or guideline between Langsmith or Lang Fuse. Is that correct, Ash? Like, you can use whichever one you prefer, but you should probably use something. Some observation.
Speaker A - Ash Tilawat

Yeah, yeah, we're recommending Langsmith or Langfuse, but you can pretty much. I'm. If some people want to make their own. And I said, sure, but the one we recommend is Langsmith or.
Speaker B - Aaron Gallant

Yeah, we'd recommend one of these two. Yes, you could potentially roll your own. Is always a theoretical possibility. And you know, you could also potentially use other platforms or even platforms that aren't really intended for LLM stuff. But, you know, maybe you're integrated with them in other ways or used to them. But. Plus I'm sure Datadog or whatever is frantically adding LLM features as we speak. But. But we recommend one of These too, if you don't have strong opinions other because they've got good integration and options and will help you keep your velocity high. All right, any questions coming in on this? It looks like everything in the thread is mostly handled. I mean, there was some chat and debugging about like, okay, I see my trace, my performance is bad, like my latency is high. How do I actually deal with that? It's a good question and it's going to depend on the specifics. But the general thing I'd say is if at all possible, dig into the trace to find the step you're stuck on. Like the step with the high latency, assuming it mostly is in one step. If that's not possible, make your tracing more granular, you know, add more traceable decorators, that kind of thing. And then once you've identified that bottleneck, you need to identify, you know, what your lever is. It's possible with LLM stuff, just the number of tokens is going to always increase the latency, typically. And the APIs themselves can be a little unreliable or spiky sometimes, but if it's consistently high latency, like every single call takes 10 seconds or something, then yeah, dig into it, find the bottleneck and either reduce it or worst case, figure out how to make it async or otherwise sort of, you know, streaming or do something to sort of work around how slow it is to still provide a decent user experience or you know, plan to pre process that kind of thing. All right, so that is it for laying Smith and laying fuse. Here's. I mean you saw this code in the file, but here it is for your reference and reference on data sets. Now to laying serve. So what is laying serve? Well, as I said, it's a framework and really 90 more 90 some percent of the code of laying serve is fast API. So if you are a Python person and you're familiar with fastapi, you should be very comfortable. And if you're not, that's fine. Fastapi is a little biased because I'm a Python person, but it's a pretty approachable framework. It's not quite as minimalist as say Flask or something, but it is not a maximalist framework like, like Django or whatever. What it really does because it's not going to provide explicit pre baked data models or user auth or migrations or stuff like that. What it provides is a very convenient way to set up say an API, right, and to define API endpoints. And it's built on fairly performant Things by default like UVicorn. And it can support Async by default for its routes. So that can play nicely with the Async API calls you might be making to your expensive LLM. And anything that you build with Lang Serve really is just a fast API app. So for most, like, if you get serious about developing on this, you'll end up looking up documentation not for Lang Serve, but fast API, like fast API documentation, fast API, how do I add middleware? How do I deal with jwt, whatever. Whatever your questions are, that would be fast API that this is built on. But what's cool about Laying Serve is it's particularly suited towards making a microservice that serves a LLM runnable. And if that's all you want, I mean, you don't have to expand it beyond that, you know, you don't have to add a whole ton of other routes or other functionality necessarily. I mean, you might have to figure out middleware and off, I suppose, but that's probably about it. And so this microservice that you get from Lang Serve more or less mirrors the API that's exposed by the runnable that we've seen in past, where you can invoke it, you can stream it, you can asynchronously invoke it. And this will basically just let you do that, but with HTTP requests, right? Which again, microservice architecture that can be very convenient for integrating with other services or tools that you might have in your system. And yes, this is an alternative for those of you who are perhaps hitting issues with other ways to serve your LLMs. And yeah, let's, let's dig into it here. So what we see here, some of the stuff that fastapi gives you and thus Lang Serve gives you, it gives this documentation page and this documentation is automatically generated. It's open API compatible, so there's a JSON schema representation of it, if that's relevant to you. But you can also, we'll see when we look at it live, you can navigate it and see and actually experiment with all these endpoints, send requests. And obviously you can, you can and should turn off this documentation page for production deploys. It's for development purposes. But this, this Swagger endpoint on Slash Docs gives you all this information. And all of these endpoints that you see pictured here are endpoints that are not defined directly by you. These are endpoints that Lang Serve will make for you based on the runnable you provide it. So your job to use langserv is really just make a runnable, which means at the very least, you know, connect to OpenAI. But you know, it could mean make a chain, make an agent, make a graph and then instantiate laying serve and tell it. Here's my runnable and that's really it. You'll see the code is pretty darn simple and you get a lot of functionality exposed. So what is the functionality in a nutshell? Invoke is the most straightforward endpoint that, you know, you'd sort of be using or thinking about the most just mean run the thing. But you can also batch, which if you want to give several inputs and get several outputs and you can also stream. So let's see here. I want to look at the next slide or do I want to. Yeah, this is, this is enough for this. So let's get to the code. Swagger versus Dot HTTP files. Swagger. We're getting swagger for free here. That's what I would say. And so that's what's nice. And it's. Oh, and also it'll be doc. Like if you're writing good doc strings, that documentation will be propagated here, which is handy if you're doing full. So Swagger is this documentation page that's being generated for us. So it's being generated based on the code so we don't have to document. It just gets made for us and it's interactive, so it's almost like a little demo page. So what we're going to see here, let's look at this example LangChain server. And this is a conversational agent. So what we have here, skip over all this for now. We'll see. We. We are still actually importing fastapi and we're doing some lane chain imports. This will basically be how we make our runnable. Fast. API uses Pydantic for its data models. So if so by using Pydantic you specify the types and fields on things and it's that pedantic information. Plus I don't, I don't actually see doc strings, but if there were doc strings on. On things that would. That'd be what's pulled into the documentation. We've got an in memory vector store and some other OpenAI prompting stuff. And then this, this is it. This is Lang serve. This is like the workhorse thing that you'd be using for mvp. Lang serve and add routes and add routes will add these routes that you see here. So how do you get it all set up? Well, you load your environment variables or however else You've got your secrets set. We're setting up our silly MVP vector store. So this is going to be a very basic rag type thing and we're retrieving these, we're calling these Eugene's thoughts. So we're anthropomorphizing a bit, I guess and this is what Eugene thinks or. No, we're not calling the LLM Eugene, we're just saying there's somebody named Eugene and they have these thoughts and that's what the rag gets and that's the tool. So it's a tool using agent and then we're saying you are a helpful assistant. You have a scratch pad. And then we set streaming to true when we connect to the LLM and that will let us stream individual tokens which will play nice with of all the other functionality we have here. And then we do the usual binding the tools to the LLM, putting it together in a chain with the prompt and the output and having that be the agent executor with the tools. So all, all this code that I just sort of moved through quickly is basically the same as code we saw in the agents lecture. So hopefully you recall that then this is how you actually make a fastapi app. You just instantiate a fastapi object and you can pass some information, just a metadata for it. And then this because we, because fastapi expects pydantic models for its functions, we're making some simple sort of catch all models here to annotate types. So we're saying the input to our function is going to have a single field input of type string and we're being a little bit loose about the output. We probably could be more precise but you know, any will work as just say the output is whatever and then all we have to do is run, add root and this is what we imported from Laying Serve. We give it the app which this is what we instantiate with fastapi and then we give it the runnable. The runnable is the agent executor. That's the final result of all of our LangChain LLM code. So add routes with fastapi app and runnable and I'll explain the rest of it in a sec. But that will add all of these routes. That will make this already into a ready to go microservice exposing our LLM API. What is this stuff doing? Well, you'll see here the two classes we put here, we're basically specifying some types and I think you don't necessarily have to do this. Like if you don't do this, it'll probably still run in serve, but doing this makes the documentation work better. Decorates. Not decorates, but annotates tells it what sorts of things to expect. And similarly there's config options. We're just saying that it's named agent, but I imagine there's other config options that could go here. So that's it. Like we are not ourselves defining a single route. You know, if you're familiar with making backend servers, which I imagine you are, you know that you typically make some sort of function or method and you put a decorator on it or otherwise say like this is what happens when a request to slash whatever. We're not doing any of that. We are just calling this magic helper function with the fast API app itself and the runnable and then we get all the stuff we act all the endpoints we are carrying about for now do. Could you add app like endpoints directly to this? Of course, it's just a fast API app, so you could also just develop whatever else you want on it. Entry point is as mentioned, UVicorn, which is a fairly performant, you know, Async supporting web server used in the Python ecosystem. And we're just going to listen on port 8000 and now if this is when I ran it, maybe right here. Ah, okay. So we're going to bring up this service and now if we copy where it lives not found. Well that's just because we didn't even define a root route. Right, so there is no root route. We, you could define one or you could for convenience set it to redirect to somewhere else. That's fine. We don't have a root route. We go slash docs though now we see our swagger docs. So all those questions hopefully start becoming a little clearer. And as I said, open API, what that means is this entire page can be represented in JSON and open API is a standard for representing APIs. And this could potentially integrate with a bunch of other tools if you cared about that. And you can see it's like specifying schemas with neos and stuff like that. But for our intents and purposes, the main thing we want to do is just see, I mean look at all these routes. We didn't define any of them. They were given to us by add routes. And you can refer in more for the most part, you know, these more configurated ones you shouldn't be messing with. The create feedback does let you potentially Automate the recording of. So you see how this sends individual runs to Lang Smith because Lang serve is from the Lang or integrates with the Lang Smith Lang chain stuff. And so this lets you record those zeros or ones, but do it through your own API. So you could expose like a thumbs up or a thumbs down in your ui. And if the user clicks thumbs up, you send a one back to Laying Smith. And if they click thumbs down, you send a zero. Right. So that's, that's nice, but really Invoke is sort of the main workhorse here. And you can click, you can see some information like, okay, it expects a string and this is its schema and this is what a response should look like. Let's try it. So this is Eugene's thoughts. What does Eugene think about? Or Eugene, I think cats and sticks or something. Cats and dogs. Okay, about cats. And then we say execute and this is sending it to our microservice and we got this response. 200 Hooray. Eugene things cats like fish. Okay. The rag also did. The rag worked. So we know all that must have happened behind the scenes. Of course we could go to Langsmith and see the traces. But you've seen that enough. You've got a run id. We're not. We haven't left any feedback on this, but all that's exposed here, so that's pretty convenient. The other thing that we can of course do here, Ash, do you remember the name of the other endpoint that's.
Speaker A - Ash Tilawat

Exposed Batch or invoke or.
Speaker B - Aaron Gallant

Oh, no, no. Playground. Isn't there a playground?
Speaker A - Ash Tilawat

Oh, you should be.
Speaker B - Aaron Gallant

Yes. So the playground. This. Honestly, as a developer, I'd probably mostly want to be looking at this and yeah, so I only showed Invoke and so we get through everything. I'm not going to step through the others. You can read the documentation, but they should all work the same. You can batch multiple requests and you can stream. We'll see some streaming in the notebook, so we'll get to that there. But you can also instead of Slash docs, go to Slash Playground. And that exposes a somewhat friendlier interface. I mean, I think they need. There's some improvements they could do to iterate this. But if we put, you know, what does we do about dogs? And we interact with it and we see, okay, we see the trace basically we see, hey, get Eugene's thoughts for dogs. So the agent realized it should use its rag and queried about dogs and then it got the response from that and at some point it's Going to decide. All right. It saw that Dogs like Sticks got the document back and then it made its output. Eugene seems to think Dogs like Sticks and we can even see even more painful detail. And Jason payloads shooting back and forth if we want. You'll see runnable, runnable sequence, runnable, lambda runnable things. So these are all runnables. That's just the language for these LangChain LLM type objects. All right. And you can even potentially share this. So this, this interface of it could be more useful if you're working with an internal team and not everybody's technical. And so you don't. You know, some people might be afraid of that Swagger Docs site and this might even be convenient for your own experimentation. I believe this basically mostly uses just the dot invoke at the point though, as open source, if you want to check what it's doing. Exactly. Okay, so some questions that have come in. What does Lang Serve do that fastapi doesn't do? Well, Lang Serve is what's adding all of this. If you just use fastapi, none of these routes would be here. You'd have to code up these routes yourself. Laying Serve is accelerating you in terms of making an LLM microservice on fastapi. That's what Laying Serve is doing. Fastapi is still the underlying framework that is providing Unicorn, that's providing Swagger itself. But Laying. Sorry, Laying Serve provides these endpoints. And also this playground is pure Laying Serve. This is actually Swagger is part of fastapi. But the routes themselves we did not have to define. Let me scroll up to see if I've missed any other. All right. How would I use this if it's disconnected from my back end? Yeah, so it's microservice API message passing sort of stuff. So it'd be network requests. You could have your front end make requests to this back end to do things with an LLM and receive the response. And then, you know, make it pretty or acceptable to your user. See where my understanding, I have a python backend, fast API, etc. Magic swagger API JS backend. That means. I know. Yeah. So I think the happy path here wouldn't be to have. I mean, it depends what you're doing. Right. You could have both a next JS and a Laying Serve if you wanted. In that case, you'd need to figure out, you know, from a networking perspective, how potentially your two different servers could make requests to one another, because that's possible. Or you could have each of them serve the same front end. And the front end makes requests to the JS backend for certain things and makes requests to the laying serve backend for LLM things. Right. So all the requests come from the front end. That would probably be a little easier. But I mean, the happy path here is probably to not necessarily go multiple backends unless you have a compelling reason to, and to see this as an alternative to trying to encapsulate your LLM in a next JS backend. But if you've already done that and you're really comfortable with that, you don't need to do this. We're just offering this because it's a good, convenient and fairly easy to use alternative, especially for people who've been hitting issues. Because I know not everybody necessarily is happy with where they're NextJS backend is. That's that. That's my take on it. At least it will vary. And especially for like a larger app, you could have reasons to have multiple backend microservices that you orchestrate together somehow. But I am leaving that open ended intentionally. Okay, so let's. Oh, let's start the notebook. That's the last thing I show here for this, I think so. Another cool thing about laying serve, I also need to get to render. Yeah, I definitely need to keep moving. I might defer questions for a bit. We'll make sure to follow up in Slacker afterwards. So that back end is still up, right? So we can just use requests and make, you know, web requests. Web HTTP request and ask. It should still be up. There's something. Oh, it's not local host, it's 0000. All right, very quick check here. Did I stop it? All right, well, let's see if the runnable part works. We'll get to that. So this would make a web request and May I. We can debug it if we have time. But what we have here is this allows us, instead of if you're using Python as a client as well as the server, instead of just making web requests, which you could do, and you could even do this with curl or whatever, you can use remote runnable. And remote runnable gives you a runnable that has the exact same interface as a regular runnable. And so what that means is you'll see our friends invoke. Right, which we saw when we were making chains and agents and stuff and a invoke for asynchronous and batch and stream. We see all of those. So this is as if we instantiated the runnable, the code inside the server here. It's as if we instantiated all of this and got this agent executor, like the remote runnable, basically is the agent executor that we've connected to, but we've connected to it through the API. And this is a very convenient way, especially if you're working with a team. But perhaps you're organizing yourself to have all of the abstraction of the LLM, you know, in something that you just import and connect to in two lines and then you can interact with it, you can invoke it asynchronously and then await the results. And you get the response of how can I assist you? And you can invoke it, ask what do you think of cats? And you get the response, which is showing that it's using the rag and all of that. And you can even stream. So streaming will show things token by token. I'm given that it's not clear that it's connecting correctly. I don't know that this will run live, but it gives you the full response. So we're prompting it to tell me a story and it gives the response, but the response gets typed out, basically, token by token, that's being returned. So I encourage you to play with this. But in the time remaining, I want to make sure we look at Render so people have a full end to end of how to actually possibly deploy this stuff if they want. And if you're confused about like, oh, should I be doing this? Or something else. Ash is feeling those questions in the thread, but the high level that I'd say is there's a lot of tools in this space. So if we present multiple options, it's because you have the ability to choose. Now, if you're really happy with your next js, that's great. But this can be a great way to get something started as well, if you want to check it out, and especially if you've been hitting issues. All right, so render. What Render does is render lets us deploy things. So you can see I've deployed some stuff already, but we'll make a new one. And the deployment. Let's close some of these tabs, kind of. There we go. And this example now, they do sleep the spin down the free instances, and they seem to do so pretty aggressively because I clicked it not that long ago. But once it's up, you'll see the swagger page, the documentation, you can interact with the API. So it's what you'd expect. Now, if we want to make A new. We make new and then web service and then we want to. I'm going to give it a public. I believe this is public, right? Yep. I'm going to give it this repository. But you can also get private repositories and we will. Yeah. So Ash is hopping because he's got another thing to handle, but he will follow up on questions async in the thread, especially if they're questions that have more to do with like what tools you should use for what or what these specific expectations are on the project and similar. So now if we look at the base of this repo that we just connected to, we'll see there's a. There's a deployment readme here that you can refer to as well that has this information. But you can see that there is a Docker file. And so we're going to say that this is a Docker project. You can specify other sorts of projects. You could actually do this more natively in Node or Python, in which case it's going to look for some specific entry point like a requirements or Poetry file or a, you know, package JSON or whatever, whatever sort of file is expecting to define an environment for that language. But Docker, it's going to look for the Docker files. Take a look at what the Docker file is doing building on Python. It's installing from the Poetry file, which is going to pull all of our stuff, mostly lang chain, lang serve and then it's executing the same entry point that we saw in that agent py server that we looked at. Right. So it's the uvicorn port 8. Well, different port for some reason, but there it is. So this is all fine, basically. Main branch, Docker default entry point. Sure, put it in Oregon. I don't think we have a lot of choice at the free tier. We don't need to change the root directory. This is the main tricky thing. So for this what we need to be doing is we need to. And let me pull this off to the side here we. This is the equivalent of sort of like that.env now we could say add from a dot in from right. Or we could define each variable individually and you can do things more securely. You could have an actual secret file that is accessed during the build and there's other advanced options. Don't worry about all this right now. The. The MVP way to do this is to basically grab the contents of your.env and either define them piecewise here or paste them all here. I'm going to very Briefly stop. Well, I guess these aren't actually. This is the same API key you're using, so. But normally you want to consider those secrets and you shouldn't be like screen sharing them. All right. And you see how they get populated and such. So now if we click deploy, we'll see that it should basically pull the repo, see the docker file, build the docker, use our secrets to connect to things and serve the API. So while we're waiting for it to do that, let me catch up on other questions. I guess the main question is serverless questions and JS vs Python stuff, which at this point I do think, yeah, I don't know if he'll do another session for it or not, but I'm sure that Ash will follow up on that. That. Any other questions that aren't about the age old battle of JS versus Python, go ahead, Benji.
Speaker C

Yeah, so maybe this isn't great because it is sort of a typescript question, but in Langsmith I essentially want to bundle five traces together. I guess maybe what you can answer is, is this possible? Is there a way where I can see like, okay, the aifi function ran generate summary and generate tags and sort of see that in a pull down?
Speaker B - Aaron Gallant

Sure. I don't know about a pull down, but I think you can search and filter the traces. So if there's something in common like the function, that's where I would.
Speaker C

Yeah, yeah, I know, I know I can do that and I am doing that. But. And that's, that's kind of my fallback. But I would just love how if you do a run, you can see it goes into like OpenAI call or like input OpenAI call output. I would love to go one tree further.
Speaker B - Aaron Gallant

Yeah, it doesn't. The trees are within the level of a single interaction. So if you put all of those interactions within like a chain or an agent or something, then that, that should be in a single tree, in a single trace item. But if they're actually separate, like at that level, then they're going to show up as separate traces. And so far as I know there's not like a UI option for manipulating that differently. But that's an excellent feature request for the Langsmith people if you want to pass it to them.
Speaker C

Yeah, yeah, I think that makes sense. I guess. Basically I have five buttons on my ui. Each button does a function, but then I have a big button that does have a route. It does run a function, but the function is basically run these five functions. So I'm wondering if I can get that big button that runs those five functions to be the primary trace.
Speaker B - Aaron Gallant

Well, so maybe if you, if you wrap all of this in that. So it sounds like there's one entry point function that's executing all the other functions. Is that what you were describing?
Speaker C

That's correct.
Speaker B - Aaron Gallant

Try putting, have you tried putting traceable on that entry point function?
Speaker C

Yes. Yeah, that's kind of where I'm getting stuck and where I think it might be a Typescript thing which is not quite as well documented and probably not.
Speaker B - Aaron Gallant

Something you're prepared to help with answer offhand but.
Speaker C

But I want to know if it's possible, you know.
Speaker B - Aaron Gallant

Yeah, I mean in principle I would think that would work, but there could easily be gotchas. But yeah, in principle if you set the higher level function as a trace, then that should make it, it align like a trace is a line in the list of traces and that should make it show up and then it should hire. At least there should be a hierarchy of whatever sub calls that's making that also get instrumented. But if that's not happening, I mean you could experiment a little bit with it and also file an issue against Lane Smith. I mean a lot of this stuff is open source, but. And yeah, just as a brief comment to all the like, oh, Python versus we actually get dragged back to the Python versus JavaScript TypeScript stuff. Stuff. I mean I, I'm at least of the opinion there's. It doesn't make any sense to be at all partisan about these things. These, they're all tools and they, you know, tools just depend on your situation and who you're working with and the comforts and all that. But I will say that from a practical perspective, most data science, machine learning, statistics, LLM, et cetera, ecosystem is initially developed, a lot of it is initially developed in Python and so that sort of has first class support most of the time. This isn't to say that you shouldn't use TypeScript or JavaScript, but as you observed, like maybe this is something that would have, that would work in Python and doesn't yet work in, in TypeScript and it probably will. It might just be delayed by a month or two.
Speaker C

So yeah, I think I'm, I'm gonna, I'm gonna keep playing with it for a couple hours. I mostly just wanted to know if you thought it was possible. And yeah, for me TypeScript is most practical because my whole project is in TypeScript. But yeah, I hear what you're saying and happy to use Python where it.
Speaker B - Aaron Gallant

Makes sense and Rosten, I see your hand. Let's look at the output here first and then we'll get to more questions for a bit. So what happened here? Well, it finished above here is the logs of, you know, docker build or whatever and then docker built and it's deploying and it's running its Service on Port 8080. I think 8080 might have been picked because that might. Might be what render expects FAST API to run on, to then expose, to forward the port to actually be visible on the web. So you see other ports, other things happening here. Seems like it might even be restarting the service for some reason. Let's see if it's actually go. Yeah, it's up. So here we go. So the root route, if you. If we look at the actual source of this. Let's do that real quick. App Server PI, server py we are defining one. So this is what it looks like to add a route to fast API. Now, this is a very simple route. So all we're doing is we're listening on the root and we're returning a redirect to slash docs. So if we go to the root, instead of getting a, you know, not found, we just get redirected to the swagger docs. Lovely. We're enabling the feedback endpoint to potentially, you know, annotate data. And we're setting, we're setting the entry. So that's all the server is. The server is. I mean, let's just count that again. The entire server, even with our root route. Say that 10 times. Fast root route redirect is like 20 lines. Right. And then what is it doing? Well, it's adding routes with the app, the FAST API, the chain which we're importing from the other Python file. But that's our runnable and this is a decent pattern. If you are developing this yourself, you don't necessarily want to stick everything in one file. You can have a server PY with your fast API, a lang serve code and then a chain py or agent py or, you know, LLM py and the LLM py will have your LLM connection and your prompt. Or maybe you. It's actually better to keep your prompt out of your code if you're. If you get serious about iterating it. So you're loading this from a text file and then, you know, actually making. This is the runnable, which is a chain using LangChain expression language, combining the prompt to the LLM to the output input. And so this chain, which Will apparently this LLM answers a question and then ends the question with a follow up. So it's. It answers questions with questions. Sort of a game really. And this chain is the runnable that we care about that is used here. So yeah, we see all of this in fast API in the swagger and we can interact with it if we wanted. We can try it out and we. Our question is what is the capital of Canada? And we can see that it is sending this request to our server and we get the capital of Canada is Ottawa is located in Ontario. Did you know that Ottawa is also home to many national museums? Right. So it's following up with a question and we could also of course play with this at the playground. Now one thing I'd note and we'll do it here again. What is the capital of Canada? It pretty much. Oh, slightly different response. Right. Probabilistic. But have you ever visited Ottawa? No, I have not visited Ottawa. Why do you ask? Right. If we, if we try to continue the conversation as you'll see. Oh, I think it's actually. It shouldn't have the full history here. I don't think so. I think it might be bluffing. What was the first question? Yeah, so it doesn't have history. It actually bluffed that second time. It was just so polite that it assumed that we, we were talking about something real. But like the first question you asked me was about what the first question you asked was. Well that's because kind of a goldfish situation here. It doesn't know the previous interaction. So the other example in the demo class. Demo should keep history. If you want to see how to add chat history and manage that, obviously then you might need to start thinking about how many tokens you're keeping. So if you're preparing something for actual repeated interactive use, it gets a lot more complicated. If your LLM is just something that you send something to and get a response and it's. It's more atomic or more restful, really stateless, then your, your world is a lot simpler and actually for a lot of non human facing things that is fine, that's even preferable. But just calling out that like the built in history that you're used to from just chatting with LLMs at ChatGPT or Cursor or something is not something you get by default. When you just connect to an LLM and expose it as an API you need to make a point of maintaining message history and doing what you want with that. All right, so a little. I'll check for Questions. And if you've got more questions, go ahead and ask does this require a message, a microservice, or can it be done serverless serverlessly? So I, I believe you possibly could. I think I have read about like FAST API on Lambda and if you can do fast API on, on AWS Lambda, you could probably do this because this is basically fast API. I wouldn't necessarily start there unless you're comfortable with that or opinionated about that. But. And again, microservices, you have to think a little. Sorry, serverless, which I don't like the name serverless. There's still a server. Really what we're talking about are extremely ephemeral function calls, right? That, that you worry even less about the infrastructure than we are here. I mean, to me this is already like, we're barely. This doesn't feel like I'm worrying about the server all that much because I'm certainly not managing it myself. But yeah, yeah, we can talk a little bit about cloud nomenclature here. So back in the day you had to buy a rack off the. So basically there's, there's levels of how much, how much you're managing what you're, you know, the metal underneath you. And serverless is kind of the least. It's still there, all the metal is still there and it's still doing its metal things. It's just you are interacting with it in a way that's very ephemeral and very much just provided by whatever framework you're kind of committed to that it has some benefits. It has a lot of benefits, actually. I mean, perhaps you need less DevOps. And the other main benefit is you, you know, if, if what you're running doesn't need to be up all the time. Right. Serverless, as it's called, because it really is just ephemeral abstracted function calls that spawn when you ask for them, they're more expensive per second per. Per unit of compute than actually running a server yourself or even a hosted server. But if you aren't running it 24 7, if it just spins up every so often, it can actually potentially be more efficient because most of the time it's just not even awake and you don't have to pay for it when it's not awake. Whereas if you, or at least you pay less, there might be some maintenance fee. I'm not going to try Cloud cost structures are their own advanced topic these days, but it can potentially be simpler infrastructure and even a cost savings or at least. But it can also be the opposite of A cost savings. That's the other thing to be aware of. It can, it can very easily cost more. So long story short I yeah I think you can cram fast API and thus you can, thus you can stick laying serve in a. In a lambda type thing if you wanted because of how ephemeral it is. All this stuff I was just saying about like state and history would be even like you could with. With this the way we're doing it here, you could keep the message history in memory right. And because at least that memory will persist through a user session. But if you have microservices, sorry if you have serverless you might not be able to do that. Like you might not be able to persist things as a variable or as an in memory object across user interactions because your, your function is the separate function invocation is a separate memory scope and it's not going to know. And so if you care about state across serverless interactions it's going to have to interact with. With you know, database or something that persisted. Ross, I did see your hand up. I'm sorry if you gave up because I'm yammering on so long. Feel free to ask your question.
Speaker D

No, no worries. I think you just answered it. So I have everything running in an edge based function. However I just looked it up and it's 30 seconds long that it'll like stay alive. And so if you're running like a deno server on there, great, you'll have that memory like I have it set up with memory. However if it shuts down after 30 seconds and you're not done with that conversation then it'll lose it. So the idea here is to transition everything over but the laying serve onto an EC2 instance have that server running at all times so that it never goes down as stores ephemeral history. And then you can continually ask it questions obviously limit like the max tokens of history for price.
Speaker B - Aaron Gallant

That would certainly be a approach. I mean and you could also potentially even deploy elsewhere. But any regular server, you know, stuff just like your computer, your laptop or whatever stuff is going to stay in memory as long as that session is open. As long as the box is up, obviously you still can't really consider that durable. If the box resets or whatever for whatever reason, then you still lose all of that. But it will certainly persist more than 30 seconds most of the time. Cool. Thank you. Sure. Any other questions out there? All right, well thank you all. Good luck continuing your projects and yeah, exactly. I see some, some chats in the slack this is not like you can still use with edge functions. You just have to think about memory, right? Like, you might want to persist it to a database, which might actually be better for what you're doing. Anyway, there are. There's no universal answer for the right way to solve things. Depends on the problem you're solving. So, you know, that's. That said, the general things I'd encourage right now are, you know, keep it simple, slash comfortable, do things that you are familiar enough with, and stretch to learn bit by bit. Don't try to, like, do something completely alien to you in every way, because then you'll just be overwhelmed. Overwhelmed. So increase your toolkit, but do it incrementally. All right? That's all I've got. Thank you, everybody, and, yeah, good luck continuing your projects.
