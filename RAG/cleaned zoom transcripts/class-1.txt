%START_METADATA%
Class_Name="Class 1 - AI First Development"
Class_Date="2025-01-06"
%END_METADATA%

Speaker A - Austen Allred

If necessary. And there's going to be a lot of logistical stuff today, more than there normally will be. But we're all getting onboarded, we're all getting everything all set up. We're getting. There's access to a whole bunch of stuff that we'll talk about later. But just to set the agenda for what we're going to be talking about today, we're going to start with kind of introduction, hello, a little bit of overview stuff. And then immediately after that, we're going to kick it over into our first lesson. We're going to start talking about AI, first development, and what our first project is and what we're going to be working on pretty much immediately, and then we'll start building from there later on today, the founder of Trilogy, his name is Joe, and he's one of the brainchildren and the deep pockets behind all of this, is going to pop in and say hello and talk about why we're all here and why we're excited and why all of this exists in the first place. But I think this is just really, really cool and I am excited to be here. So with that, I am going to share my screen and share a deck, and we're going to walk through some stuff together. Know that generally speaking, we avoid. We're more excited about building and logistics than going through details and getting access to software. But there's going to be a lot of that today. There's a lot of software we're going to be using. There's a lot of logistics that we have to cover, and there's a lot of the. As we get started, it's going to feel like a lot, but very, very quickly, you'll get into the swing of things and we'll spend most of our time learning and researching and building. So with that, going to share my screen and present, and I will monitor the Zoom Slack, or, sorry, the Zoom Chat to the extent that I can. So feel free to message in there, but know that those. Those messages will disappear pretty much after this meeting is gone. If there's something that's more permanent, feel free to ask it in the Slack Slack chat, but for now, let's just use a Zoom chat. We'll get into all the fanciness of best ways to do these meetings later. All right, so Gauntlet AI, what is going on? Why are we all here? What is this? So basically this is the result of, and I apologize if some of you have heard this before in info sessions. Some people haven't. So we're going to go over it again. We started working with a handful of companies over a year ago. We, you know, we ran code schools and different, you know, learn to code type of programs. And all of a sudden, company after company was coming to us saying, hey, this is really cool, but we've got to figure out what's going on in AI. We need AI. AI. And we went out to those companies and said, what is it about AI that you what about it? And they said, we don't know. Just we hear buzz that there's something going on with AI. We don't know what it is, so can you figure that out? So we sent out a research team and they spent quite a bit of time painstakingly going company to company, trying to figure out what was working within AI. And what they found was pretty surprising, at least to me. They found that 95% of companies had no idea what they could or should be doing with AI. And all right, let's refrain from drawing if we could. And a handful of people were really, really advanced. And it was kind of one of those situations where the people who really knew what was going on were like individual engineers sitting in the basement playing with LLMs. And it had not, it wasn't coming top down from the OpenAI team of this is the best way to build stuff. This is exactly what you should be doing. It was a lot of random engineers figuring out a lot of stuff along the way. And you'll notice that that's kind of a theme of what's happening in AI. We are in the homebrew computer club days, so people are doing magic with AI, but not many people know what is possible or what is out there. And frankly, I think most people haven't seen what AI is truly capable of. So we, we don't profess to be the people who know everything. And there's still going to be stuff that we don't understand yet. And that's unique for me in an educational position where we're going to be learning stuff right alongside of you guys and stuff is going to be changing faster than any curriculum can possibly be built. But that's what also makes this really exciting. So the way we look at AI is it is the biggest force multiplier in human history. If you look at kind of an analogy of what the steam engine did for factories or kind of what the Internet did for generalized human knowledge, we think AI does that level of change for individuals who are trying to do things, for trying to build things. So we had a few of the companies that we were working with. They are some of the biggest hirers of software engineers on the planet. They have tons and tons of engineers that they're working with and they measure things really religiously. And they would see crazy, crazy shifts in productivity. We're talking across thousands of engineers. Hey, the Entire team got 50% more productive last quarter and then another 50% more productive this quarter. And you know, you're really good at math. That's more than 2x in like six months. That's completely unheard of. And so they started looking into it more and more and realizing that it was really, really difficult for them to get their existing staff to adopt new skills and adapt, adopt new practices. And that if they could find people who are really smart and really hardworking, who were really willing to go all in and become AI first, the amount of value that you can create by having that is so extreme that it, it kind of became a conversation of how can we get as many of those smart, talented people using AI as we possibly can? And that became Gauntlet. So those companies are funding everything. Those are the companies that want to hire you all. We're not going to have enough engineers on the other side for them to hire. They would hire what we're going to have at the end and some again. But yeah, at the end of the day, AI is super, super powerful. And we'll go over some other stuff in a little bit. But so the way that Gauntlet AI works, our overall goal is to create the most sought after AI builders on the planet. We'll talk about what we mean when we say AI builders, but it is an extremely intensive 12 weeks. When we say extremely intensive, we really, really mean it. We're not exaggerating or lying when we say 80 to 100 hours a week. I had a couple people, I had one person this morning message me and like, hey, I think this is going to be difficult on top of my full time job, don't you? I said yes, I think, I hope we've been clear. This is impossible to do. In addition to a full time job. It does not work. We are on a mission to create a credential that is more sought after than Stanford or mit. We want, at the end of this, our goal is for every company to look at what you guys are doing and say, man, anybody who came through Gauntlet, that's hands down the first people we want to hire in any situation, how does it work? So it's going to be four weeks remote and then eight weeks, all expenses paid. In Austin Texas. We'll talk about the logistics of all of that along the way. Basically, February 3rd is moving day. Assuming you're still with us at that point. We're flying everybody out, we're putting everybody up in hotels. We've got another eight weeks of fun times in Austin. Participation is 100% free. I'm sure you've all, you're all aware that none of you have paid anything. None of you ever will pay anything. And if you complete the gauntlet, you receive an offer from, there's a company matching process along the way. You get $200,000 a year job as an AI engineer in Austin, Texas. That's 200,000. Base benefits are on top of that. It's a full time W2 very intensive role. Probably can't continue on at 100 hours. Some people can so not necessarily 100 hours a week, but they're not going to be 9 to 5 clock in, clock out jobs either. Very intensive, very startup like. And there's a lot to build. So with that said, there are things that are easy for us to solve for and there are things that are difficult for us to solve for or that require a little bit more effort and energy. So in the admissions process, as you all saw, we started with the ccat, which is a general cognitive assessment. Our bar for that was very, very high. If you are here, you are smart almost by definition. We know what you're capable of. We know what your intellectual horsepower is. I find that it's rare people admit that in this day and age, but that does matter. The next parts are all going to be about how hard you can work, how fast you can learn. And this is, as I said, this is all in. This is not a side project. This is not something you're going to be able to get by half heartedly. It will be very obvious to everybody instantly if that's the way you're trying to do things. If this isn't for you, I should say this up front. There is no ill will or mal intent. If that's the case, that's totally fine. Let me, you know the, if you want to, to ring the bell and say, hey, I'm out 100 acceptable, no judgment. That doesn't mean you're not a good person, but that door is always open to you. We want you to be here for the right reasons. We want you to be here and be committed. Everybody. You, if you're not committed, you owe it to, I think everybody else who is committed to open that up to yourself and let us know, and we'll make adjustments and everybody will go along their way. I've gotten a bunch of emails over the last 48 hours. I don't know if it's something that happened in the unofficial discord. People saying you're positive that you're not going to charge us a bunch of money if we drop out early or if we don't take one of the jobs that are available. No, there's, there is never any scenario under which you have to pay Gauntlet AI a penny. So I hope that we're 100% clear there. If this is a scam, it's the most inefficient, ineffective scam there has ever been. So there. Yeah, you'll never pay any of us a penny. All right, so with that, let's get into building with AI and what that means. So when we talk about AI, we're specific, we're not talking about. There are companies like OpenAI and XAI and Mistral and Anthropic that are spending literally billions and billions of dollars building these really powerful models. They're slurping up all the data that exists, they're generating new data, they're turning it into these super powerful models. Our goal is to be able to use all of that in the most effective way possible. So we're not going to compete with OpenAI, right. We're not building the next ChatGPT, we're not doing frontier research of AI. Our goal is to ride the wave and we mean that very intentionally and we'll talk about that quite a lot. If you're on the cutting edge of AI, you can do stuff that would have blown everybody's minds. Honestly, at this point, probably six months ago, but definitely a year ago, absolutely five years ago, you couldn't even fathom how quickly you can do stuff, how much you can do. Our focus is making use of AI. Sebastian asked a really good question in the chat. Are we going to train our own models or use pre trained models and leverage from there? Yeah, that's what I'm trying to say is we're going to use those models that other people have trained for us. Creating a model that is better than them is a multi billion dollar decade long investment. So we're using all that stuff that people are building if we were in the 1980s. We're not trying to build the next best computer, we're trying to figure out how to use computers really effectively. So that's what we mean when we talk about riding the wave. And then let's talk A little bit about AI first. This is a graphic that AI created for me. You'll notice that will be a theme. We do practice what we preach. At least we try to. So what I wanted this graph to display is that when you first get started, if you are used to building stuff. Yes, John, Applied AI is a really. Yeah, really good way to think about it. We'll talk about. Anthony, that's a good question. What about fine tuning models? We have strong opinions about fine tuning models. Generally speaking, even fine tuning right now is a lot less effective than using different techniques that are much cheaper and more effective. We can teach you how to fine tune. It's not crazily difficult. But generally speaking, using better QC first principles and better rag. And we're not even talking about that stuff even today, do a better job than fine tuning for a lot. You need just a crazy amount of data to do it to make a model better with fine tuning. But we'll talk about all that as we get into the weeds. I'll share the prompt with you. I think it was. It was the prompt I used to create this. I think it was GPT4.0. And I basically said, show me a curve where. And I made a couple of changes along the way where there's one curve that increases over time and then there's a flat line that stays exactly in the same point and there's a crossover point. And I told it what the x and Y axes were. But basically all that we're trying to say here is building AI first. When we talk about building AI first, you're not writing code. We don't want you pretty much ever to initially write code. The AI is a better engineer than anybody in this room when given the opportunity to do so. Your job is to direct and to conduct and to prompt and to give boundaries to and to teach that AI what it needs to do in the beginning that feels slower and it sometimes is slower, especially if you're a really, really good engineer. It takes a minute to get to the point where you're better with AI than you are manually. So if you're a really good engineer, and I see in the chat, um, yes, I did. We are getting. Everybody's getting Cursor Pro today. Um, we'll be sending you invites to that literally right after this call. That's. You're. You're jumping a couple slides ahead. Um, but the. It may feel slower. If you're a good engineer at first, I promise you give it a week, less than a week and you'll Be on the other side. So just get used to letting AI write the code for you. Let AI do the first pass. At first, it's gonna be like, I could just do this more easily myself. But pretty quickly that will not be the case. All right, I'm gonna ignore the chat a little bit. We gotta, we gotta keep moving on. Yes. We're gonna give, we're gonna give you guys access to so many tools that access to tooling will not be the thing that you're, you're frustrated by that. We've got tools that you guys, we haven't even. You've never seen because some of them we built internally anyway. So when we step, take a step back and look at. Okay, in the new world of AI, what does it mean? What would it take to build a product that's worth, say, $100 million? If you were to ask that question five years ago, it's like, okay, well, I, you know, a team of 100 people, I need a bunch of capital. I need, like. We don't think any of that is pretty much true anymore. We think it pretty much requires. And you know, Trilogy, who is behind one of the companies behind this, and they also operated Trilogy University, which you'll notice we're not quite a direct clone of, but we're basically rebuilding Trilogy University, they own and operate more than 150 different software companies. They measure everything more rigorously than any company you've ever seen measure stuff. They're really, really good at building software products. And some of this comes from their thinking. But so what we think is required to build a product worth $100 million is AI First Development. That takes the, the number of people requirement down from maybe it used to be 100. Now it's probably one, maybe two, maybe three. But it's definitely much, much smaller. A lot, lots smaller investment, a lot, lot more AI. And that, that's what we talk about when we say AI first development. There are, I mean, we've talked to individual teams that literally, it used to be 15 people operating this team, now it's one person. And AI, that's 100% happening in production in major companies today. And the second part is both requirement to use AI first development. Because AI is changing so rapidly and you have to learn to learn to use AI. We're not going to focus on that as much today, but that'll be a major component of Gauntlet. Not just, you know, building with AI to make stuff really quickly, but in order to do that, you have to be able to learn really rapidly. You have to be a voracious, self driven learner. And we're also going to be using a tool that we call brainlifts. So think of a brainlift is basically a second brain, but it's a second brain that we build in such a way that it can be fed to models. And we'll talk about reasons that that's true a lot in the future. But one of the main reasons is that for AI, if you think about what AI generates, it's basically a consensus driven view of the world. It swallowed all of the data that it's found and it kind of unintentionally develops this mental model of the way the world operates. And sometimes that mental model is wrong or it overemphasizes or under emphasizes different factors that are really, really, really important to building good products and to making AI work well. So one of the things that we'll talk about a lot is how we build a second brain. We call it a brain lift and how we make it so that what's in that brain lift overrides the general thinking of the model. That's the way you force the model to do the right thing. And it's, you do have to force it, you have to push it, you have to fight with the model because the model is so trained on the consensus view that when there's something that's non consensus but true, we really have to ram it down the model's throat. We'll talk a lot about that and then that also helps us. You have to learn really quickly and adopt really quickly even. We started building this curriculum several months ago and there are entire portions of the curriculum that we would have built that became completely irrelevant. There are other parts of the curriculum that we had to completely rewrite. And that's in a period of several months. Right. Imagine what it looks like over five years. I can literally can't even imagine what things will look like five years from now or a year from now. So we're going to have to learn to learn and learn to stay on the cutting edge of what AI is rolling out. That's a little bit intimidating, but I think it's super, super exciting. It's a good question. We're going to have a full session on this later this week. For the tooling that we use for brainless, we use a tool called workflowy and we'll give you all access to that. But basically there are any number of tools that operate pretty similarly. Workflow is basically based on a graph database and it Lets you kind of outline things and fill it. Well, we'll talk about it all, but yeah, we'll give you access to the tool. And the tool we use is called workflowy. But the tool isn't the important thing. It's more the format and what you're putting in the brainlift and how you're using it and all that other stuff. So the other thing that we'll talk a lot about is what we call qc first AI or quality control first AI. So the great and terrible thing about AI is if it doesn't know the answer to something, it's going to give you its best guess. And that best guess, because it's based on the consensus driven view of all the data on the Internet, it's going to sound pretty plausible. So as an example, if you tell AI to build X writing code, it will build X. It may be completely the wrong way to build X. It may not be the X that you were thinking about. So a lot of what we do is kind of reigning in or building a framework around the AI and letting AI fill in that gap. And we call that QC versus AI. So a lot of the time you're defining guidelines at which. So as an example, you could say, hey, I want X very explicitly. Now go try five different times to do X and give me, based on this Y metric, when you're giving me an output, give me the output that best solves for. Yes, Workflow is very similar to Obsidian Roam. It's what they use at Trilogy, which is the reason we use that. Before that, I use Rome Research, Obsidian, they're all pretty similar. I mean, tiny investor in Rome research should disclaim that we're not going to use that for, for Gauntlet. All right, now let's get into some logistics because there is a whole lot of software, there's a whole lot of stuff that's going to feel a little overwhelming at first because there's so many different tools that we're throwing at you. Obsidian and Rome are. Yeah, they're tools to. They're second brain tools. You can. Yeah. All right, so the most important ones, email. You should all have access to email. But I can see on my dashboard how many of you have logged into that email so far and it is not yet 100% of you. So if you have not used your GauntletAI.com email, please log into that. We sent those invitations to your personal email address. But the easiest way for us to grant access to tools is going to be sending stuff to your gauntletai.com email address or sometimes we can just white label, hey, let anybody with gauntletai.com email address create this tool automatically. And so email is probably the most important tool. Second is Slack. If you're not in the Slack, get in the Slack. That's an invite link that you can join. Most of our day to day communication is going to be in Slack. And then Calendar, we have a Google Calendar. I've noticed that some of you are not as familiar with using Google. So Google Calendar, if you use the Google Calendar link in 99% of ways, it will whatever. You can either add it to the calendar app that you're currently using or the calendars are pretty good at being interoperable with each other. So if there's a personal calendar you use, I can show you my calendar setup. I've got seven different calendars and I can show this is my home calendar, my personal calendar. Is this the Gauntlet calendar? We can get all of that. If you're not getting access to any of this, please, I guess the first three, please let me know. The other stuff is all stuff you're going to be getting access to. Some of them during the first lesson, some of them as time goes along. But every additional access thing will be sent to your Gauntlet AI email address. So the first tool we're going to get access to is worksmart. So Worksmart is a tool that it's mostly for you. It's a little bit for us. But you turn it on and it's going to watch what you're doing, it's going to see how productive you're being. It's going to look at the way you're using different tools and recommend changes. It's really, really cool. The part that's for us, the part that's for us is it will monitor your usage and send that data pretty much to me specifically. And we're going to use AI to look at that and identify if there are any. If there's particular models that are working better than others, if there are workflows that are operating better than others. There's a lot of cool stuff that we can do. You can delete any. So it's something that you turn on. It's not monitoring anything unless you're turning it on. We do ask you to turn it on. It just makes everything at Gauntlet run. Turn it on when you're doing Gauntlet stuff. If you're not doing Gauntlet stuff, turn it off and go about your day. Do whatever you want. We are requiring it just because it's impossible to do a lot of the things that we need to do without worksmart. But again, turn it off if you don't want to. And it does allow you to delete any of the logs that you don't want at any time. So that's totally an option. You're also going to get access to what we call the platform since we're using personal computers. What's the soft Work Smart from recording sending personal information? Yeah, that's a really good question. So there's couple ways that you can manage that. One is you can either turn it off when you're doing personal stuff, or some people want to create a separate user within a computer that's for gauntlet stuff only and use that user only. Install worksmart on that part on that user and don't install it on your personal user. So however you want to do it is totally up to you. It's just the only way that we can make something like this work. So, yeah, personally I have it on. I just turn it on and off. I don't want to be logging in and out of different users. I'm a little bit lazy. But if you're more conscious about that, then put it on a different user on your computer. Work Smart available on Linux. I believe it's on Linux. Yeah. Let me. I'll triple check. But. But, yeah, all right. And then platform. When we say platform, it's the platform that we built for Gauntlet. If there's a question about something, you'll get access to that later today. But it's. That's the system that we built. So you'll use your Gauntlet AI email to log into it. That's how you'll submit assignments. That's how you'll. That's kind of our learning management system that we built. Then development tools we talked about. What does Gauntlet do with the Work Smart data? We're basically just trying to use it to learn how to make AI, like learn what's working and what's not within AI. WorkSmart isn't recording the screen. We do have a few people who volunteered to stream the entirety of their experience and get a little more data from that. But it takes screenshots every so often when it's turned on and it logs your keystrokes. So know that, you know, if you're doing personal stuff using passwords, please turn Work Smart Off. Obviously, we're not like, I don't want your personal information, I don't want your personal data. We're Just trying to. Trying to learn from it. This is a good question. It's not open source, but I can. We can show you the logs, turning it into a fully automated engineer model. I mean, candidly, you guys, Crossover, which is the company that built Worksmart, originally built it as a tool to allow freelancers to build stuff. They have tens of thousands of engineers using Worksmart. If they want to build a model with that stuff, they would do that. What we're really looking for is to understand how you guys are using AI and how we can help Gauntlet students better use AI. So for the students who are. Yeah, I'll send you guys all the. All the data to the Work Smart stuff. It's not. Yeah, it's a really. I mean, I like using it just because it helps me know what to use, but I want you guys to be aware of everything that it's doing and make sure that that is the Work smart data from top CCAP performers, new models to wrangle models, zero human development. If only. No, there's. We're so far away from that as you guys will very quickly see. Yeah, I don't. Okay, I'm having trouble keeping up with the chat, so I'll send out a bunch of FAQs on WorkSmart or feel free to message me. Totally understand the skepticism, and I wanted to make sure to talk about that head on, because if I were in your shoes, I would be asking the same questions. We're not going to use it to say, hey, you weren't sitting there for 10 hours today. But we. I mean, there is a. There's a mode called Gemba Walk where we can just walk around and see what everybody's working on and stuff like that. But yeah, at the end of the day, Work Smart is a way to help you become more productive, and it's a way for us to get smarter and better as Gauntlet. So that is what it is. The development tools that you'll be getting access to, everybody will get access to Cursor Pro. Yeah, I mean, look, let's. Let's. I think that's a really fair comment, Mike, and it's something we should talk about. The purpose of this program is to create the best employees for the hiring companies as possible. In order to do that, my job is to, like, you guys don't pay me. Right. The companies pay me because they want us to help create really great employees who are going to create a lot of value. And so the way that I generate the most value is I help you guys become the most effective employees that you possibly can be. That's my job. If. And I think the. The incentives are actually more aligned there than they have been in times when people are paying me tuition and I say, hey, I'm going to try to train you really well. I am going to be. If my relationship with you guys is. If you're falling behind, I'm going to be honest with you. I'm going to tell you that you need to step it up. I'm going to tell you that you're falling behind. Not because I. I mean, because I want the best for all of you. So I only succeed if you guys are all successful. If. Yeah, if. If stuff isn't working, then it. It harms me. Um, so we're.
Speaker B - Aaron Gallant

We're on.
Speaker A - Austen Allred

We're definitely on the same team, and the goal is to create as much value as we possibly can for the hiring companies when you're ready to work for them along the way, obviously, you're going to be learning a ton. You're going to. I think it'll be a formative experience, but, yeah, is Worksmart used to measure the amount of hours? So, yeah, Ryan, that's like, Gauntlet isn't really free. We're putting in 80 to 100 hours a week. I mean, you're right in the sense that, like, learning takes time and we're asking a lot, but you're not generating anything like the IP that you're creating. You're not, like, building products that we're going to go sell. The only value that the companies get out of Gauntlet is hopefully you guys become good enough that they can hire you. That's the only value these companies are getting. So our goal is to do that. All right. Yes. That's a really good question. First, let me get into some of the more logistics real quick. You'll be getting Cursor Pro later today. That's probably the. I'd say, my assumption is that that's going to be the main tool most of you use. Cursor is really good. Everybody gets AWS accounts, so anything that you deploy, the companies are covering. All of that. You'll get workflowy, which we'll use. For brainless. There's a tool called e4, which we'll talk about later. It's a tool that lets you run different. It lets you run different queries and run them against a bunch of different models at the same time as a tool that we've built internally so you can see which models are better, which models are faster. You can see how similar and how different all the different models are. It's really, really cool. I wouldn't have a concern if it was a company laptop. Yeah, create a different user on your laptop and treat it as a. If that's a concern, I. Yeah, create a different user and just install Worksmart on that one user and treat that user like it's the work user. Are there PCs, laptops available for use in Austin? If you don't have a laptop, let us know. I think most people were planning on bringing a laptop, but, you know, we're not, like, gonna show up with 200 MacBooks. But if there's. If you don't have a computer that works, let us know. Yeah. So let's talk about streaming for a minute. A handful of people. I think there are five people who have volunteered to stream pretty much the entire experience of Gauntlet. The reason that we want them to do that is a. Because we think it'll be cool for them to stream and build an audience. We're not trying to keep Gauntlet private. We're trying to build in public as much as we can. And for the streaming explicitly, we are going to watch how they're using AI, what practices and techniques are working the best. We're going to use AI to analyze that, and we're going to see what learnings we glean from it and share it with everybody else who's in Gauntlet. And so thank you for the handful of people who have volunteered for that. And, yeah, I'm excited about that. So in case it's not clear, you're welcome to share publicly all of the stuff that you're working on. There are times when we actually require it, both because it's cool to share and because interaction with the real world and the other people. Building AI is the way that you learn how to build with AI Right now, I think, you know, compare it again to the Homebrew computer club days. The way that you learned how to use a computer was like hanging out with other people using computers. There wasn't, you know, there weren't schools, there weren't handbooks. So we're doing our best to build a school and build, you know, curriculum, but we know that we won't have all of it. All right, guys? Yeah. If you have questions to me about, like, we've got to. We got to get moving on. If you have questions about any of the specifics, feel free to message me. The curriculum that we're working on consists of two parts, and we're going to be kind of repeating this process over and over again. So we have the speed build and that's you're going to get your first speed build today. And that's learning how to build stuff really quickly with AI. And then there's the AI evolution, which is taking stuff that only AI is capable of and layering it into the products that we're building. And then when you're submitting a project in the platform, it's there are four things that it consists of. One is a link to the application which will be production quality deployed. One is a link to your code which will be GitHub. One is a link to the brainless that you've used. And we'll talk more about that in I think it's Wednesday. And then a video walkthrough of the product that you built that's posted on X. Why X? Because X is basically where all of the AI discussion that's meaningful and all the AI learning happens and learning in public is an important part of building with AI right now. And that's. That's it. I'm going to spend some time going through some of this other stuff and if you have any questions, feel free to reach out to me and I'm going to turn the time over to our instructors and with that we're jumping right into our first lesson.
Speaker B - Aaron Gallant

Great. Well, thank you so much, Austin. So my name is Aaron Gallant and I will be your instructor for the sort of regular initial sessions on Monday and Wednesday where we'll be going over core topics in the AI space. The skills and tools you need to do the things we're asking you to do. So today that means AI first development. And so in the interest of time, let's just jump right in. Oh well, I'll do it as I have a slides up though. All right. Soon that my desktop is up. So because I am juggling a lot of windows and sharing stuff, I will not try to monitor the Zoom chat during lecture. Just so you know, I will have an eye on the Slack chat. So, so that can be a, that can be a place to ask questions and I will see. But what is AI first development? So Austin already motivated us a bit, but the idea is to use these modern generative machine learning tools as the sort of core engine of our development process. And I'm going to note that I will tend to use language like LLMs and generative ML. I will use those terms more than terms like AI. It's not because I don't think there's any such thing. As AI. It's because LLM and Generative ML are more specific. They are what we are actually using AI. Well, that's philosophical and we can get to that at some future point if we ever have time. As I already mentioned during class there will be a Slack thread and I'll go ahead and even make, I guess we'll just use the all Gauntlet AI channel here. So I will make a question thread. My eyes will be in this question thread. Now, I will not necessarily answer every single question that is asked during lecture. There are also other staff who are watching and especially if it's a logistical or a policy question, then you can expect an answer from somebody else most likely. My focus is of course on the lecture material and the topics at hand. So you'll be tagged. You all probably know how to use Slack or similar tools. So the main thing I'd emphasize is if you have a question, ask it like, we are here to help, so don't hesitate about asking questions. All right. What are our goals today? And this is something also important to our learning methodology here for people who are potentially new to that. We set out the learning objectives and these are things that we want you to get from this, from this material. So at the end you can use this to sort of check your own understanding and refer back to it, study it. You know, if there's a part that you didn't quite fight catch, that can be a good clue where you need to study. So we are going to learn about how the AI first methodology helps you ship that and we're going to do so with an actual somewhat, well, not somewhat real world example. So Ash, our head of product, had to put together an initial LMS for Gauntlet in short order. And so course he used a bunch of LLM tools and we're going to step through the process that he did and you will end up in the near future actually using and having access to the output of all this. I want to talk about a few of the particular sort of best of class tools in this space, but I do want to emphasize, as Austin also basically alluded to, this is such a fast changing space that, you know, there's a lot of likelihood that during the Gauntlet program some new LLM tool will launch that's probably actually pretty good and might be better than something that came before it. So it's, it's. We will offer tools and workflows, but it's not necessarily the end all, be all. Yeah, I see the question on replit. Replit is definitely A contender too. You just. We didn't show replit because we only have about an hour for the lecture. But you know, there's not necessarily objectively best tools here. Use the tools that work for you. Of course, they're the tools that we will be providing you, that we're paying for. So there's that aspect of it. And cursor in particular, I think is still somewhat uniquely positioned. But we'll get to that and we'll talk a little bit about chain of thought and some basic prompt engineering and really prompt engineering. I mean, part of what's so cool about LLMs is that you can just give them natural language and get results and getting to, getting good at that, even though it might not feel like am I really into. No, you're. That that counts. I mean it's, it's a little bit of a dark magic sometimes, but that definitely counts as a legitimate and important technique and one worth understanding. So. All right, paradigm shift here. So traditionally, how do you develop code? Well, I imagine you all know this. You've all done this in some way. You perhaps start with some templates maybe, or you just start with that little blank editor and you write code. Right. And then somewhat recently there are these chat agents like say ChatGPT3, right. That is good enough. That hey, instead of maybe looking up stuff in your documentation or searching Stack overflow, you talk to this chat bot that is kind of like talking to Stack overflow. Sometimes when you're doing this and it gives you things now, it has some limitations, right? It's, it's a lot of copy pasting and it only has the context that you give it in those prompts and a recurring. You're going to hear me talk about context and context window a lot throughout this course. And that's because that's like, that's the input of the actual machine learning model. That is the LLA that matters so much. And you want to have, if you're not, if you don't come. I come from a data science machine learning background, so I, I might take some of this language a little bit for granted. If you don't come from that background, that's okay. But you want to build a mental model where you see LLMs as a predictive statistical model. Basically that's what they are over the natural language distribution of tokens and such. And you know, the input matters a lot. And because when you're just chatting, it only has what you give it. It doesn't know other things. I mean, it knows what it's trained on in a sense, but it doesn't have access to your whole code. It doesn't have all the context that you have. So that's where AI first development is going to is a change. Right. We are using some different tools that be a variety of clever things. We won't have time to dig into the full details of how it all works, but there's lots of resources for that. But as an example, it doesn't just shove the entire entire code base in the context window all the time. I mean, there are LLMs that are big enough that you might be able to do that, but you wouldn't really want to. That would be expensive and slow. Instead, it uses various tricks. There's repo maps that let you sort of structurally understand a repository. It's similar to what IDs have had for a long time where they, they can link symbols across files, right? Like, oh, this was act. This import is from here, this was defined there. So you can understand the structure of files and then intelligence. And you can also potentially have a vector database of your code base. And all this is happening automatically. This is kind of like the magic that Cursor is doing for you. Cursor is doing these sorts of things so that when you interact with the LLM, it has appropriate context. And this might not sound like a lot, but it makes a huge difference. It changes. Instead of you still kind of writing most of the code and just using chat as like a somewhat more convenient stack overflow, you are, even when you're writing code, your code is like implicitly a prompt for more code. And you will end up spending most of your time in Cursor hitting tab and reading code. At least that's what I do. Right. And then like editing things here and there, you still need to be engaged, it still work and you will, but you read more than you write. And so that's why it's sort of more like a character. All right. To the tech stacks that are work best with LLMs. I'm going to defer that question to anybody who wants to answer their that question with their own experience in the thread. There's so many options out there. I will say I, I've been impressed with Cursor personally because Cursor is the main tool that it doesn't just do green. Of course we're doing Greenfield today and it's great. And a lot of what you're doing, you're starting projects from scratch. But you know, I, I'm often working with existing code Cursor works pretty well. All right, so a little bit about why this matters for the overall journey here. Well, we're asking you to do a lot of stuff. Gauntlet is essentially you building a lot of projects. Like there's, there's obviously we provide support and instruction, but the Gauntlet part of it is you making things and being able to make things efficiently. Now, I. People can differ here, but I would say, you know, there's the somewhat infamous motto, move fast, break things. Right. I would say with, with Generative ML, you want, you don't want to move fast breakings, you want to move fast, make things like, you just want to actually make a prototype, get to that part. Because any developer knows you get to the part where you actually have a running prototype and there's a sort of flywheel where you have a fast feedback loop and you're doing something and it gives and you get information from having done that and then you do the next thing and it gets better and better and better and you want to get to that point fast. And that's what these tools will enable. So we are going to step through, as I said, the building of the lms. You will be lms. Sorry for dropping so many acronyms. Learning Management System, it's Content Management System for educational stuff. It's not the most complicated, crazy thing ever, but it's actually a pretty important piece of what we're doing. And it's also. It's an industry in its own right. There are companies that make money doing this. So to build our Learning Management system, we're going to. The goal is something that has authentication, user management and, and of course, sort of content management. The main goal is you're going to use this to access slides and recordings and other resources, things like that. Right. And also submit stuff. Right. So we want to step through the. Basically what ash did, using LLMs, chatting with them to get the code that you need to get the plan first and then to get code and then iterate on the code. So that's another thing. And that's why I said move fast, make things not move fast, break things. Because even though the goal is to move fast, you still want to play. It's still worth at least five or 10 minutes thinking about what you're doing, writing it down and writing it down with the help of an LLM, because that's what we doing. You can use an LLM as a sort of brainstorm buddy. And in this case it really is just a chat interface LLM and the output from this would be something like a PRV project requirements document, which I imagine you're likely. But even when you want to move fast, you still need to plan. There's a saying in academia about how months in the laboratory can save you hours in the library. Right. So you, you, you want to spend those, that little bit of time planning and understanding what you're doing and what came before. So to plan with LLMs, and I'm going to have to tab out here in a sec. We're going to make a structured PRD that will be input for. We're going to show V0, which is a tool from Vercel that helps you build from this. But there are others in this space as we've discussed. And one of the main techniques you'll see in the prompting in this chat session is chain of thought prompting, where we basically ask the LLM to think through what it's doing. And it might seem funny, it might seem like, why does it. Like when you ask a personal question, it doesn't necessarily matter if you say, make sure you think that through. Maybe it does. But this sort of approach though, is very effective with a lot of LLMs and is one of the parts of prompt engineering because it essentially encourages the LLM to the base. Intuition I get is to sort of spend more tokens, to spend a little bit more of its own time possibly activating more parameters and getting to the tail ends of whatever probability distributions it needs to to give you the best output. It also makes the output pretty literate and easy for you to see and review and iterate on if you need. So let's see here, I guess, yeah, a few more slides before you do the example. So a few more details here. When do you want to break down prompts like that? Well, basically when it's a really complex task like make a whole lms, you might want to break. I'm sure you could ask an LLM to, hey, describe, make a PRD to make a whole lms. Like you could, you could give it a simple prompt like that and it would give you something that looks plausible. LLMs are great at giving you things that look plausible. That is exactly what they are trained and tuned for. But you want things that are good. And that means in this case breaking it down so it is sequential and has pieces that logically correspond to the pieces you actually care about, the pieces that will actually have to be built, things like that. And again, this is when you, when it matters. And in this case, because this output is going to be then fed to another LLM. It does matter, you know, if, if you blindly, without really quality checking the output from one LLM, continue to feed that to another LLM, the randomness of LLMs might steer you farther and farther off course. And here's a few example chains. So I'm going to leave this for reference because I think just keep an eye on the time and how much more we have to go through. You can see, I think the general idea here. You give it a overall goal, but then you break it out into different steps and you ask it for each of these steps individually and you are involved. That's the point here. You can, of course you could. While you're doing this, you could start by brainstorming and chat with the LLM saying, hey, I want to make a, you know, verification loop. What would you suggest as the steps? And you could see the output from that, maybe use that yourself. But you want to be engaged in this. This is not something to just throw and trust that the element will do it correctly. So spending tokens. Well, I'm being a little bit loose, I'll admit, but what I essentially mean is that you're putting more a token kind of means word. Technically, it's not exactly a word. It depends a little bit on the language model, but it means that we're just pushing. When you ask an LLM to think through this, you'll often notice that the response is more structured and verbose. Right? Like instead of just directly answering the question, it will say, okay, well, to do this, I need to blonde, blah, blah. Right. And that structure, because an LLM is a next token predictor. It is, it is giving you that next word based on the previous words. Structuring the output such that it has. That has those words as part of the output can increase the quality and reliability of what comes next. Can basically make it better. So spending tokens, the short of it is, well, literally spending money, literally pushing more words, more text through your, through your model and having a larger cloud bill. So that's literally what you'd. But the hope is, is that you're doing it in a way that increases the quality of the output that you care about. All right, so yes, I see that our archon is already on the actual prompt here. So let's go ahead and get to that. So here to make the prd and as I'll let you. Yeah, ass is fielding the question question for why he framed this prompt this way. I'll just, I mean, Aaron, the easy.
Speaker C - Ash Tilawat

Answer is because I used to be A technical product manager. So I probably said that.
Speaker B - Aaron Gallant

Yeah, you know, do what you know. Right, that's, that's totally fair. But you'll see here the structure of this prompt though, illustrates a lot of the things I was just talking about. It's breaking things down step by step. And I imagine this is not the very first prompt that Ash wrote to do this. I imagine that Ash himself iterated a little, although he's written a lot of prompts, so it probably didn't take him that much prize. But, you know, writing a prompt like this might take some iteration, some planning on your part and you know, you break down the user types, their, their goals and you can see like, what are we aiming at? Well, we're aiming at user stories. As a student, I need to. Whatever, right? Why? And then how the system will essentially facilitate that. So, you know, TPM type stuff or some sort of p. And then also kind of very basic prompt engineering. But this is all, this is a pretty good practice. You almost always want to tell the language model what you want tell it structure of the output, right? And in this case we want bullet points, we want clear and straightforward sentences and so that's what we get. And that facilitates our review, that facilitates potentially giving it to another tool, that kind of thing. So not going to read all of this. You have this link, by the way, it's in the slides, so you can definitely review all this as you'd like. Then the next step though is, okay, well, this is a lot of user stories. Let's take the top three user stories for each role. And then what do we need? What are the data models? Right? And then also telling it, hey, this is what we're using. And language model is trained on new enough Internet, it knows what we're talking about. And again, this helps it know the sort of output that's expected, expected. And again, think step by step. And by the way, you'll notice we'll paste some of these prompts into Claude. Different language models will behave different. The think step by step. In the case of chat, GPT might still be doing something, but clearly the. This part of it say output, you know, concise, blah, blah, blah. That's stopping the chat GPT from actually, actually at least quote, thinking out loud. It's not giving. It's. It's out loud, step by step. Whereas chat, whereas Claude, although they can just paste the same prompt into cloud. Right now it's probabilistic. The first time I did this, it actually did think through a little bit more at first in this case, it thought through a little bit, said, let me break this down. You're going to get varied responses sometimes like you're going to get different models, different invocations of the same model. I'll go back to the first time I did this. You don't think I'm crazy? Yeah, it's been a while playing with it. So yeah, it thought about this step by step and it sort of iterated the steps in order for all of them and then asked if it wanted me, if it, if I wanted to elaborate on the user stories. Right. So as opposed to immediately structuring the user stories individually, let's go back to the current one though. So, all right, so we were here. So we were at top three user stories and now we asked it for data models and it gives at least rudimentary data models. This isn't exactly a create table statement, but this is enough structure that if we feed it to Vercel or something, we could probably get a structured schema or V0, I should say, we could get a more structured schema. It indicates what the keys are, things like that. And you know, the top level entities we need we can look through. And yeah, this looks about right. Users, courses, materials, enrollments. If anything, this LMS is right now there's only one course at Gauntlet. You're all enrolled in it. This is thinking though, hey, you could have multiple courses and not everybody's going to be enrolled in every course. So you need to talk that relationship, that kind of thing. And this is still output. It's even giving the functionality requirements and what clerk will do. And now we're asking it to define API endpoints so what the actual back end will need to do based on this data model, based on the needed functionality. And so it's a list of API endpoints. And again, all of this was originally asked chatting with an LLM to rapidly iterate, you know, not starting himself writing code writing plans, but getting this sort of technical planning document with an LLM. And I think you get the idea of this one. Is there anything at the end that we need to look at? Looks all right. We eventually even get to like MVP launch requirements. That's how you know you did what you needed. So now of course, if there's output that you should be reading the output of your, of your. And if there's something that you want to change, just tell the LLM, just say, hey, actually, you know, I only want the top 5 MVP launch requirements. Can you, you know, refactor it and make, make it more concise that way or something like that. But for now we will move to the next step and do a quick check on questions. I think they've been answered by other people. I added the concise version of Claude. Yeah, there are other versions of Claude too. Yeah, you're right. I mean there's a whole drop down. Yeah. Sonnet versus yeah. All right, so back to the slides for a moment. So you have, you've done your plan and you did your planning with your Brainstorm bug, which could be ChatGPT or Claude or whoever. Next, you need to generate the prototype and what you want to do. You can't just copy paste this entire. Like if you copy pasted all of this into V0, you probably wouldn't really get the best output. Again, there is, there is some human labor to be done ultimately kind of a good thing. That's why there are jobs at the universe. Right. But you want to find the actionable chunks, you want to look through and feed these portions to V0. It's sort of similar to the idea of chain of thought. Right. But you're breaking down these chunks. And while I'm saying this, by the way, I want to emphasize I sort of alluded to this earlier. This is just one way of doing this. This is not necessarily the end all be all way of doing this. If you find a workflow with these tools that works for you, that's fine. You don't have to ask permission to do things differently. But the principles to take away from this are that you, you want to understand what the language models can do and then critically what they can't necessarily do or what they are weaker at and make sure that your workflow account possible. So we're going to end up doing few shot code generations. There's another prompt engineering thing probably already know. But few shot basically means give it some examples. In the case of V0, it's basically part of the, the UI and flow is that you give it components and patterns and stuff that you want it to use. It's, it's a, it's a front end development tool and so it's going to sort of speak that language and know what you. And then once you get it, V0 itself provides tooling to iterate. So we'll scroll. This session is also linked. Yeah, right here. So you can scroll through this yourself. So I will again take a somewhat quick pass in the interest of time, but feel free to be more thorough yourself later. And again, if, when you're being more Thorough later you have questions you want to ask them in the Slack, that's fine. Slack is also, you know, anytime async sort of thing. So that'll be a recurring theme here. We have an hour to lecture, but you're working more than an hour. You will need to spend some time on your own study and thinking as well. That's how schools work. So where are we starting here? So. So we're starting selecting the tooling. So Ash already knew what he wanted to use. And again, look at this prompt. So this is Ash here, our hero. I should say me, but I want to give ash credit. Prompting V0 and speaking from it here. Create a reusable table called component should have filtering available integer values. Search bar like this is not necessarily a layperson prompt. This is a prompt written by somebody who knows how to speak front end a little bit. You don't have to be a front end expert because you're not actually doing all that. You're not cranking out all this react or anything. You're asking an LLM to do it. But you need to have a mental model and you need to know the terminology. You need to use it collected because that's what the language model speaks. So anyway, we give it that prompt and we'll see here. I mean, this is already, by the way, this is, this is the output. So here's a, here's a visual preview. But for now let's look at the code first. So the first thing it gave us, it gave us, okay, this is a schema essentially, right? You know, it's giving some structure to the data, having some mock data that fits that structure or really a function that generates mock data. I like that. Having basic table filters and such. I mean, we're not gonna, we don't need to read every line of this code at the moment, but I mean, you can see it output a lot. Now again, if you were actually checking this into a repo, you do want to take the time to at least scroll through and inspect everything. Think of it like a code review of a pr, right? So as I said, you're going to spend more time reading code than writing it. And you might find things to change. Because even if it was a human writing this, there are often implicit assumptions or just little subtleties, right? So now you'll notice the output. Besides, creating these files files explains what the files do. So this is convenient as you want to review it yourself and says what the implementation should provide and says how you can further customize it. You know, you can add columns, change the styling. Then apparently what happened next was an error. Right. And did the UI prompt this automatically or did you have to paste this in yourself?
Speaker C - Ash Tilawat

Yeah. So what happens when you get an error on v0 and this is the same for Replit Agent, it gives you the logs on the right side and then you can just click a button to say to transfer it over to the chat window. And then I do like to adjust that prompt to say where the error was because if you let it go looking, it'll then just fix the random file that wasn't supposed to be fixed. But yeah, it, it's just a button that pops up on the right side that you have to click.
Speaker B - Aaron Gallant

Yep. Yeah, Claude cloud artifacts work the same way. When I was running through these prompts with Claude, it was, it started developing stuff, it had an error, it says we have an error, you want me to try to fix it? Like yes, please. And hopefully it fixes it well to, to see the button you'd actually have to trigger this chat yourself, which by the way you should like you can and change even the responses some like start your own fresh session, you know, start from whatever prompts you want and you'll, you'll see this pretty quickly. Should be self explanatory, I think. So when we said revise the code, the LLM started revising the code to address the error figures out. Okay, we're using all for all Weeks. Some, some picky variable name or schema mapping thing. We don't need to worry about details at the moment. But it looks like it fixed it, so that's nice. Then we're asking for the next feature. So we, we are chunking here. We asked it first for tables, now we're going to ask it for sidebar. Right. And the sidebar should basically let you navigate between all these places, all these parts of the elements. And so you can see by the way there's clearly some prompt engineering that Vercell did on our behalf because this response here is already kind of thinking out loud. So I'm pretty sure that like you know this, this user prompt is being put into some sort of template that is being sent to the LLM that is telling the LLM how to respond, how to give a balance of code and explanation because the output here always has a very consistent structure. Thinking about what it's doing and getting code and explaining what the code does, not asking it to do. If you wanted to do this with something that wasn't V0, if you want to just like fire up a vanilla llama and try to get it to do something like this. You'd have to do quite a bit of prompt engineering. And then we can ask it to move stuff to reorganize things. Right. So it's reorganizing stuff and it will update the project's structure so it understands that. And another update, let's add a column inside the table, and it can do that. And then each time it's giving you the files it modified. That takes me to preview for some reason, though. Ah, but it looks like it does. Sure. It does indicate that there is what the diff was. If we navigate. Yeah. Show diffusion. Yeah. So it looks like it takes a little bit of clicking. There's UI problems here. And I'm sure v0 will probably iterate on their UI again over the next 12 weeks. While you're. While you're doing this, V0 itself is likely to change, but you can navigate to the file that it said that was changed and you can see the diff, and you can click the little diff indicator and you can see the actual change. So all the. All these other lines were generated from previous parts of the discussion, but this latest discussion just generated a couple of lines like this. Right. And so again, think of this as code review. And at least when I do code reviews, I find breaking it into diffs very helpful. That way, you know what you should actually be focusing on. All right. Update to include project as a type. Sure. Yeah, that makes sense. Create a page for calendar. Great. Create. There's. There's a lot. Let's see here. I might not just read all of this. Let's at this point, sort of get to the end preview. So this, the point is, is this is quick, but this still takes time. Like a lot of this is out. The prompts are all fairly straight, but then you do have to read the output and think about it. But this is a good example of what we mean when we say, like, you're not writing the code, you're. You're interacting with an LLM. Right. We're asking for forms, we're asking for a lot of pieces here. And oh yeah, there's another error, which again, we automatically asked it to fix. And let's just get the. The very end here. Ah, so there was even an error during deployment. And was this just like running the front end? Ash, what was this last error here? It looks a little.
Speaker C - Ash Tilawat

Yeah, so that was a build error. So with v0, you can deploy directly to Vercel. So that was a build Error on when I was trying to compile. And then so you can actually, when you try to deploy it, bring your build errors back and then try to fix them as well.
Speaker B - Aaron Gallant

Yeah. Part of what's nice about v0 is the integration with Vercel, which is itself a host. And so yeah, we can feed it back and fix the error. And here is our preview of what we have and we can navigate it, we can see the different components. Maybe the calendar doesn't go anywhere yet. I guess we're using Google Calendar, but we can look at the other pieces.
Speaker C - Ash Tilawat

I will say sometimes v0 when you're switching back from versions, it will lose itself. So sometimes it should have a page in and then it will.
Speaker B - Aaron Gallant

I lose track version. It's actually got here in the preview and there's even a console which we don't need. But I assume right now, But I assume this is where like when you did the build the deploy error, I assume maybe this is where it was also displayed. So what is the ultimate result here? Well, the ultimate result of course is this code which you can also then check into a repo as you probably should after you finish reviewing and making sure it's what you want. And so that's step two of our overall AI first development step one was the prd. What's step three? So we have our scaffold. Step three is iterating and you know, actual development with Cursor. So Cursor, and I already said this at the beginning, Cursor, which is one of the tools you'll be given, is in my opinion somewhat unique in that it's well suited not just for making something from scratch, but dumping in a bunch of existing code like this and understanding that code and helping you iterate on it more effectively. And frankly, even if you don't use it, like even if you just use it as if you were using VS code, but you only use the tab for smart LLM completion. That can already potentially be a little bit of a speed up. So you almost don't even need to learn anything to use it. But of course you should learn things because there are a few tricks to what it can do that make it more effective to use. So the goal though, as I said, is when you use Cursor, it's an editor. It's. It looks like VS code and this is CSS global CSS from that project that we prototyped in V0. But what we want to do is have Cursor complete most of what we're doing. We'll Start typing and cursor will suggest more code. Or we can actually chat with an LLM and ask it to do things. And this will. Because coding is often very repetitive, this will just speed up literally the mechanical process of you writing. So what are some of the things you can do? Well, you can do command, command or control if you're not on the Mac K for inline generation. So what that means. Oh, you can see, by the way, it's already. Yeah, it's already suggesting stuff like even just making blank space. And it's something to get used to. You have to sometimes ignore it because you don't always want it. But it's pretty clever. It learns very well from what you're doing. And if you start doing stuff, that will become part of its context window that it will use to inform what it's going to suggest. So this gray text here, I guess I'll teach this one first. Instead of Command K, this is what I was talking about, magic of text. If you press tab, you get whatever text it's suggested. And if you press tab again, you get whatever text it's suggesting. And apparently it thinks I I should use Tatilian web font. That must be a popular font. That's probably why it's like statistically suggesting it. I guess we don't actually want this. We didn't ask for. For it, so I'll go ahead and get rid of it. But for now though, we'll show to generate Control K. And this is like inline generation. And you can see there's, there's model options. Stick with the default for now. We can save type of model chatter for later lectures and for Slack, but we can say, you know, style all ordered, list all elements to be filtered. This would be terrible. But just, just to show what you can do. All right, yeah, well, I mean, that's, that's a very minimal example, but it shows the ui and you can see whenever you do this, instead of the like tab to complete, it comes up with this green, which is like code that would be added, kind of like a git diff. And you can either accept it or reject it. So if you accept it, it becomes part of your code. If you reject it, of course it doesn't. And you can give it more complicated instructions than that. Obviously. At the same time, the purpose of Control K isn't necessarily to be like write a complete implementation of whatever because it's inline. So its scope more is like write a function that does this or write some CSS style file that does that. If you want to do larger things. That's what command L is for. And that takes you over here to the right. Now, there's actually a lot going on here, so we'll, we'll break this down. You'll see this is the context. So by default, it will assume that whatever file you came from should be in its context, and it will even kind of suggest other files that you could include in the context window that might be relevant. Right. And it's doing that based on its understanding. Oh, if you click it twice, apparently you get to see the file. Right. Get rid of that. But it's doing that based on its structural understanding of the repository. So it's like, okay, you're looking at global css, what are some related files? And of course, global CSS is kind of connected to every file. So these suggestions might be kind of almost arbitrary. But often if you have a very large code base, this would be a good way to see, hey, these are the files related to it. And clicking it basically says, hey, send this file along in this prompt as well. And then the sort of stuff you can ask. You can ask it to write code, but you can also potentially ask questions. I mean, let's see here. How does the global CSS file impact the styling of sidebar, the sidebar and toggle? And it should be able to give a somewhat literate answer because it should be able to point out. Now, of course, there's stuff that just impacts everything, so that's the first thing it's pointed out. But if there's anything in here that impacts those specifically, which there might even not be. Yeah, okay. Layers and borders and background colors. And so it's breaking down the parts of the code that are showing what's connected across these files. And this is just asking a question. You're not even actually asking it to write code. You can of course ask it to write code. You'll also see that for each of these, you can potentially, like, copy paste. You can apply this code to your editor. So there's a lot that you can do here. I use the chat mostly, actually, though, to chat and ask questions. And another thing here, so sometimes instead of giving it a specific file, you can notice, like, what is the entry point? Something like that. We'll see how it does with that. And there's this. So if instead of hitting enter, you do control, that will chat with the entire code base. So that's potentially a little more expensive because it'll probably result in more tokens being sent. Although, of course, the way Cursor works. You're not necessarily paying per token, but you do have a set number of like high quality chats per month. I, I look at their exact pricing model option. Now of course this is a front end app, so I'm not sure exactly what it's going to say here. A back end app might be a little clearer, but it thinks, yeah, okay, it identifies it as a Next JS application. And I want to emphasize here like all I did, all I did here was ask what is the entry point. Naive prop and didn't and just said across the whole code base and it was able to understand this is Next JS layout. TSX is the root and you know, it identifies this as the entry point, which I think is fair. And again, if this was a backend app, it would probably find the actual like point that starts the, starts the server, you know, that kind of thing. So you can ask it to write code. If you ask it to write code, it'll output blocks like this that you can copy over if you want. I'll let you explore that. So can you speak to agent mode? Yeah, so that's, that's exactly where I'm going next. So there's other stuff here. There's even something new that I'm not going to show because it warns you that oh, this is a new experimental but bug finder. I guess we'll just idle search for bugs. And it even warns you that this is an experimental expensive, weird feature. So the slightly less new but still very cool feature is Agent. And let's use Agent. And what we're going to do here actually is use Agent to finish fleshing out this because the first output from v0 had a lot of the structure but it did not have Clerk for authorization. Did not actually have the authorization part. So Ash added that by prompting and doing it in agent mode. So again, this is on the sidebar, the command L. Sorry, yeah, Command L side of things. And we click composer and there's normal mode. But normal mode lets you potentially combine steps and combine files. But what's particularly interesting is agent mode, which is even more autonomous. You can run commands and can do quite a lot. So Ash, does this look right to you? Should I add any other context or just let it rip?
Speaker C - Ash Tilawat

No, I cursor agent. I just did like a zero shot prompt. Very little context and it just sort of worked out well.
Speaker B - Aaron Gallant

Let's, let's, let's see if lightning strikes twice.
Speaker C - Ash Tilawat

The, the, the students are saying in the chat, they're like yolo Mode. So I basically YOLO mode the cursor.
Speaker B - Aaron Gallant

Yeah. So yeah, Clerk. Clerk was yoloed via cursor. And let's see if we can double yolo at least watch Cursor agent do its thing. And you can see again clearly anybody who's done prompt engineering. There's some prompt engineering happening here. Like this was fed into some sort of template that's causing it to say, hey, think out loud, explain what you're doing. Look for time, blah, blah, blah. So it's first, let me check your structure. So it's looking at our code and then it knows, hey, this is probably the first file I should look at. Layout the clerk. We're gonna have to install this. And then it's recommending this command. Oh, which I might have to install NPM in my box here. It's just called Node JS on Fedora or npm probably. Well, we may not step through all of this, but you'll see this is an important point. While we're waiting for that, it suggests commands, but it isn't going to just run the commands. All right, let's hope that gives us npm. So we. And this, this is important because this is running as you right now, by the way, I happen to be running this inside. I'm actually on a Linux distribution and I'm running this inside a vm. You might want to consider that sort of containment when you're using these tools, just because it gives you a little bit of assurance if something weird happens. It's not because these tools are going to do anything malicious with intentionality. It's just bugs. Right. Or just mistakes. So, okay, got npm? Sure, go ahead and run npm. Install Clerk next. Js, it looks like it's using cloudsonnet, but there's a drop down here you can select. And after we finish running through this, I'll show a little bit more about Cursor, how you can customize it. You can customize models. You can use something called Cursor Rules, which is kind of like a system prompt, if you're familiar with that from a prompt engineering perspective. All right, this is thinking a while. Yeah, well, it's not really in our interest to watch something spin. So while this thinks you get the idea, it's going to be an interactive session. So the agent mode is powerful, but it's not completely autonomous. It's going to think through things and then it's going to say, hey, I, I, we should install Clerk. And I, yeah, you're right, you should install Clerk. Right, yes. And then it's going to think of something next after that, like, oh, then I'm going to have clerk to this file and I'm going to add this and you'll see the diffs and you'll approve and allow that into your editor. But of course change it if you need to. Right. You're essentially, you can think of these agents as like very eager and oddly knowledgeable junior engineers who report to you and are always available. Right. Like they know a lot of stuff and they're very fast, but they don't always have the judgment, the context of what your project really is. They only have the context you feed them. Even with the tool as clever as cursor that tries to essentially figure out the context from your repository, from the surrounding stuff. We all know from working in software development that the code itself is not always all the content. There's context that is just in people's heads for context that is, you know, in some doc or in some discussion or whatever. Anyway, it's going to town. It's missing some other module here. So I'm not, I'm not going to try to fight with setting up a whole node environment for this. I don't do a lot of JavaScript development, I will confess, but this would definitely speed me up if I was going to do that. That and it is, it is actually adding stuff even though it's missing some imports. So that would be easy enough.
Speaker C - Ash Tilawat

Aaron, do you want me to just share mine?
Speaker B - Aaron Gallant

Sure, if you want to do that. Your session.
Speaker C - Ash Tilawat

Quick one before. I know you have to do the project, so I'll do, I'll do a really quick one.
Speaker B - Aaron Gallant

Yeah, yeah, I was going to step through the rest. Yeah, run through it and I'll. I'll show the rest of cursor stuff and then the file project. Excuse me.
Speaker C - Ash Tilawat

So here the clerk module is installed. Then it made the middleware. Middleware. Right away there was an error or a warning that was then resolved. It created the provider which then it wrapped the sort of main component and the root layout in and then it created the sign in component which I believe it just used a component out of Clerks, one of the components that comes built in. And then there was a sign up component and then finally just added user to one of my components. Then I added the keys and the agent pretty much did all the tasks. There wasn't really much I did after that sort of ended there before we sort of got into class.
Speaker B - Aaron Gallant

Very cool. And by the way, I looked more. It actually basically worked for Me too. Those were just lint warnings saying that it was missing certain things. But it actually did still create and suggest a lot of the same sort of code. So. So that's a little bit about rapid development iteration. Now, obviously this is still an editor. This is basically VS code. You should still be editing code yourself itself sometimes. It's not like you never edit code when you're doing this approach to development, but you should definitely take advantage of these aspects of it. The tab completions, as I said, are just like free speed once you get used to them. And the the L to chat with it, the composer with the agent mode to do all sorts of more powerful things. Basically you can ask it to add features or potentially refactor code bases would be another good use of agent mode. And then Control K for little inline instructions, which I would most often use for just like adding a function or something like that. Although honestly there are lots of times where you can do Control K, but it might even be faster ergonomically to literally just start typing. Well, this is a CSS file, so you don't really have functions here, but you know to start typing some sort of function and literally. Yeah, well, it's already suggesting stuff, but literally start typing your function and it will suggest code to come next. So that can be faster as well. All right, so back to the slides for a little bit. Talked about this, talked about this. You can also of course control K to select code. That is something I should show here. So in addition to just control K to add stuff, you can select code and hit Control K. And then this code would be the context and I can say rename app to. I mean that's kind of silly, but you can see that it will just change and then you can accept or reject and we'll go ahead and accept and then it has that. So that can be another way to use it. So you can also customize cursor. So if we go to the settings here, we can see that there are models. This is the default. Default's not bad. There's lots of other models that you can enable and throw at it and multiple models are enabled because when you're actually interacting with it, you can select the model you're using here. There's more advanced configuration you can do in terms of the features and cutting edge stuff that's happening. As you can see, the defaults tend to be very bleeding. Cursor embraces. The idea of my cursor itself is iterating quickly and giving us very current things and Changing kind of every week. And then the other important thing here is rules for AI. So you have an example here. These are the cursor rules, these are global rules. So these will happen for all of your checks. If you want your, you know, to say, you know, always output Pep8 when writing Python, always make it right and just tell the LLM that you all. It might almost do that by default, to be honest. But like you're telling the LLM, hey, this is what I want all the time. And then the other thing that you can have are custom rules that are project specific and that is by making a dot cursor rules file in that project and that'll be picked up automatically and that will give a place to put, you know, maybe project standards or other things that are relevant to that specific development. And there's a whole curated repository of these sorts of things. So there are starting points in a bunch of like if you're doing full stack development, you want a scalable go back and here is not word wrapped, but here is a cursor file, right, which is essentially again a system prompt telling it how to behave to make a good scalable go back in and what it's doing. It looks like there's even some multi shot examples, few shot examples in here. It's a little hard to tell because no new lines, but of course that doesn't matter so much to the LLM. So it is definitely good to look for, to make or look for relevant cursor rules files when you're looking at a project to make cursor behavior even better. And then yeah, we already stepped through code generation with chat, so when you see code you like, you can click Apply and that'll put it in your. In your. So that is basically it for the slides. Oh, there's one other thing that I want to show a little bit from earlier, but the. If I go to Claude, the main flow that Ash actually used was ChatGPT and V0, but Claude for instance, actually did both. Claude has a thing called artifacts and as I was making the prd it basically said hey. I said, hey, make me the actual data models. And it actually made an artifact, actually made code. And for some reason it decided to use Prisma, I think. And I said no, do it in SQL. And so then it changed it to SQL and make me some next JS API routes and it made the next JS API routes and then I asked it to iterate on the API routes and it made a different version of it. So again, different tools Similar functionality. There's going to be a lot of that. There is not necessarily any objectively best one, but Claude is definitely worth considering as a tool. It's a contender. That said, while it does have the niceness of like the chat seamlessly going to artifact generation, it doesn't have the seamless like preview deploy thing. Right. And the diff. The diff view actually isn't quite as nice. It's got this versioning thing down here, but it's not telling me what lines it shows change. So trade offs. All right, I'm going to check to see if there are any questions. What changed most recently about workflow and cursor. I mean, I guess agent mode. Right. Agent mode is. I. I consider that cursor as a whole feels recent to me still. I've been doing this for a while, but within cursor. Agent mode is definitely new and different and pretty cool. All right, so with the time remaining, we're going to talk about the project. So. And I believe this should already be set for people with the link, so I'm just going to even drop this. It's probably going to be linked in other places too. Oh, it's already shared. Thanks. So the structure, structure of Gauntlet is that you're going to be working on projects and the first week is spent essentially creating a often full stack app that would potentially be a significant amount of work to develop. But you will be accelerated by the power of these tools that we just discussed. And so what that means is that you will be spending a week in this case trying to make a chat tool, something kind of like Slack. Right. Because these chat tools are the foundation of our, you know, distributed asynchronous workforces in the modern technological era. They often are the workplace. Even to places that go back to the office, these are still pretty important. And if you're remote, principally this, this is home. So how does this. How what is Chat Genius then? Chat Genius is a project that offers the functionality of Slack, but then also will have an avatar that will represent users and enable them to essentially delegate communication or have much more intelligent auto responses on their behalf. So a little bit about the structure here and there'll be time to go over more of the logistics and structure. But this co. This will be turned in, I believe via the lms, but there will, you know, you'll be writing code, there'll be a brain lift that will also be turned in, but more on Brainless on Wednesday we actually teach more about brain lifts and then a walkthrough and this would say be like a screen share a loom type thing that you can share, that you can publicize X, LinkedIn or whatever else. But X and LinkedIn are good starting places and interacting with the feedback. And what we really want to see is, you know, generate some buzz like get people to use it. You know, get people interested, get actual users, get actual feedback, iterate on it and, and see how far you can take it. You know, we're giving baseline goals here, but the more you go and if you come up with your own ideas of things to add, that's great. This, this is not, don't see this structure as in any way limited. This is meant to be a foundation for what we actually love to see you. So what is the first week? On the first week you should develop something such as Slack as a reference app. And it should have these core features. It should have authentication, real time messaging, some sort of way to organize those messages in the channels and dms, some way to share files and search, search the text as well. I would say something for you know, users to have presence and status, some sort of threads and some sort of emojis and that's like baseline, that's kind of, that's kind of Slack. Slack has other stuff. Right, but that's a lot of of what we use. And please do use AI first approaches in doing this. And also it is okay, so cursor itself, it looks familiar to you because it's built on VS code, right? They didn't start completely from scratch. It is okay if assuming that the license is compatible with things to build on existing projects or to use open source software however you see fit. Just make sure that you know, you actually are building on it and making it appropriate for what you're doing. But potentially, you know, this is the week one goal. But I will say that if you get through it extra quick, you can always add more features you come up with or even start thinking about the AI objectives. So week two, the goal by the way at the end of this week on Friday is that you have your chat app, right? And it's just a chat app at that point. It doesn't have AI features, just developed with it. Week two, you will be adding AI features. And the high level goal again is this concept of an AI avatar. And what this AI avatar means is something that should be able to represent a user, act somewhat on their behalf, communicate in this case on their behalf, and start with text because these are text tools and the main, what the user provides the train the material you have to work with for your prompts will be text. But given a prompt this avatar should be able to create and send communication on behalf of that user and it should be automatically informed by content on Slack, I. E. It's context aware. Sort of like you saw with cursor, right? How cursor is context aware somewhat automatically when I talk to it and when I ask it to do things. And this also should sort of sound like the user should. Should be prompted and given some information of how to sound like the user to. To give it some personality and the envisioned workflow here. But again if you come up with your own ideas, please add them. The goal is impress us, not do exactly what we say. But the idea is that when you are away instead of just like becoming a. A blank red dot or a white dot or whatever zzz, you can have this as your sort of AI representative that if somebody has a question they can get a response from that avatar. So you could send it to send communication or people could ask it questions and automatically receive a response without your intervention. And of course there should probably be a notice like this was sent by the avatar. So might not be 100% and double check things later and all that. But it can potentially be a way to have something much better than a static away message that actually can handle some of your business while you answer questions on your behalf and potentially have access to your DMs and context like that. So that's the AI feature and then the advanced features. If you knock this out, the part is to really make it an avatar. What I mean by that is to make it represent the user with voice or video. So voice synthesis is a pretty well established space and we'll provide more resources on it as we get closer to it. There's a number of product offerings in that area. You often have to give them a few minutes of speaking or something to make them sound like something, somebody you could potentially have them, you know, let the user select it to sound like somebody else but you, you know, having the user sound like themselves, match themselves would be a good feature to target. And then video synthesis is basically the same thing, just a lot more dimensions and a lot bigger bites. And there are again services. We link a few of them. Hey gen and vid, if you get to this part, they do have free tiers, so start there. But if you are doing, if you're at this level and you're running the limitations there, we want to help you. So let us know and we'll figure out how to unlock whatever resources you need. So and then the idea is that this video or voice avatar would match the user and represent them and perhaps even have expressions and gesture and not be too creepy. I mean that would be the ultimate goal. The cutting edge here, if you look at the more impressive demos from these services, is pretty good. Like they, they are somewhat getting out of that creepy valley, making things that look really very, especially in smaller windows, very good. It does require a decent amount of information from the user to look like the user. So if it's going to just be based static on the user's like single picture, it probably won't be. You wouldn't be able to make a fully video realistic avatar based on that. But you know what, if you figure out a way it definitely impress us. And if, and another way you could do it is, well, if we just have the static picture, then we just synthesize the voice. Or maybe we go for some more stylized avatar that doesn't try to be photo stick, but instead tries to be artistic in some way. That can be a good way to, to deal with those limitations but make it not creepy. So that's the goals of the project. There's more here in the doc in terms of resources and information you'll need. And of course there will be more lectures where we actually talk about some of the techniques you need to build these features. But already today we've talked about the AI assisted development. So what you should need to start working on this to start working on the week one objective. So after this lecture you should start, it's another hour of the day and you're going to need all them that you can get to to implement all these projects. So that should be your initial target. There's some more structure here. So this should also be on your calendar. But there will be a check in already tomorrow. No, that's Wednesday. I guess you should hopefully hit MVP by tomorrow. By MVP I mean like a working chat, something that starts and maybe doesn't even have all the functionality, but at least like has messages and channels. And then there'll be a check in on Wednesday and then app should be complete on Friday. So that's the schedule. That's the goal for this week. You want to have a working app already tomorrow that gets you to the point where you have those feedback loops and you can add features and get ready to go. All right.
Speaker A - Austen Allred

Yeah, real quick you guys, sorry, I'm still getting over cold. As Aaron said, the MVP we're going to have due tomorrow. We'll send you a link to an invite to the lms, which is the place you'll be submitting it. You saw we actually decided to rebuild the LMS using AI. So you saw some of the process of doing that. But there's going to be a whole lot of researching and figuring stuff out and guess and check and so use the help channel in slack. We've got TAs there. But we're definitely, as mentioned, we're getting thrown in the deep end very intentionally, very quickly and then as a, a fun little thing. On top of that, we're going to be looking through the MVPs that are submitted tomorrow. I'm going to, based on the criteria of what Austin likes, be giving $500 to the coolest MVP. So that gives you, I mean, you know, including sleep time, like 30 hours to work on it. So we're intentionally having a quick check in early, early to see where you get and that's a good question. I don't know. Surprised me. I mean, honestly, look, you're talking 30 hours from now. A chat app that works is pretty impressive. So yeah, so we'll be giving $500 to whoever stuff is the coolest and then later today. So get started now. You've got a few hours to work on it. Um, and then we'll have Joe, who is one of the founders of. I. I don't know, he's a spiritual co. Founder of Gauntlet for sure, founder of Trilogy, founder of Trilogy University, and he's one of the smartest people I've ever met. And so we're really excited to meet him. Yes. And I have not been. Yeah, don't like, obviously if you just pull an entire like fully built Slack, that's probably not what we're looking for. But yeah, let's see. And yeah, I think you can do whatever you want with him. The goal is to build the best chat app you can build using AI. First development and the first turning point is going to be in a few hours. It's not too terribly long. Yeah, we'll meet Joe later today. Clearly the intent here is to throw everybody into the deep end and get building as much as we can as quickly as we can. Obviously there are a lot of AI techniques and practices that are more advanced that we'll cover later. So yeah, you're going to. You should have access to. So let me add a disclaimer that I know I have like four emails of people that have different account issues. Ignoring those which we will solve, you'll be getting access to an AWS account. You should have a Cursor Pro invite your inbox, and you'll also be getting GitHub. The UI can be whatever you want. Brainlifts. We're learning about tomorrow in the morning, and we're not going to pay attention to the brainlift submission until the end of this week, but I think everyone's.
Speaker C - Ash Tilawat

Really worried about deployment. So let me get one point across, which is I want to see a video demo, so if that's local, that's fine. Like, I don't want anybody to worry. Like, you should be worried about rebuilding this application and giving it all the functionality that it's required and taking that over the edge. Deployment should be when we've added the AI features and we're ready to, you know, fully out there and launch this. But right now, let's work on MVP and get something out there that's really good and I just want to see a loom video of what's. How it's working, what's going on. It could even just be on Zoom.
Speaker B - Aaron Gallant

You can record yourself.
Speaker A - Austen Allred

I will. The. The first project said the. The final deadline. So at the end of the week, it needs to be deployed. But not for the mvp.
Speaker C - Ash Tilawat

Yeah, not for tomorrow. That's what I'm saying.
Speaker A - Austen Allred

Yeah. Sound good? Okay. I'm going to dig through a bunch of questions about access. I mean, you can use whatever you want. We're giving you AWS for free, so, I mean, obviously if you're doing your own Azure stuff, then that's not super expensive. So. Yeah, so we're trying to make the deadlines at 5pm Central, which is why it's kind of the. It's on the calendar at 3pm Pacific, 5pm Central. The reason we're doing that is we're gonna. On the. The final deadline is actually basically midnight on Sunday, but if you submit on Friday, that will give you a chance to make any revisions you need over the weekend. So we're trying to stagger it out so that nobody gets surprised and. And something isn't up to par and it's a big shock to everybody. So that's why we're doing more required submissions this time than there will be in the future. Next time we're not going to have you submit like three or four different versions. It'll just be one submission. So, yeah, I'm trying to scaffold it a little bit, but yeah, we'll get AWS invites out here soon. You should have cursor invites. I know. I have a bunch of you that that have X didn't work. Where X is some account will solve those. And yeah, VO has a free tier that's good enough for anything we would be doing. So yeah, you can go ahead and slack me later on. We'll get into a more in the future stuff that's like mission critical. Email is better because slack gets lost in the noise. But for right now, just slack is great. Come to me for pretty much anything and we'll solve it all. Okay, we'll see you with Joe in just a little bit.
